Iteration 0:  80%|████████  | 16/20 [00:32<00:08,  2.24s/it, episode=10, norm_ret=-10.473, true_ret=-23158216.000, steps=600]
Agent gate_2 episode reward: [-57.80339495]
All agents episode reward: [-57.80339495]
Agent gate_2 episode reward: [-14.62306335]
All agents episode reward: [-14.62306335]
Agent gate_2 episode reward: [-8.93689428]
All agents episode reward: [-8.93689428]
Agent gate_2 episode reward: [-9.19457448]
All agents episode reward: [-9.19457448]
Agent gate_2 episode reward: [-7.42642741]
All agents episode reward: [-7.42642741]
Agent gate_2 episode reward: [-4.69065433]
All agents episode reward: [-4.69065433]
Agent gate_2 episode reward: [-0.9098528]
All agents episode reward: [-0.9098528]
Agent gate_2 episode reward: [-0.73299371]
All agents episode reward: [-0.73299371]
Agent gate_2 episode reward: [-0.20599666]
All agents episode reward: [-0.20599666]
Agent gate_2 episode reward: [-0.20545434]
All agents episode reward: [-0.20545434]
Agent gate_2 episode reward: [-0.14778855]
All agents episode reward: [-0.14778855]
Agent gate_2 episode reward: [-0.14227427]
All agents episode reward: [-0.14227427]
Agent gate_2 episode reward: [-0.13583185]
All agents episode reward: [-0.13583185]
Agent gate_2 episode reward: [-0.14671217]
All agents episode reward: [-0.14671217]
Agent gate_2 episode reward: [-0.14004271]
All agents episode reward: [-0.14004271]
Agent gate_2 episode reward: [-0.15235883]
All agents episode reward: [-0.15235883]
Agent gate_2 episode reward: [-0.14486649]
All agents episode reward: [-0.14486649]
Agent gate_2 episode reward: [-0.14637395]
All agents episode reward: [-0.14637395]
Agent gate_2 episode reward: [-0.15396639]
All agents episode reward: [-0.15396639]
Agent gate_2 episode reward: [-0.16308905]
All agents episode reward: [-0.16308905]
Iteration 1:  80%|████████  | 16/20 [00:31<00:07,  1.94s/it, episode=30, norm_ret=-0.183, true_ret=-16125188.000, steps=600]
Agent gate_2 episode reward: [-0.15829685]
All agents episode reward: [-0.15829685]
Agent gate_2 episode reward: [-0.16295832]
All agents episode reward: [-0.16295832]
Agent gate_2 episode reward: [-0.16590764]
All agents episode reward: [-0.16590764]
Agent gate_2 episode reward: [-0.16753231]
All agents episode reward: [-0.16753231]
Agent gate_2 episode reward: [-0.18589402]
All agents episode reward: [-0.18589402]
Agent gate_2 episode reward: [-0.18032902]
All agents episode reward: [-0.18032902]
Agent gate_2 episode reward: [-0.19819495]
All agents episode reward: [-0.19819495]
Agent gate_2 episode reward: [-0.19451792]
All agents episode reward: [-0.19451792]
Agent gate_2 episode reward: [-0.20455442]
All agents episode reward: [-0.20455442]
Agent gate_2 episode reward: [-0.21034126]
All agents episode reward: [-0.21034126]
Agent gate_2 episode reward: [-0.1929297]
All agents episode reward: [-0.1929297]
Agent gate_2 episode reward: [-0.19937477]
All agents episode reward: [-0.19937477]
Agent gate_2 episode reward: [-0.19955118]
All agents episode reward: [-0.19955118]
Agent gate_2 episode reward: [-0.20745434]
All agents episode reward: [-0.20745434]
Agent gate_2 episode reward: [-0.20779909]
All agents episode reward: [-0.20779909]
Agent gate_2 episode reward: [-0.22497919]
All agents episode reward: [-0.22497919]
Agent gate_2 episode reward: [-0.22035329]
All agents episode reward: [-0.22035329]
Agent gate_2 episode reward: [-0.20462443]
All agents episode reward: [-0.20462443]
Agent gate_2 episode reward: [-0.2222531]
All agents episode reward: [-0.2222531]
Agent gate_2 episode reward: [-0.20996129]
All agents episode reward: [-0.20996129]
Iteration 2:  80%|████████  | 16/20 [00:31<00:07,  1.93s/it, episode=50, norm_ret=-0.232, true_ret=-14548538.000, steps=600]
Agent gate_2 episode reward: [-0.21876429]
All agents episode reward: [-0.21876429]
Agent gate_2 episode reward: [-0.2317026]
All agents episode reward: [-0.2317026]
Agent gate_2 episode reward: [-0.22976225]
All agents episode reward: [-0.22976225]
Agent gate_2 episode reward: [-0.22342254]
All agents episode reward: [-0.22342254]
Agent gate_2 episode reward: [-0.22899801]
All agents episode reward: [-0.22899801]
Agent gate_2 episode reward: [-0.23664892]
All agents episode reward: [-0.23664892]
Agent gate_2 episode reward: [-0.24593878]
All agents episode reward: [-0.24593878]
Agent gate_2 episode reward: [-0.23376867]
All agents episode reward: [-0.23376867]
Agent gate_2 episode reward: [-0.23480802]
All agents episode reward: [-0.23480802]
Agent gate_2 episode reward: [-0.23887236]
All agents episode reward: [-0.23887236]
Agent gate_2 episode reward: [-0.24418691]
All agents episode reward: [-0.24418691]
Agent gate_2 episode reward: [-0.24105084]
All agents episode reward: [-0.24105084]
Agent gate_2 episode reward: [-0.24051476]
All agents episode reward: [-0.24051476]
Agent gate_2 episode reward: [-0.23260198]
All agents episode reward: [-0.23260198]
Agent gate_2 episode reward: [-0.24904187]
All agents episode reward: [-0.24904187]
Agent gate_2 episode reward: [-0.25262374]
All agents episode reward: [-0.25262374]
Agent gate_2 episode reward: [-0.23992368]
All agents episode reward: [-0.23992368]
Agent gate_2 episode reward: [-0.24915926]
All agents episode reward: [-0.24915926]
Agent gate_2 episode reward: [-0.25951432]
All agents episode reward: [-0.25951432]
Agent gate_2 episode reward: [-0.25411745]
All agents episode reward: [-0.25411745]
Iteration 3:  80%|████████  | 16/20 [00:31<00:08,  2.01s/it, episode=70, norm_ret=-0.263, true_ret=-13752989.000, steps=600]
Agent gate_2 episode reward: [-0.2395978]
All agents episode reward: [-0.2395978]
Agent gate_2 episode reward: [-0.25173028]
All agents episode reward: [-0.25173028]
Agent gate_2 episode reward: [-0.27738717]
All agents episode reward: [-0.27738717]
Agent gate_2 episode reward: [-0.25340878]
All agents episode reward: [-0.25340878]
Agent gate_2 episode reward: [-0.26820116]
All agents episode reward: [-0.26820116]
Agent gate_2 episode reward: [-0.26469735]
All agents episode reward: [-0.26469735]
Agent gate_2 episode reward: [-0.26776927]
All agents episode reward: [-0.26776927]
Agent gate_2 episode reward: [-0.27546736]
All agents episode reward: [-0.27546736]
Agent gate_2 episode reward: [-0.26345198]
All agents episode reward: [-0.26345198]
Agent gate_2 episode reward: [-0.26440234]
All agents episode reward: [-0.26440234]
Agent gate_2 episode reward: [-0.27595061]
All agents episode reward: [-0.27595061]
Agent gate_2 episode reward: [-0.28553334]
All agents episode reward: [-0.28553334]
Agent gate_2 episode reward: [-0.26970005]
All agents episode reward: [-0.26970005]
Agent gate_2 episode reward: [-0.26820921]
All agents episode reward: [-0.26820921]
Agent gate_2 episode reward: [-0.29370747]
All agents episode reward: [-0.29370747]
Agent gate_2 episode reward: [-0.28021666]
All agents episode reward: [-0.28021666]
Agent gate_2 episode reward: [-0.2777621]
All agents episode reward: [-0.2777621]
Agent gate_2 episode reward: [-0.27696062]
All agents episode reward: [-0.27696062]
Agent gate_2 episode reward: [-0.2838777]
All agents episode reward: [-0.2838777]
Agent gate_2 episode reward: [-0.2983389]
All agents episode reward: [-0.2983389]
Iteration 4:  80%|████████  | 16/20 [00:31<00:07,  1.95s/it, episode=90, norm_ret=-0.302, true_ret=-14209861.000, steps=600]
Agent gate_2 episode reward: [-0.29253773]
All agents episode reward: [-0.29253773]
Agent gate_2 episode reward: [-0.29764456]
All agents episode reward: [-0.29764456]
Agent gate_2 episode reward: [-0.28726384]
All agents episode reward: [-0.28726384]
Agent gate_2 episode reward: [-0.33057421]
All agents episode reward: [-0.33057421]
Agent gate_2 episode reward: [-0.29358604]
All agents episode reward: [-0.29358604]
Agent gate_2 episode reward: [-0.29100947]
All agents episode reward: [-0.29100947]
Agent gate_2 episode reward: [-0.29342157]
All agents episode reward: [-0.29342157]
Agent gate_2 episode reward: [-0.32188174]
All agents episode reward: [-0.32188174]
Agent gate_2 episode reward: [-0.30052066]
All agents episode reward: [-0.30052066]
Agent gate_2 episode reward: [-0.30804541]
All agents episode reward: [-0.30804541]
Agent gate_2 episode reward: [-0.30903972]
All agents episode reward: [-0.30903972]
Agent gate_2 episode reward: [-0.30873334]
All agents episode reward: [-0.30873334]
Agent gate_2 episode reward: [-0.30797553]
All agents episode reward: [-0.30797553]
Agent gate_2 episode reward: [-0.32846161]
All agents episode reward: [-0.32846161]
Agent gate_2 episode reward: [-0.32546048]
All agents episode reward: [-0.32546048]
Agent gate_2 episode reward: [-0.32000408]
All agents episode reward: [-0.32000408]
Agent gate_2 episode reward: [-0.33164465]
All agents episode reward: [-0.33164465]
Agent gate_2 episode reward: [-0.33277306]
All agents episode reward: [-0.33277306]
Agent gate_2 episode reward: [-0.31922471]
All agents episode reward: [-0.31922471]
Agent gate_2 episode reward: [-0.33022673]
All agents episode reward: [-0.33022673]
Iteration 5:  60%|██████    | 12/20 [00:23<00:15,  1.95s/it, episode=110, norm_ret=-0.331, true_ret=-13982450.000, steps=600]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -14547443.000 at episode 101 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.33339058]
All agents episode reward: [-0.33339058]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -13867906.000 at episode 102 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.3192787]
All agents episode reward: [-0.3192787]
Agent gate_2 episode reward: [-0.32863096]
All agents episode reward: [-0.32863096]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -13830009.000 at episode 104 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.32138706]
All agents episode reward: [-0.32138706]
Agent gate_2 episode reward: [-0.33151576]
All agents episode reward: [-0.33151576]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -13707418.000 at episode 106 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.32148607]
All agents episode reward: [-0.32148607]
Agent gate_2 episode reward: [-0.32887107]
All agents episode reward: [-0.32887107]
Agent gate_2 episode reward: [-0.3515435]
All agents episode reward: [-0.3515435]
Agent gate_2 episode reward: [-0.33686039]
All agents episode reward: [-0.33686039]
Agent gate_2 episode reward: [-0.33385356]
All agents episode reward: [-0.33385356]
Agent gate_2 episode reward: [-0.33726696]
All agents episode reward: [-0.33726696]
Agent gate_2 episode reward: [-0.34456462]
All agents episode reward: [-0.34456462]
Agent gate_2 episode reward: [-0.35450401]
All agents episode reward: [-0.35450401]
Agent gate_2 episode reward: [-0.34374453]
All agents episode reward: [-0.34374453]
Agent gate_2 episode reward: [-0.36269431]
All agents episode reward: [-0.36269431]
Agent gate_2 episode reward: [-0.34330411]
All agents episode reward: [-0.34330411]
Agent gate_2 episode reward: [-0.3534917]
All agents episode reward: [-0.3534917]
Agent gate_2 episode reward: [-0.35458008]
All agents episode reward: [-0.35458008]
Agent gate_2 episode reward: [-0.36918686]
All agents episode reward: [-0.36918686]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -13692823.000 at episode 120 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.34103528]
All agents episode reward: [-0.34103528]
Iteration 6:  70%|███████   | 14/20 [00:26<00:11,  1.87s/it, episode=130, norm_ret=-0.357, true_ret=-14178547.000, steps=600]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -13576061.000 at episode 121 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.33950122]
All agents episode reward: [-0.33950122]
Agent gate_2 episode reward: [-0.35131105]
All agents episode reward: [-0.35131105]
Agent gate_2 episode reward: [-0.34293761]
All agents episode reward: [-0.34293761]
Agent gate_2 episode reward: [-0.35641653]
All agents episode reward: [-0.35641653]
Agent gate_2 episode reward: [-0.34583785]
All agents episode reward: [-0.34583785]
Agent gate_2 episode reward: [-0.37085382]
All agents episode reward: [-0.37085382]
Agent gate_2 episode reward: [-0.35529629]
All agents episode reward: [-0.35529629]
Agent gate_2 episode reward: [-0.36320257]
All agents episode reward: [-0.36320257]
Agent gate_2 episode reward: [-0.38195023]
All agents episode reward: [-0.38195023]
Agent gate_2 episode reward: [-0.36711854]
All agents episode reward: [-0.36711854]
Agent gate_2 episode reward: [-0.35830399]
All agents episode reward: [-0.35830399]
Agent gate_2 episode reward: [-0.36687324]
All agents episode reward: [-0.36687324]
Agent gate_2 episode reward: [-0.36826353]
All agents episode reward: [-0.36826353]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -13261337.000 at episode 134 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.34846893]
All agents episode reward: [-0.34846893]
Agent gate_2 episode reward: [-0.3721658]
All agents episode reward: [-0.3721658]
Agent gate_2 episode reward: [-0.37587171]
All agents episode reward: [-0.37587171]
Agent gate_2 episode reward: [-0.38264376]
All agents episode reward: [-0.38264376]
Agent gate_2 episode reward: [-0.374721]
All agents episode reward: [-0.374721]
Agent gate_2 episode reward: [-0.36727578]
All agents episode reward: [-0.36727578]
Agent gate_2 episode reward: [-0.37973521]
All agents episode reward: [-0.37973521]
Iteration 7:  80%|████████  | 16/20 [00:30<00:07,  1.89s/it, episode=150, norm_ret=-0.387, true_ret=-14538732.000, steps=600]
Agent gate_2 episode reward: [-0.37384933]
All agents episode reward: [-0.37384933]
Agent gate_2 episode reward: [-0.39544468]
All agents episode reward: [-0.39544468]
Agent gate_2 episode reward: [-0.37238559]
All agents episode reward: [-0.37238559]
Agent gate_2 episode reward: [-0.38600947]
All agents episode reward: [-0.38600947]
Agent gate_2 episode reward: [-0.37622457]
All agents episode reward: [-0.37622457]
Agent gate_2 episode reward: [-0.37895568]
All agents episode reward: [-0.37895568]
Agent gate_2 episode reward: [-0.41718907]
All agents episode reward: [-0.41718907]
Agent gate_2 episode reward: [-0.37394768]
All agents episode reward: [-0.37394768]
Agent gate_2 episode reward: [-0.39294261]
All agents episode reward: [-0.39294261]
Agent gate_2 episode reward: [-0.40361743]
All agents episode reward: [-0.40361743]
Agent gate_2 episode reward: [-0.38505069]
All agents episode reward: [-0.38505069]
Agent gate_2 episode reward: [-0.40364678]
All agents episode reward: [-0.40364678]
Agent gate_2 episode reward: [-0.38656073]
All agents episode reward: [-0.38656073]
Agent gate_2 episode reward: [-0.38360692]
All agents episode reward: [-0.38360692]
Agent gate_2 episode reward: [-0.41345261]
All agents episode reward: [-0.41345261]
Agent gate_2 episode reward: [-0.41066233]
All agents episode reward: [-0.41066233]
Agent gate_2 episode reward: [-0.42493942]
All agents episode reward: [-0.42493942]
Agent gate_2 episode reward: [-0.42063475]
All agents episode reward: [-0.42063475]
Agent gate_2 episode reward: [-0.41449727]
All agents episode reward: [-0.41449727]
Agent gate_2 episode reward: [-0.39434611]
All agents episode reward: [-0.39434611]
Iteration 8:  80%|████████  | 16/20 [00:32<00:07,  1.98s/it, episode=170, norm_ret=-0.404, true_ret=-13831393.000, steps=600]
Agent gate_2 episode reward: [-0.39431767]
All agents episode reward: [-0.39431767]
Agent gate_2 episode reward: [-0.39897354]
All agents episode reward: [-0.39897354]
Agent gate_2 episode reward: [-0.38495179]
All agents episode reward: [-0.38495179]
Agent gate_2 episode reward: [-0.40390097]
All agents episode reward: [-0.40390097]
Agent gate_2 episode reward: [-0.40758693]
All agents episode reward: [-0.40758693]
Agent gate_2 episode reward: [-0.40541623]
All agents episode reward: [-0.40541623]
Agent gate_2 episode reward: [-0.39693536]
All agents episode reward: [-0.39693536]
Agent gate_2 episode reward: [-0.42217793]
All agents episode reward: [-0.42217793]
Agent gate_2 episode reward: [-0.42105748]
All agents episode reward: [-0.42105748]
Agent gate_2 episode reward: [-0.40819928]
All agents episode reward: [-0.40819928]
Agent gate_2 episode reward: [-0.41428092]
All agents episode reward: [-0.41428092]
Agent gate_2 episode reward: [-0.40198706]
All agents episode reward: [-0.40198706]
Agent gate_2 episode reward: [-0.42331172]
All agents episode reward: [-0.42331172]
Agent gate_2 episode reward: [-0.44112329]
All agents episode reward: [-0.44112329]
Agent gate_2 episode reward: [-0.42350551]
All agents episode reward: [-0.42350551]
Agent gate_2 episode reward: [-0.42332477]
All agents episode reward: [-0.42332477]
Agent gate_2 episode reward: [-0.42555208]
All agents episode reward: [-0.42555208]
Agent gate_2 episode reward: [-0.40630595]
All agents episode reward: [-0.40630595]
Agent gate_2 episode reward: [-0.43655214]
All agents episode reward: [-0.43655214]
Agent gate_2 episode reward: [-0.40607833]
All agents episode reward: [-0.40607833]
Iteration 9:  80%|████████  | 16/20 [00:30<00:07,  1.89s/it, episode=190, norm_ret=-0.429, true_ret=-13717913.000, steps=600]
Agent gate_2 episode reward: [-0.43792709]
All agents episode reward: [-0.43792709]
Agent gate_2 episode reward: [-0.42391741]
All agents episode reward: [-0.42391741]
Agent gate_2 episode reward: [-0.40613723]
All agents episode reward: [-0.40613723]
Agent gate_2 episode reward: [-0.42034077]
All agents episode reward: [-0.42034077]
Agent gate_2 episode reward: [-0.41901407]
All agents episode reward: [-0.41901407]
Agent gate_2 episode reward: [-0.45104943]
All agents episode reward: [-0.45104943]
Agent gate_2 episode reward: [-0.42496187]
All agents episode reward: [-0.42496187]
Agent gate_2 episode reward: [-0.43551167]
All agents episode reward: [-0.43551167]
Agent gate_2 episode reward: [-0.44095632]
All agents episode reward: [-0.44095632]
Agent gate_2 episode reward: [-0.42748429]
All agents episode reward: [-0.42748429]
Agent gate_2 episode reward: [-0.44372139]
All agents episode reward: [-0.44372139]
Agent gate_2 episode reward: [-0.46269553]
All agents episode reward: [-0.46269553]
Agent gate_2 episode reward: [-0.44629436]
All agents episode reward: [-0.44629436]
Agent gate_2 episode reward: [-0.4561526]
All agents episode reward: [-0.4561526]
Agent gate_2 episode reward: [-0.43236647]
All agents episode reward: [-0.43236647]
Agent gate_2 episode reward: [-0.47338046]
All agents episode reward: [-0.47338046]
Agent gate_2 episode reward: [-0.43919192]
All agents episode reward: [-0.43919192]
Agent gate_2 episode reward: [-0.45617571]
All agents episode reward: [-0.45617571]
Agent gate_2 episode reward: [-0.44380484]
All agents episode reward: [-0.44380484]
Agent gate_2 episode reward: [-0.47175224]
All agents episode reward: [-0.47175224]
Loaded 1 agents from ppo_agents_butterfly_scB
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scB/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scB/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scB/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scB/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scB/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scB/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scB/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scB/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scB/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scB/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -61890232.000 ± 18496594.000
  Average reward: -61890232.000 ± 18496594.000
  Total reward: -61890232.000 ± 18496594.000
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scB/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scB/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scB/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scB/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scB/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scB/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scB/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scB/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scB/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scB/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -14440154.000 ± 626384.375
  Average reward: -14440154.000 ± 626384.375
  Total reward: -14440154.000 ± 626384.375
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scB/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scB/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scB/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scB/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scB/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scB/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scB/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scB/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scB/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scB/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -14275424.000 ± 464159.531
  Average reward: -14275424.000 ± 464159.531
  Total reward: -14275424.000 ± 464159.531
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -61890232.000
Rule-based avg reward: -14440154.000
No control avg reward: -14275424.000
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
