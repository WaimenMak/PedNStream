Iteration 0:  80%|████████  | 16/20 [00:32<00:08,  2.08s/it, episode=10, norm_ret=-12.020, true_ret=-69251.727, steps=600]
Agent gate_2 episode reward: [-108.67766887]
All agents episode reward: [-108.67766887]
Agent gate_2 episode reward: [-2.84496205]
All agents episode reward: [-2.84496205]
Agent gate_2 episode reward: [-4.29110049]
All agents episode reward: [-4.29110049]
Agent gate_2 episode reward: [-0.94991176]
All agents episode reward: [-0.94991176]
Agent gate_2 episode reward: [-0.38855125]
All agents episode reward: [-0.38855125]
Agent gate_2 episode reward: [-0.94861155]
All agents episode reward: [-0.94861155]
Agent gate_2 episode reward: [-1.20286284]
All agents episode reward: [-1.20286284]
Agent gate_2 episode reward: [-0.39846404]
All agents episode reward: [-0.39846404]
Agent gate_2 episode reward: [-0.27952933]
All agents episode reward: [-0.27952933]
Agent gate_2 episode reward: [-0.21391218]
All agents episode reward: [-0.21391218]
Agent gate_2 episode reward: [-0.48827834]
All agents episode reward: [-0.48827834]
Agent gate_2 episode reward: [-0.29734721]
All agents episode reward: [-0.29734721]
Agent gate_2 episode reward: [-0.87145667]
All agents episode reward: [-0.87145667]
Agent gate_2 episode reward: [-0.30523572]
All agents episode reward: [-0.30523572]
Agent gate_2 episode reward: [-0.64862068]
All agents episode reward: [-0.64862068]
Agent gate_2 episode reward: [-0.39911668]
All agents episode reward: [-0.39911668]
Agent gate_2 episode reward: [-0.42025437]
All agents episode reward: [-0.42025437]
Agent gate_2 episode reward: [-0.42538669]
All agents episode reward: [-0.42538669]
Agent gate_2 episode reward: [-0.35053844]
All agents episode reward: [-0.35053844]
Agent gate_2 episode reward: [-0.36324677]
All agents episode reward: [-0.36324677]
Iteration 1:  80%|████████  | 16/20 [00:33<00:08,  2.15s/it, episode=30, norm_ret=-0.477, true_ret=-87118.023, steps=600]
Agent gate_2 episode reward: [-0.4281771]
All agents episode reward: [-0.4281771]
Agent gate_2 episode reward: [-0.89690903]
All agents episode reward: [-0.89690903]
Agent gate_2 episode reward: [-0.52936569]
All agents episode reward: [-0.52936569]
Agent gate_2 episode reward: [-0.3897694]
All agents episode reward: [-0.3897694]
Agent gate_2 episode reward: [-0.36332372]
All agents episode reward: [-0.36332372]
Agent gate_2 episode reward: [-0.43343784]
All agents episode reward: [-0.43343784]
Agent gate_2 episode reward: [-0.38995626]
All agents episode reward: [-0.38995626]
Agent gate_2 episode reward: [-0.47281847]
All agents episode reward: [-0.47281847]
Agent gate_2 episode reward: [-0.42469578]
All agents episode reward: [-0.42469578]
Agent gate_2 episode reward: [-0.44350446]
All agents episode reward: [-0.44350446]
Agent gate_2 episode reward: [-0.47922483]
All agents episode reward: [-0.47922483]
Agent gate_2 episode reward: [-0.4354032]
All agents episode reward: [-0.4354032]
Agent gate_2 episode reward: [-0.43101054]
All agents episode reward: [-0.43101054]
Agent gate_2 episode reward: [-0.41958751]
All agents episode reward: [-0.41958751]
Agent gate_2 episode reward: [-0.4345926]
All agents episode reward: [-0.4345926]
Agent gate_2 episode reward: [-0.45951906]
All agents episode reward: [-0.45951906]
Agent gate_2 episode reward: [-0.4237662]
All agents episode reward: [-0.4237662]
Agent gate_2 episode reward: [-0.45162717]
All agents episode reward: [-0.45162717]
Agent gate_2 episode reward: [-0.49299214]
All agents episode reward: [-0.49299214]
Agent gate_2 episode reward: [-0.452089]
All agents episode reward: [-0.452089]
Iteration 2:  80%|████████  | 16/20 [00:33<00:08,  2.03s/it, episode=50, norm_ret=-0.508, true_ret=-76300.586, steps=600]
Agent gate_2 episode reward: [-0.48097024]
All agents episode reward: [-0.48097024]
Agent gate_2 episode reward: [-0.4749704]
All agents episode reward: [-0.4749704]
Agent gate_2 episode reward: [-0.54026992]
All agents episode reward: [-0.54026992]
Agent gate_2 episode reward: [-0.50068461]
All agents episode reward: [-0.50068461]
Agent gate_2 episode reward: [-0.49363213]
All agents episode reward: [-0.49363213]
Agent gate_2 episode reward: [-0.51502317]
All agents episode reward: [-0.51502317]
Agent gate_2 episode reward: [-0.57648385]
All agents episode reward: [-0.57648385]
Agent gate_2 episode reward: [-0.53030707]
All agents episode reward: [-0.53030707]
Agent gate_2 episode reward: [-0.47630238]
All agents episode reward: [-0.47630238]
Agent gate_2 episode reward: [-0.49619986]
All agents episode reward: [-0.49619986]
Agent gate_2 episode reward: [-0.56320385]
All agents episode reward: [-0.56320385]
Agent gate_2 episode reward: [-0.51758699]
All agents episode reward: [-0.51758699]
Agent gate_2 episode reward: [-0.53183187]
All agents episode reward: [-0.53183187]
Agent gate_2 episode reward: [-0.58271915]
All agents episode reward: [-0.58271915]
Agent gate_2 episode reward: [-0.5692331]
All agents episode reward: [-0.5692331]
Agent gate_2 episode reward: [-0.5446464]
All agents episode reward: [-0.5446464]
Agent gate_2 episode reward: [-0.58577564]
All agents episode reward: [-0.58577564]
Agent gate_2 episode reward: [-0.56446152]
All agents episode reward: [-0.56446152]
Agent gate_2 episode reward: [-0.59205699]
All agents episode reward: [-0.59205699]
Agent gate_2 episode reward: [-0.61996883]
All agents episode reward: [-0.61996883]
Iteration 3:  80%|████████  | 16/20 [00:32<00:08,  2.00s/it, episode=70, norm_ret=-0.600, true_ret=-79325.031, steps=600]
Agent gate_2 episode reward: [-0.60816121]
All agents episode reward: [-0.60816121]
Agent gate_2 episode reward: [-0.58606943]
All agents episode reward: [-0.58606943]
Agent gate_2 episode reward: [-0.57859902]
All agents episode reward: [-0.57859902]
Agent gate_2 episode reward: [-0.59086595]
All agents episode reward: [-0.59086595]
Agent gate_2 episode reward: [-0.61485078]
All agents episode reward: [-0.61485078]
Agent gate_2 episode reward: [-0.58796473]
All agents episode reward: [-0.58796473]
Agent gate_2 episode reward: [-0.60133852]
All agents episode reward: [-0.60133852]
Agent gate_2 episode reward: [-0.61274196]
All agents episode reward: [-0.61274196]
Agent gate_2 episode reward: [-0.60684169]
All agents episode reward: [-0.60684169]
Agent gate_2 episode reward: [-0.60774277]
All agents episode reward: [-0.60774277]
Agent gate_2 episode reward: [-0.61116095]
All agents episode reward: [-0.61116095]
Agent gate_2 episode reward: [-0.61453377]
All agents episode reward: [-0.61453377]
Agent gate_2 episode reward: [-0.69534308]
All agents episode reward: [-0.69534308]
Agent gate_2 episode reward: [-0.65361854]
All agents episode reward: [-0.65361854]
Agent gate_2 episode reward: [-0.65491606]
All agents episode reward: [-0.65491606]
Agent gate_2 episode reward: [-0.67511603]
All agents episode reward: [-0.67511603]
Agent gate_2 episode reward: [-0.66201439]
All agents episode reward: [-0.66201439]
Agent gate_2 episode reward: [-0.68128755]
All agents episode reward: [-0.68128755]
Agent gate_2 episode reward: [-0.68814649]
All agents episode reward: [-0.68814649]
Agent gate_2 episode reward: [-0.63634802]
All agents episode reward: [-0.63634802]
Iteration 4:  80%|████████  | 16/20 [00:31<00:07,  1.98s/it, episode=90, norm_ret=-0.701, true_ret=-78747.461, steps=600]
Agent gate_2 episode reward: [-0.70986956]
All agents episode reward: [-0.70986956]
Agent gate_2 episode reward: [-0.64403036]
All agents episode reward: [-0.64403036]
Agent gate_2 episode reward: [-0.71407171]
All agents episode reward: [-0.71407171]
Agent gate_2 episode reward: [-0.71010235]
All agents episode reward: [-0.71010235]
Agent gate_2 episode reward: [-0.71992164]
All agents episode reward: [-0.71992164]
Agent gate_2 episode reward: [-0.65753573]
All agents episode reward: [-0.65753573]
Agent gate_2 episode reward: [-0.71099858]
All agents episode reward: [-0.71099858]
Agent gate_2 episode reward: [-0.74813335]
All agents episode reward: [-0.74813335]
Agent gate_2 episode reward: [-0.71129412]
All agents episode reward: [-0.71129412]
Agent gate_2 episode reward: [-0.68233881]
All agents episode reward: [-0.68233881]
Agent gate_2 episode reward: [-0.71215899]
All agents episode reward: [-0.71215899]
Agent gate_2 episode reward: [-0.71830512]
All agents episode reward: [-0.71830512]
Agent gate_2 episode reward: [-0.68912217]
All agents episode reward: [-0.68912217]
Agent gate_2 episode reward: [-0.72395136]
All agents episode reward: [-0.72395136]
Agent gate_2 episode reward: [-0.71894435]
All agents episode reward: [-0.71894435]
Agent gate_2 episode reward: [-0.78826956]
All agents episode reward: [-0.78826956]
Agent gate_2 episode reward: [-0.78797062]
All agents episode reward: [-0.78797062]
Agent gate_2 episode reward: [-0.69873964]
All agents episode reward: [-0.69873964]
Agent gate_2 episode reward: [-0.7596792]
All agents episode reward: [-0.7596792]
Agent gate_2 episode reward: [-0.6869104]
All agents episode reward: [-0.6869104]
Iteration 5:  70%|███████   | 14/20 [00:28<00:12,  2.10s/it, episode=110, norm_ret=-0.782, true_ret=-85533.594, steps=600]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -84371.750 at episode 101 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.7735567]
All agents episode reward: [-0.7735567]
Agent gate_2 episode reward: [-0.80531661]
All agents episode reward: [-0.80531661]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -78735.578 at episode 103 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.72889517]
All agents episode reward: [-0.72889517]
Agent gate_2 episode reward: [-0.76054914]
All agents episode reward: [-0.76054914]
Agent gate_2 episode reward: [-0.8280388]
All agents episode reward: [-0.8280388]
Agent gate_2 episode reward: [-0.75766542]
All agents episode reward: [-0.75766542]
Agent gate_2 episode reward: [-0.75849988]
All agents episode reward: [-0.75849988]
Agent gate_2 episode reward: [-0.8079764]
All agents episode reward: [-0.8079764]
Agent gate_2 episode reward: [-0.78522259]
All agents episode reward: [-0.78522259]
Agent gate_2 episode reward: [-0.8178298]
All agents episode reward: [-0.8178298]
Agent gate_2 episode reward: [-0.83358719]
All agents episode reward: [-0.83358719]
Agent gate_2 episode reward: [-0.77057859]
All agents episode reward: [-0.77057859]
Agent gate_2 episode reward: [-0.81476104]
All agents episode reward: [-0.81476104]
Agent gate_2 episode reward: [-0.78929913]
All agents episode reward: [-0.78929913]
Agent gate_2 episode reward: [-0.82390142]
All agents episode reward: [-0.82390142]
Agent gate_2 episode reward: [-0.85759812]
All agents episode reward: [-0.85759812]
Agent gate_2 episode reward: [-0.8426338]
All agents episode reward: [-0.8426338]
Agent gate_2 episode reward: [-0.79443865]
All agents episode reward: [-0.79443865]
Agent gate_2 episode reward: [-0.80775818]
All agents episode reward: [-0.80775818]
Agent gate_2 episode reward: [-0.90364758]
All agents episode reward: [-0.90364758]
Iteration 6:  75%|███████▌  | 15/20 [00:30<00:10,  2.14s/it, episode=130, norm_ret=-0.841, true_ret=-87367.719, steps=600]
Agent gate_2 episode reward: [-0.87158459]
All agents episode reward: [-0.87158459]
Agent gate_2 episode reward: [-0.84198157]
All agents episode reward: [-0.84198157]
Agent gate_2 episode reward: [-0.80428525]
All agents episode reward: [-0.80428525]
Agent gate_2 episode reward: [-0.82406984]
All agents episode reward: [-0.82406984]
Agent gate_2 episode reward: [-0.82068014]
All agents episode reward: [-0.82068014]
Agent gate_2 episode reward: [-0.8361873]
All agents episode reward: [-0.8361873]
Agent gate_2 episode reward: [-0.82561079]
All agents episode reward: [-0.82561079]
Agent gate_2 episode reward: [-0.89157272]
All agents episode reward: [-0.89157272]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -76340.938 at episode 129 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.78941168]
All agents episode reward: [-0.78941168]
Agent gate_2 episode reward: [-0.90683531]
All agents episode reward: [-0.90683531]
Agent gate_2 episode reward: [-0.88492198]
All agents episode reward: [-0.88492198]
Agent gate_2 episode reward: [-0.81374474]
All agents episode reward: [-0.81374474]
Agent gate_2 episode reward: [-0.82470473]
All agents episode reward: [-0.82470473]
Agent gate_2 episode reward: [-0.87149823]
All agents episode reward: [-0.87149823]
Agent gate_2 episode reward: [-0.81899055]
All agents episode reward: [-0.81899055]
Agent gate_2 episode reward: [-0.82304108]
All agents episode reward: [-0.82304108]
Agent gate_2 episode reward: [-0.8788405]
All agents episode reward: [-0.8788405]
Agent gate_2 episode reward: [-0.92096351]
All agents episode reward: [-0.92096351]
Agent gate_2 episode reward: [-0.89774162]
All agents episode reward: [-0.89774162]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -76259.617 at episode 140 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.82094305]
All agents episode reward: [-0.82094305]
Iteration 7:  80%|████████  | 16/20 [00:34<00:08,  2.24s/it, episode=150, norm_ret=-0.889, true_ret=-82702.914, steps=600]
Agent gate_2 episode reward: [-0.85314146]
All agents episode reward: [-0.85314146]
Agent gate_2 episode reward: [-0.86296555]
All agents episode reward: [-0.86296555]
Agent gate_2 episode reward: [-0.83471437]
All agents episode reward: [-0.83471437]
Agent gate_2 episode reward: [-0.83730792]
All agents episode reward: [-0.83730792]
Agent gate_2 episode reward: [-0.91996508]
All agents episode reward: [-0.91996508]
Agent gate_2 episode reward: [-0.95982723]
All agents episode reward: [-0.95982723]
Agent gate_2 episode reward: [-0.8698121]
All agents episode reward: [-0.8698121]
Agent gate_2 episode reward: [-0.95441203]
All agents episode reward: [-0.95441203]
Agent gate_2 episode reward: [-0.88166521]
All agents episode reward: [-0.88166521]
Agent gate_2 episode reward: [-0.92105691]
All agents episode reward: [-0.92105691]
Agent gate_2 episode reward: [-0.90366854]
All agents episode reward: [-0.90366854]
Agent gate_2 episode reward: [-0.86279248]
All agents episode reward: [-0.86279248]
Agent gate_2 episode reward: [-0.89011505]
All agents episode reward: [-0.89011505]
Agent gate_2 episode reward: [-0.96346]
All agents episode reward: [-0.96346]
Agent gate_2 episode reward: [-0.94072069]
All agents episode reward: [-0.94072069]
Agent gate_2 episode reward: [-0.90068319]
All agents episode reward: [-0.90068319]
Agent gate_2 episode reward: [-0.89423159]
All agents episode reward: [-0.89423159]
Agent gate_2 episode reward: [-0.99890411]
All agents episode reward: [-0.99890411]
Agent gate_2 episode reward: [-0.88380262]
All agents episode reward: [-0.88380262]
Agent gate_2 episode reward: [-0.90674725]
All agents episode reward: [-0.90674725]
Iteration 8:  80%|████████  | 16/20 [00:33<00:08,  2.04s/it, episode=170, norm_ret=-0.949, true_ret=-78944.258, steps=600]
Agent gate_2 episode reward: [-0.9042056]
All agents episode reward: [-0.9042056]
Agent gate_2 episode reward: [-0.99927109]
All agents episode reward: [-0.99927109]
Agent gate_2 episode reward: [-0.95440412]
All agents episode reward: [-0.95440412]
Agent gate_2 episode reward: [-0.93971346]
All agents episode reward: [-0.93971346]
Agent gate_2 episode reward: [-0.9024001]
All agents episode reward: [-0.9024001]
Agent gate_2 episode reward: [-0.92169272]
All agents episode reward: [-0.92169272]
Agent gate_2 episode reward: [-0.93399777]
All agents episode reward: [-0.93399777]
Agent gate_2 episode reward: [-0.97634418]
All agents episode reward: [-0.97634418]
Agent gate_2 episode reward: [-1.02792281]
All agents episode reward: [-1.02792281]
Agent gate_2 episode reward: [-0.93499318]
All agents episode reward: [-0.93499318]
Agent gate_2 episode reward: [-0.95537171]
All agents episode reward: [-0.95537171]
Agent gate_2 episode reward: [-0.9925078]
All agents episode reward: [-0.9925078]
Agent gate_2 episode reward: [-0.93944607]
All agents episode reward: [-0.93944607]
Agent gate_2 episode reward: [-1.0044941]
All agents episode reward: [-1.0044941]
Agent gate_2 episode reward: [-0.98474444]
All agents episode reward: [-0.98474444]
Agent gate_2 episode reward: [-0.99511762]
All agents episode reward: [-0.99511762]
Agent gate_2 episode reward: [-1.06428099]
All agents episode reward: [-1.06428099]
Agent gate_2 episode reward: [-0.94104024]
All agents episode reward: [-0.94104024]
Agent gate_2 episode reward: [-1.04704344]
All agents episode reward: [-1.04704344]
Agent gate_2 episode reward: [-0.96286136]
All agents episode reward: [-0.96286136]
Iteration 9:  70%|███████   | 14/20 [00:28<00:12,  2.05s/it, episode=190, norm_ret=-1.006, true_ret=-87406.711, steps=600]
Agent gate_2 episode reward: [-0.97993102]
All agents episode reward: [-0.97993102]
Agent gate_2 episode reward: [-0.98124591]
All agents episode reward: [-0.98124591]
Agent gate_2 episode reward: [-0.98062996]
All agents episode reward: [-0.98062996]
Agent gate_2 episode reward: [-0.99692368]
All agents episode reward: [-0.99692368]
Agent gate_2 episode reward: [-0.99792291]
All agents episode reward: [-0.99792291]
Agent gate_2 episode reward: [-1.10706777]
All agents episode reward: [-1.10706777]
Agent gate_2 episode reward: [-0.99626381]
All agents episode reward: [-0.99626381]
Agent gate_2 episode reward: [-0.96702221]
All agents episode reward: [-0.96702221]
Agent gate_2 episode reward: [-0.9628257]
All agents episode reward: [-0.9628257]
Agent gate_2 episode reward: [-1.09345588]
All agents episode reward: [-1.09345588]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -76031.352 at episode 191 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.95362842]
All agents episode reward: [-0.95362842]
Agent gate_2 episode reward: [-0.99674771]
All agents episode reward: [-0.99674771]
Agent gate_2 episode reward: [-1.0328073]
All agents episode reward: [-1.0328073]
Agent gate_2 episode reward: [-1.02383678]
All agents episode reward: [-1.02383678]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -75583.148 at episode 195 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.95769713]
All agents episode reward: [-0.95769713]
Agent gate_2 episode reward: [-1.05065158]
All agents episode reward: [-1.05065158]
Agent gate_2 episode reward: [-1.06411904]
All agents episode reward: [-1.06411904]
Agent gate_2 episode reward: [-1.09483834]
All agents episode reward: [-1.09483834]
Agent gate_2 episode reward: [-0.98509546]
All agents episode reward: [-0.98509546]
Agent gate_2 episode reward: [-1.09329758]
All agents episode reward: [-1.09329758]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -203088.047 ± 125728.312
  Average reward: -203088.047 ± 125728.312
  Total reward: -203088.047 ± 125728.312
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -81071.562 ± 2266.719
  Average reward: -81071.562 ± 2266.719
  Total reward: -81071.562 ± 2266.719
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -81069.000 ± 2673.414
  Average reward: -81069.000 ± 2673.414
  Total reward: -81069.000 ± 2673.414
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -203088.047
Rule-based avg reward: -81071.562
No control avg reward: -81069.000
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
