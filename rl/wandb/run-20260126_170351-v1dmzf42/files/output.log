Iteration 0: 100%|██████████| 10/10 [00:18<00:00,  1.80s/it, episode=10, norm_ret=-16.787, true_ret=-82052.312, steps=600]
Agent gate_2 episode reward: [-80.32584995]
All agents episode reward: [-80.32584995]
Agent gate_2 episode reward: [-21.88328662]
All agents episode reward: [-21.88328662]
Agent gate_2 episode reward: [-7.56533656]
All agents episode reward: [-7.56533656]
Agent gate_2 episode reward: [-7.17966147]
All agents episode reward: [-7.17966147]
Agent gate_2 episode reward: [-7.81198112]
All agents episode reward: [-7.81198112]
Agent gate_2 episode reward: [-7.96063345]
All agents episode reward: [-7.96063345]
Agent gate_2 episode reward: [-8.80572378]
All agents episode reward: [-8.80572378]
Agent gate_2 episode reward: [-8.63949585]
All agents episode reward: [-8.63949585]
Agent gate_2 episode reward: [-8.68818708]
All agents episode reward: [-8.68818708]
Agent gate_2 episode reward: [-9.0120808]
All agents episode reward: [-9.0120808]
Iteration 1: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it, episode=20, norm_ret=-12.771, true_ret=-308236.531, steps=600]
Agent gate_2 episode reward: [-9.19125286]
All agents episode reward: [-9.19125286]
Agent gate_2 episode reward: [-8.65837033]
All agents episode reward: [-8.65837033]
Agent gate_2 episode reward: [-9.51662273]
All agents episode reward: [-9.51662273]
Agent gate_2 episode reward: [-9.11980257]
All agents episode reward: [-9.11980257]
Agent gate_2 episode reward: [-9.59379762]
All agents episode reward: [-9.59379762]
Agent gate_2 episode reward: [-10.44436053]
All agents episode reward: [-10.44436053]
Agent gate_2 episode reward: [-37.30885256]
All agents episode reward: [-37.30885256]
Agent gate_2 episode reward: [-7.80036217]
All agents episode reward: [-7.80036217]
Agent gate_2 episode reward: [-12.57503947]
All agents episode reward: [-12.57503947]
Agent gate_2 episode reward: [-13.49970533]
All agents episode reward: [-13.49970533]
Iteration 2: 100%|██████████| 10/10 [00:16<00:00,  1.68s/it, episode=30, norm_ret=-4.367, true_ret=-74493.062, steps=600]
Agent gate_2 episode reward: [-6.75051793]
All agents episode reward: [-6.75051793]
Agent gate_2 episode reward: [-8.13738435]
All agents episode reward: [-8.13738435]
Agent gate_2 episode reward: [-3.20215384]
All agents episode reward: [-3.20215384]
Agent gate_2 episode reward: [-3.40039463]
All agents episode reward: [-3.40039463]
Agent gate_2 episode reward: [-3.43675016]
All agents episode reward: [-3.43675016]
Agent gate_2 episode reward: [-3.73232783]
All agents episode reward: [-3.73232783]
Agent gate_2 episode reward: [-4.43103642]
All agents episode reward: [-4.43103642]
Agent gate_2 episode reward: [-3.59196959]
All agents episode reward: [-3.59196959]
Agent gate_2 episode reward: [-3.63271103]
All agents episode reward: [-3.63271103]
Agent gate_2 episode reward: [-3.35508765]
All agents episode reward: [-3.35508765]
Iteration 3: 100%|██████████| 10/10 [00:16<00:00,  1.68s/it, episode=40, norm_ret=-3.934, true_ret=-76614.664, steps=600]
Agent gate_2 episode reward: [-3.49128836]
All agents episode reward: [-3.49128836]
Agent gate_2 episode reward: [-3.55744191]
All agents episode reward: [-3.55744191]
Agent gate_2 episode reward: [-4.06794618]
All agents episode reward: [-4.06794618]
Agent gate_2 episode reward: [-3.92523907]
All agents episode reward: [-3.92523907]
Agent gate_2 episode reward: [-3.99596347]
All agents episode reward: [-3.99596347]
Agent gate_2 episode reward: [-4.1721402]
All agents episode reward: [-4.1721402]
Agent gate_2 episode reward: [-4.05329302]
All agents episode reward: [-4.05329302]
Agent gate_2 episode reward: [-4.17216642]
All agents episode reward: [-4.17216642]
Agent gate_2 episode reward: [-4.01754159]
All agents episode reward: [-4.01754159]
Agent gate_2 episode reward: [-3.89051468]
All agents episode reward: [-3.89051468]
Iteration 4: 100%|██████████| 10/10 [00:17<00:00,  1.76s/it, episode=50, norm_ret=-4.364, true_ret=-84491.031, steps=600]
Agent gate_2 episode reward: [-4.42935609]
All agents episode reward: [-4.42935609]
Agent gate_2 episode reward: [-4.13513407]
All agents episode reward: [-4.13513407]
Agent gate_2 episode reward: [-4.1589006]
All agents episode reward: [-4.1589006]
Agent gate_2 episode reward: [-4.1419605]
All agents episode reward: [-4.1419605]
Agent gate_2 episode reward: [-4.25868343]
All agents episode reward: [-4.25868343]
Agent gate_2 episode reward: [-4.6775057]
All agents episode reward: [-4.6775057]
Agent gate_2 episode reward: [-4.29508062]
All agents episode reward: [-4.29508062]
Agent gate_2 episode reward: [-4.51484877]
All agents episode reward: [-4.51484877]
Agent gate_2 episode reward: [-4.32874387]
All agents episode reward: [-4.32874387]
Agent gate_2 episode reward: [-4.69663243]
All agents episode reward: [-4.69663243]
Iteration 5: 100%|██████████| 10/10 [00:17<00:00,  1.78s/it, episode=60, norm_ret=-4.747, true_ret=-79330.453, steps=600]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -78059.625 at episode 51 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.37322898]
All agents episode reward: [-4.37322898]
Agent gate_2 episode reward: [-4.55982776]
All agents episode reward: [-4.55982776]
Agent gate_2 episode reward: [-4.50811443]
All agents episode reward: [-4.50811443]
Agent gate_2 episode reward: [-5.09033394]
All agents episode reward: [-5.09033394]
Agent gate_2 episode reward: [-4.90043212]
All agents episode reward: [-4.90043212]
Agent gate_2 episode reward: [-5.00007333]
All agents episode reward: [-5.00007333]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -75964.117 at episode 57 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.44550618]
All agents episode reward: [-4.44550618]
Agent gate_2 episode reward: [-4.55261386]
All agents episode reward: [-4.55261386]
Agent gate_2 episode reward: [-5.30170372]
All agents episode reward: [-5.30170372]
Agent gate_2 episode reward: [-4.73495278]
All agents episode reward: [-4.73495278]
Iteration 6: 100%|██████████| 10/10 [00:17<00:00,  1.78s/it, episode=70, norm_ret=-4.893, true_ret=-81890.016, steps=600]
Agent gate_2 episode reward: [-4.98664637]
All agents episode reward: [-4.98664637]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -74401.578 at episode 62 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.49839553]
All agents episode reward: [-4.49839553]
Agent gate_2 episode reward: [-4.75480808]
All agents episode reward: [-4.75480808]
Agent gate_2 episode reward: [-4.58864782]
All agents episode reward: [-4.58864782]
Agent gate_2 episode reward: [-4.95413535]
All agents episode reward: [-4.95413535]
Agent gate_2 episode reward: [-4.77311057]
All agents episode reward: [-4.77311057]
Agent gate_2 episode reward: [-4.86731114]
All agents episode reward: [-4.86731114]
Agent gate_2 episode reward: [-5.16739724]
All agents episode reward: [-5.16739724]
Agent gate_2 episode reward: [-5.15077561]
All agents episode reward: [-5.15077561]
Agent gate_2 episode reward: [-5.18644783]
All agents episode reward: [-5.18644783]
Iteration 7: 100%|██████████| 10/10 [00:18<00:00,  1.85s/it, episode=80, norm_ret=-6.458, true_ret=-128368.141, steps=600]
Agent gate_2 episode reward: [-4.99258921]
All agents episode reward: [-4.99258921]
Agent gate_2 episode reward: [-7.39501848]
All agents episode reward: [-7.39501848]
Agent gate_2 episode reward: [-6.00887265]
All agents episode reward: [-6.00887265]
Agent gate_2 episode reward: [-5.98516557]
All agents episode reward: [-5.98516557]
Agent gate_2 episode reward: [-6.17448355]
All agents episode reward: [-6.17448355]
Agent gate_2 episode reward: [-5.92875106]
All agents episode reward: [-5.92875106]
Agent gate_2 episode reward: [-6.88327971]
All agents episode reward: [-6.88327971]
Agent gate_2 episode reward: [-6.2991432]
All agents episode reward: [-6.2991432]
Agent gate_2 episode reward: [-6.42728949]
All agents episode reward: [-6.42728949]
Agent gate_2 episode reward: [-8.48874194]
All agents episode reward: [-8.48874194]
Iteration 8: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it, episode=90, norm_ret=-7.938, true_ret=-163975.016, steps=600]
Agent gate_2 episode reward: [-7.25580882]
All agents episode reward: [-7.25580882]
Agent gate_2 episode reward: [-7.10454385]
All agents episode reward: [-7.10454385]
Agent gate_2 episode reward: [-6.64463342]
All agents episode reward: [-6.64463342]
Agent gate_2 episode reward: [-8.99105933]
All agents episode reward: [-8.99105933]
Agent gate_2 episode reward: [-6.91087102]
All agents episode reward: [-6.91087102]
Agent gate_2 episode reward: [-6.33813393]
All agents episode reward: [-6.33813393]
Agent gate_2 episode reward: [-7.62122266]
All agents episode reward: [-7.62122266]
Agent gate_2 episode reward: [-8.72929991]
All agents episode reward: [-8.72929991]
Agent gate_2 episode reward: [-8.72488037]
All agents episode reward: [-8.72488037]
Agent gate_2 episode reward: [-11.06080904]
All agents episode reward: [-11.06080904]
Iteration 9: 100%|██████████| 10/10 [00:17<00:00,  1.74s/it, episode=100, norm_ret=-8.834, true_ret=-136596.391, steps=600]
Agent gate_2 episode reward: [-7.67637146]
All agents episode reward: [-7.67637146]
Agent gate_2 episode reward: [-9.01269104]
All agents episode reward: [-9.01269104]
Agent gate_2 episode reward: [-9.43259298]
All agents episode reward: [-9.43259298]
Agent gate_2 episode reward: [-8.64137069]
All agents episode reward: [-8.64137069]
Agent gate_2 episode reward: [-8.36089394]
All agents episode reward: [-8.36089394]
Agent gate_2 episode reward: [-10.52502753]
All agents episode reward: [-10.52502753]
Agent gate_2 episode reward: [-7.84089305]
All agents episode reward: [-7.84089305]
Agent gate_2 episode reward: [-8.2429845]
All agents episode reward: [-8.2429845]
Agent gate_2 episode reward: [-9.35682428]
All agents episode reward: [-9.35682428]
Agent gate_2 episode reward: [-9.25392883]
All agents episode reward: [-9.25392883]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -805849.625 ± 467291.875
  Average reward: -805849.625 ± 467291.875
  Total reward: -805849.625 ± 467291.875
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -82085.523 ± 3526.649
  Average reward: -82085.523 ± 3526.649
  Total reward: -82085.523 ± 3526.649
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -82002.484 ± 3338.270
  Average reward: -82002.484 ± 3338.270
  Total reward: -82002.484 ± 3338.270
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -805849.625
Rule-based avg reward: -82085.523
No control avg reward: -82002.484
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
