Iteration 0: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it, episode=10, norm_ret=-13.375, true_ret=-77326.180, steps=600]
Agent gate_2 episode reward: [-128.02099394]
All agents episode reward: [-128.02099394]
Agent gate_2 episode reward: [-1.10651648]
All agents episode reward: [-1.10651648]
Agent gate_2 episode reward: [-1.16592711]
All agents episode reward: [-1.16592711]
Agent gate_2 episode reward: [-0.33980225]
All agents episode reward: [-0.33980225]
Agent gate_2 episode reward: [-0.4532671]
All agents episode reward: [-0.4532671]
Agent gate_2 episode reward: [-0.64795621]
All agents episode reward: [-0.64795621]
Agent gate_2 episode reward: [-0.54911235]
All agents episode reward: [-0.54911235]
Agent gate_2 episode reward: [-0.55768351]
All agents episode reward: [-0.55768351]
Agent gate_2 episode reward: [-0.46317509]
All agents episode reward: [-0.46317509]
Agent gate_2 episode reward: [-0.44949429]
All agents episode reward: [-0.44949429]
Iteration 1: 100%|██████████| 10/10 [00:22<00:00,  2.26s/it, episode=20, norm_ret=-0.631, true_ret=-83020.891, steps=600]
Agent gate_2 episode reward: [-0.49926261]
All agents episode reward: [-0.49926261]
Agent gate_2 episode reward: [-0.56550414]
All agents episode reward: [-0.56550414]
Agent gate_2 episode reward: [-0.54594223]
All agents episode reward: [-0.54594223]
Agent gate_2 episode reward: [-0.58778043]
All agents episode reward: [-0.58778043]
Agent gate_2 episode reward: [-0.61977845]
All agents episode reward: [-0.61977845]
Agent gate_2 episode reward: [-0.58239544]
All agents episode reward: [-0.58239544]
Agent gate_2 episode reward: [-0.58367153]
All agents episode reward: [-0.58367153]
Agent gate_2 episode reward: [-0.6799117]
All agents episode reward: [-0.6799117]
Agent gate_2 episode reward: [-0.96912888]
All agents episode reward: [-0.96912888]
Agent gate_2 episode reward: [-0.68073787]
All agents episode reward: [-0.68073787]
Iteration 2: 100%|██████████| 10/10 [00:22<00:00,  2.24s/it, episode=30, norm_ret=-0.763, true_ret=-77754.102, steps=600]
Agent gate_2 episode reward: [-0.68986099]
All agents episode reward: [-0.68986099]
Agent gate_2 episode reward: [-0.75096537]
All agents episode reward: [-0.75096537]
Agent gate_2 episode reward: [-0.80061925]
All agents episode reward: [-0.80061925]
Agent gate_2 episode reward: [-0.74314305]
All agents episode reward: [-0.74314305]
Agent gate_2 episode reward: [-0.77352685]
All agents episode reward: [-0.77352685]
Agent gate_2 episode reward: [-0.76071249]
All agents episode reward: [-0.76071249]
Agent gate_2 episode reward: [-0.76917282]
All agents episode reward: [-0.76917282]
Agent gate_2 episode reward: [-0.75720916]
All agents episode reward: [-0.75720916]
Agent gate_2 episode reward: [-0.81033866]
All agents episode reward: [-0.81033866]
Agent gate_2 episode reward: [-0.77940019]
All agents episode reward: [-0.77940019]
Iteration 3: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it, episode=40, norm_ret=-0.897, true_ret=-85475.312, steps=600]
Agent gate_2 episode reward: [-0.78847288]
All agents episode reward: [-0.78847288]
Agent gate_2 episode reward: [-0.89599844]
All agents episode reward: [-0.89599844]
Agent gate_2 episode reward: [-0.88283927]
All agents episode reward: [-0.88283927]
Agent gate_2 episode reward: [-0.85239185]
All agents episode reward: [-0.85239185]
Agent gate_2 episode reward: [-0.91758161]
All agents episode reward: [-0.91758161]
Agent gate_2 episode reward: [-0.90850048]
All agents episode reward: [-0.90850048]
Agent gate_2 episode reward: [-0.87001403]
All agents episode reward: [-0.87001403]
Agent gate_2 episode reward: [-0.93462202]
All agents episode reward: [-0.93462202]
Agent gate_2 episode reward: [-0.93621208]
All agents episode reward: [-0.93621208]
Agent gate_2 episode reward: [-0.98799457]
All agents episode reward: [-0.98799457]
Iteration 4: 100%|██████████| 10/10 [00:22<00:00,  2.26s/it, episode=50, norm_ret=-0.982, true_ret=-86058.250, steps=600]
Agent gate_2 episode reward: [-0.97939064]
All agents episode reward: [-0.97939064]
Agent gate_2 episode reward: [-0.98456167]
All agents episode reward: [-0.98456167]
Agent gate_2 episode reward: [-1.00888272]
All agents episode reward: [-1.00888272]
Agent gate_2 episode reward: [-0.97221398]
All agents episode reward: [-0.97221398]
Agent gate_2 episode reward: [-0.94317671]
All agents episode reward: [-0.94317671]
Agent gate_2 episode reward: [-0.92493231]
All agents episode reward: [-0.92493231]
Agent gate_2 episode reward: [-0.95799984]
All agents episode reward: [-0.95799984]
Agent gate_2 episode reward: [-0.96658141]
All agents episode reward: [-0.96658141]
Agent gate_2 episode reward: [-0.96810142]
All agents episode reward: [-0.96810142]
Agent gate_2 episode reward: [-1.11092643]
All agents episode reward: [-1.11092643]
Iteration 5: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it, episode=60, norm_ret=-1.086, true_ret=-76410.367, steps=600]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -82795.828 at episode 51 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-1.07923495]
All agents episode reward: [-1.07923495]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -73275.008 at episode 52 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.96448701]
All agents episode reward: [-0.96448701]
Agent gate_2 episode reward: [-1.04387125]
All agents episode reward: [-1.04387125]
Agent gate_2 episode reward: [-1.06707789]
All agents episode reward: [-1.06707789]
Agent gate_2 episode reward: [-1.05618278]
All agents episode reward: [-1.05618278]
Agent gate_2 episode reward: [-1.17341325]
All agents episode reward: [-1.17341325]
Agent gate_2 episode reward: [-1.21349902]
All agents episode reward: [-1.21349902]
Agent gate_2 episode reward: [-1.06112818]
All agents episode reward: [-1.06112818]
Agent gate_2 episode reward: [-1.11735686]
All agents episode reward: [-1.11735686]
Agent gate_2 episode reward: [-1.07934928]
All agents episode reward: [-1.07934928]
Iteration 6: 100%|██████████| 10/10 [00:22<00:00,  2.28s/it, episode=70, norm_ret=-1.198, true_ret=-80656.188, steps=600]
Agent gate_2 episode reward: [-1.19215057]
All agents episode reward: [-1.19215057]
Agent gate_2 episode reward: [-1.12034278]
All agents episode reward: [-1.12034278]
Agent gate_2 episode reward: [-1.14347405]
All agents episode reward: [-1.14347405]
Agent gate_2 episode reward: [-1.19532513]
All agents episode reward: [-1.19532513]
Agent gate_2 episode reward: [-1.13906293]
All agents episode reward: [-1.13906293]
Agent gate_2 episode reward: [-1.18422835]
All agents episode reward: [-1.18422835]
Agent gate_2 episode reward: [-1.20720003]
All agents episode reward: [-1.20720003]
Agent gate_2 episode reward: [-1.30531999]
All agents episode reward: [-1.30531999]
Agent gate_2 episode reward: [-1.26706842]
All agents episode reward: [-1.26706842]
Agent gate_2 episode reward: [-1.22927068]
All agents episode reward: [-1.22927068]
Iteration 7: 100%|██████████| 10/10 [00:20<00:00,  2.04s/it, episode=80, norm_ret=-1.287, true_ret=-81228.609, steps=600]
Agent gate_2 episode reward: [-1.30573339]
All agents episode reward: [-1.30573339]
Agent gate_2 episode reward: [-1.19249841]
All agents episode reward: [-1.19249841]
Agent gate_2 episode reward: [-1.27872922]
All agents episode reward: [-1.27872922]
Agent gate_2 episode reward: [-1.29008009]
All agents episode reward: [-1.29008009]
Agent gate_2 episode reward: [-1.3632241]
All agents episode reward: [-1.3632241]
Agent gate_2 episode reward: [-1.25358698]
All agents episode reward: [-1.25358698]
Agent gate_2 episode reward: [-1.28521314]
All agents episode reward: [-1.28521314]
Agent gate_2 episode reward: [-1.2683026]
All agents episode reward: [-1.2683026]
Agent gate_2 episode reward: [-1.31360979]
All agents episode reward: [-1.31360979]
Agent gate_2 episode reward: [-1.32214637]
All agents episode reward: [-1.32214637]
Iteration 8: 100%|██████████| 10/10 [00:22<00:00,  2.20s/it, episode=90, norm_ret=-1.360, true_ret=-78297.539, steps=600]
Agent gate_2 episode reward: [-1.28843234]
All agents episode reward: [-1.28843234]
Agent gate_2 episode reward: [-1.39321541]
All agents episode reward: [-1.39321541]
Agent gate_2 episode reward: [-1.3591609]
All agents episode reward: [-1.3591609]
Agent gate_2 episode reward: [-1.40955541]
All agents episode reward: [-1.40955541]
Agent gate_2 episode reward: [-1.31096997]
All agents episode reward: [-1.31096997]
Agent gate_2 episode reward: [-1.42245169]
All agents episode reward: [-1.42245169]
Agent gate_2 episode reward: [-1.27889246]
All agents episode reward: [-1.27889246]
Agent gate_2 episode reward: [-1.38174296]
All agents episode reward: [-1.38174296]
Agent gate_2 episode reward: [-1.40621514]
All agents episode reward: [-1.40621514]
Agent gate_2 episode reward: [-1.35041126]
All agents episode reward: [-1.35041126]
Iteration 9: 100%|██████████| 10/10 [00:22<00:00,  2.27s/it, episode=100, norm_ret=-1.457, true_ret=-88857.961, steps=600]
Agent gate_2 episode reward: [-1.45524844]
All agents episode reward: [-1.45524844]
Agent gate_2 episode reward: [-1.37620625]
All agents episode reward: [-1.37620625]
Agent gate_2 episode reward: [-1.40938076]
All agents episode reward: [-1.40938076]
Agent gate_2 episode reward: [-1.4166074]
All agents episode reward: [-1.4166074]
Agent gate_2 episode reward: [-1.41030225]
All agents episode reward: [-1.41030225]
Agent gate_2 episode reward: [-1.55688243]
All agents episode reward: [-1.55688243]
Agent gate_2 episode reward: [-1.44289204]
All agents episode reward: [-1.44289204]
Agent gate_2 episode reward: [-1.50571313]
All agents episode reward: [-1.50571313]
Agent gate_2 episode reward: [-1.38303493]
All agents episode reward: [-1.38303493]
Agent gate_2 episode reward: [-1.61376174]
All agents episode reward: [-1.61376174]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -97100.695 ± 18777.070
  Average reward: -97100.695 ± 18777.070
  Total reward: -97100.695 ± 18777.070
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -80333.500 ± 3087.304
  Average reward: -80333.500 ± 3087.304
  Total reward: -80333.500 ± 3087.304
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -84912.664 ± 3523.864
  Average reward: -84912.664 ± 3523.864
  Total reward: -84912.664 ± 3523.864
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -97100.695
Rule-based avg reward: -80333.500
No control avg reward: -84912.664
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
