Iteration 0: 100%|██████████| 10/10 [00:22<00:00,  2.30s/it, episode=10, norm_ret=-11.456, true_ret=-78662.227, steps=600]
Agent gate_2 episode reward: [-68.93469365]
All agents episode reward: [-68.93469365]
Agent gate_2 episode reward: [-3.2243376]
All agents episode reward: [-3.2243376]
Agent gate_2 episode reward: [-8.56342722]
All agents episode reward: [-8.56342722]
Agent gate_2 episode reward: [-3.94207262]
All agents episode reward: [-3.94207262]
Agent gate_2 episode reward: [-5.03865691]
All agents episode reward: [-5.03865691]
Agent gate_2 episode reward: [-3.09919211]
All agents episode reward: [-3.09919211]
Agent gate_2 episode reward: [-14.82037657]
All agents episode reward: [-14.82037657]
Agent gate_2 episode reward: [-2.7254495]
All agents episode reward: [-2.7254495]
Agent gate_2 episode reward: [-2.09427585]
All agents episode reward: [-2.09427585]
Agent gate_2 episode reward: [-2.1180057]
All agents episode reward: [-2.1180057]
Iteration 1: 100%|██████████| 10/10 [00:22<00:00,  2.27s/it, episode=20, norm_ret=-2.601, true_ret=-79988.438, steps=600]
Agent gate_2 episode reward: [-2.18356652]
All agents episode reward: [-2.18356652]
Agent gate_2 episode reward: [-2.26825795]
All agents episode reward: [-2.26825795]
Agent gate_2 episode reward: [-2.46093032]
All agents episode reward: [-2.46093032]
Agent gate_2 episode reward: [-2.49546059]
All agents episode reward: [-2.49546059]
Agent gate_2 episode reward: [-2.52900947]
All agents episode reward: [-2.52900947]
Agent gate_2 episode reward: [-2.77593073]
All agents episode reward: [-2.77593073]
Agent gate_2 episode reward: [-2.62288997]
All agents episode reward: [-2.62288997]
Agent gate_2 episode reward: [-2.86826294]
All agents episode reward: [-2.86826294]
Agent gate_2 episode reward: [-2.92833776]
All agents episode reward: [-2.92833776]
Agent gate_2 episode reward: [-2.87775856]
All agents episode reward: [-2.87775856]
Iteration 2: 100%|██████████| 10/10 [00:22<00:00,  2.26s/it, episode=30, norm_ret=-3.296, true_ret=-82421.414, steps=600]
Agent gate_2 episode reward: [-3.19371336]
All agents episode reward: [-3.19371336]
Agent gate_2 episode reward: [-2.97552273]
All agents episode reward: [-2.97552273]
Agent gate_2 episode reward: [-3.14618698]
All agents episode reward: [-3.14618698]
Agent gate_2 episode reward: [-3.29964479]
All agents episode reward: [-3.29964479]
Agent gate_2 episode reward: [-3.09952554]
All agents episode reward: [-3.09952554]
Agent gate_2 episode reward: [-3.20046779]
All agents episode reward: [-3.20046779]
Agent gate_2 episode reward: [-3.65263866]
All agents episode reward: [-3.65263866]
Agent gate_2 episode reward: [-3.29449193]
All agents episode reward: [-3.29449193]
Agent gate_2 episode reward: [-3.56606756]
All agents episode reward: [-3.56606756]
Agent gate_2 episode reward: [-3.52924796]
All agents episode reward: [-3.52924796]
Iteration 3: 100%|██████████| 10/10 [00:23<00:00,  2.34s/it, episode=40, norm_ret=-3.764, true_ret=-83842.398, steps=600]
Agent gate_2 episode reward: [-3.58218213]
All agents episode reward: [-3.58218213]
Agent gate_2 episode reward: [-3.95440709]
All agents episode reward: [-3.95440709]
Agent gate_2 episode reward: [-3.54197079]
All agents episode reward: [-3.54197079]
Agent gate_2 episode reward: [-3.66968469]
All agents episode reward: [-3.66968469]
Agent gate_2 episode reward: [-3.84442082]
All agents episode reward: [-3.84442082]
Agent gate_2 episode reward: [-3.68258543]
All agents episode reward: [-3.68258543]
Agent gate_2 episode reward: [-3.82421786]
All agents episode reward: [-3.82421786]
Agent gate_2 episode reward: [-3.78643302]
All agents episode reward: [-3.78643302]
Agent gate_2 episode reward: [-3.70269756]
All agents episode reward: [-3.70269756]
Agent gate_2 episode reward: [-4.05173407]
All agents episode reward: [-4.05173407]
Iteration 4: 100%|██████████| 10/10 [00:22<00:00,  2.29s/it, episode=50, norm_ret=-4.141, true_ret=-89392.891, steps=600]
Agent gate_2 episode reward: [-4.06733375]
All agents episode reward: [-4.06733375]
Agent gate_2 episode reward: [-3.79358306]
All agents episode reward: [-3.79358306]
Agent gate_2 episode reward: [-4.0996147]
All agents episode reward: [-4.0996147]
Agent gate_2 episode reward: [-3.8569304]
All agents episode reward: [-3.8569304]
Agent gate_2 episode reward: [-4.38126698]
All agents episode reward: [-4.38126698]
Agent gate_2 episode reward: [-3.99934232]
All agents episode reward: [-3.99934232]
Agent gate_2 episode reward: [-4.42746993]
All agents episode reward: [-4.42746993]
Agent gate_2 episode reward: [-4.01622878]
All agents episode reward: [-4.01622878]
Agent gate_2 episode reward: [-4.03546963]
All agents episode reward: [-4.03546963]
Agent gate_2 episode reward: [-4.73624496]
All agents episode reward: [-4.73624496]
Iteration 5: 100%|██████████| 10/10 [00:25<00:00,  2.56s/it, episode=60, norm_ret=-4.444, true_ret=-76818.617, steps=600]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -81917.141 at episode 51 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.3748501]
All agents episode reward: [-4.3748501]
Agent gate_2 episode reward: [-4.58952596]
All agents episode reward: [-4.58952596]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -80440.664 at episode 53 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.36260036]
All agents episode reward: [-4.36260036]
Agent gate_2 episode reward: [-4.6631327]
All agents episode reward: [-4.6631327]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -78996.664 at episode 55 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.34867006]
All agents episode reward: [-4.34867006]
Agent gate_2 episode reward: [-4.60983888]
All agents episode reward: [-4.60983888]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -73898.578 at episode 57 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.12733677]
All agents episode reward: [-4.12733677]
Agent gate_2 episode reward: [-4.34266824]
All agents episode reward: [-4.34266824]
Agent gate_2 episode reward: [-4.64239882]
All agents episode reward: [-4.64239882]
Agent gate_2 episode reward: [-4.37955581]
All agents episode reward: [-4.37955581]
Iteration 6: 100%|██████████| 10/10 [00:26<00:00,  2.62s/it, episode=70, norm_ret=-4.870, true_ret=-81125.297, steps=600]
Agent gate_2 episode reward: [-4.9079379]
All agents episode reward: [-4.9079379]
Agent gate_2 episode reward: [-4.55531689]
All agents episode reward: [-4.55531689]
Agent gate_2 episode reward: [-4.82885646]
All agents episode reward: [-4.82885646]
Agent gate_2 episode reward: [-4.99462339]
All agents episode reward: [-4.99462339]
Agent gate_2 episode reward: [-4.83385791]
All agents episode reward: [-4.83385791]
Agent gate_2 episode reward: [-5.11091185]
All agents episode reward: [-5.11091185]
Agent gate_2 episode reward: [-4.77544861]
All agents episode reward: [-4.77544861]
Agent gate_2 episode reward: [-4.83460014]
All agents episode reward: [-4.83460014]
Agent gate_2 episode reward: [-4.94330492]
All agents episode reward: [-4.94330492]
Agent gate_2 episode reward: [-4.91138267]
All agents episode reward: [-4.91138267]
Iteration 7: 100%|██████████| 10/10 [00:24<00:00,  2.43s/it, episode=80, norm_ret=-5.088, true_ret=-85186.039, steps=600]
Agent gate_2 episode reward: [-5.31485292]
All agents episode reward: [-5.31485292]
Agent gate_2 episode reward: [-4.69633057]
All agents episode reward: [-4.69633057]
Agent gate_2 episode reward: [-4.92112819]
All agents episode reward: [-4.92112819]
Agent gate_2 episode reward: [-4.80058799]
All agents episode reward: [-4.80058799]
Agent gate_2 episode reward: [-5.1927621]
All agents episode reward: [-5.1927621]
Agent gate_2 episode reward: [-5.32501195]
All agents episode reward: [-5.32501195]
Agent gate_2 episode reward: [-5.45539869]
All agents episode reward: [-5.45539869]
Agent gate_2 episode reward: [-4.78052865]
All agents episode reward: [-4.78052865]
Agent gate_2 episode reward: [-4.96936936]
All agents episode reward: [-4.96936936]
Agent gate_2 episode reward: [-5.42553298]
All agents episode reward: [-5.42553298]
Iteration 8: 100%|██████████| 10/10 [00:24<00:00,  2.48s/it, episode=90, norm_ret=-5.369, true_ret=-80702.266, steps=600]
Agent gate_2 episode reward: [-5.09569494]
All agents episode reward: [-5.09569494]
Agent gate_2 episode reward: [-5.18434517]
All agents episode reward: [-5.18434517]
Agent gate_2 episode reward: [-5.37165727]
All agents episode reward: [-5.37165727]
Agent gate_2 episode reward: [-5.40303477]
All agents episode reward: [-5.40303477]
Agent gate_2 episode reward: [-5.01859118]
All agents episode reward: [-5.01859118]
Agent gate_2 episode reward: [-5.42849556]
All agents episode reward: [-5.42849556]
Agent gate_2 episode reward: [-5.1585709]
All agents episode reward: [-5.1585709]
Agent gate_2 episode reward: [-5.85180801]
All agents episode reward: [-5.85180801]
Agent gate_2 episode reward: [-5.81142349]
All agents episode reward: [-5.81142349]
Agent gate_2 episode reward: [-5.36751503]
All agents episode reward: [-5.36751503]
Iteration 9: 100%|██████████| 10/10 [00:25<00:00,  2.51s/it, episode=100, norm_ret=-5.457, true_ret=-85800.305, steps=600]
Agent gate_2 episode reward: [-5.03910593]
All agents episode reward: [-5.03910593]
Agent gate_2 episode reward: [-5.31426744]
All agents episode reward: [-5.31426744]
Agent gate_2 episode reward: [-5.44198282]
All agents episode reward: [-5.44198282]
Agent gate_2 episode reward: [-5.59540078]
All agents episode reward: [-5.59540078]
Agent gate_2 episode reward: [-5.73892002]
All agents episode reward: [-5.73892002]
Agent gate_2 episode reward: [-5.12093941]
All agents episode reward: [-5.12093941]
Agent gate_2 episode reward: [-5.41960478]
All agents episode reward: [-5.41960478]
Agent gate_2 episode reward: [-5.40898822]
All agents episode reward: [-5.40898822]
Agent gate_2 episode reward: [-5.563127]
All agents episode reward: [-5.563127]
Agent gate_2 episode reward: [-5.92928872]
All agents episode reward: [-5.92928872]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -81027.766 ± 3796.875
  Average reward: -81027.766 ± 3796.875
  Total reward: -81027.766 ± 3796.875
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -82579.719 ± 3420.119
  Average reward: -82579.719 ± 3420.119
  Total reward: -82579.719 ± 3420.119
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -82330.398 ± 4308.467
  Average reward: -82330.398 ± 4308.467
  Total reward: -82330.398 ± 4308.467
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -81027.766
Rule-based avg reward: -82579.719
No control avg reward: -82330.398
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
