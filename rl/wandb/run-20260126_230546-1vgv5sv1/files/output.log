Iteration 0: 100%|██████████| 10/10 [00:21<00:00,  2.15s/it, episode=10, norm_ret=-6.406, true_ret=-5381.681, steps=600]
Agent gate_2 episode reward: [-55.02925528]
All agents episode reward: [-55.02925528]
Agent gate_2 episode reward: [-2.91466557]
All agents episode reward: [-2.91466557]
Agent gate_2 episode reward: [-2.07480643]
All agents episode reward: [-2.07480643]
Agent gate_2 episode reward: [-1.31620995]
All agents episode reward: [-1.31620995]
Agent gate_2 episode reward: [-0.61309135]
All agents episode reward: [-0.61309135]
Agent gate_2 episode reward: [-0.47688028]
All agents episode reward: [-0.47688028]
Agent gate_2 episode reward: [-0.52164335]
All agents episode reward: [-0.52164335]
Agent gate_2 episode reward: [-0.41647371]
All agents episode reward: [-0.41647371]
Agent gate_2 episode reward: [-0.35507023]
All agents episode reward: [-0.35507023]
Agent gate_2 episode reward: [-0.33753906]
All agents episode reward: [-0.33753906]
Iteration 1: 100%|██████████| 10/10 [00:21<00:00,  2.16s/it, episode=20, norm_ret=-0.354, true_ret=-4061.153, steps=600]
Agent gate_2 episode reward: [-0.35548564]
All agents episode reward: [-0.35548564]
Agent gate_2 episode reward: [-0.32143102]
All agents episode reward: [-0.32143102]
Agent gate_2 episode reward: [-0.32920895]
All agents episode reward: [-0.32920895]
Agent gate_2 episode reward: [-0.32704169]
All agents episode reward: [-0.32704169]
Agent gate_2 episode reward: [-0.311036]
All agents episode reward: [-0.311036]
Agent gate_2 episode reward: [-0.33517003]
All agents episode reward: [-0.33517003]
Agent gate_2 episode reward: [-0.45792539]
All agents episode reward: [-0.45792539]
Agent gate_2 episode reward: [-0.38336327]
All agents episode reward: [-0.38336327]
Agent gate_2 episode reward: [-0.37178546]
All agents episode reward: [-0.37178546]
Agent gate_2 episode reward: [-0.34606177]
All agents episode reward: [-0.34606177]
Iteration 2: 100%|██████████| 10/10 [00:21<00:00,  2.14s/it, episode=30, norm_ret=-0.391, true_ret=-3912.802, steps=600]
Agent gate_2 episode reward: [-0.35947216]
All agents episode reward: [-0.35947216]
Agent gate_2 episode reward: [-0.36620794]
All agents episode reward: [-0.36620794]
Agent gate_2 episode reward: [-0.36658281]
All agents episode reward: [-0.36658281]
Agent gate_2 episode reward: [-0.39882275]
All agents episode reward: [-0.39882275]
Agent gate_2 episode reward: [-0.37392152]
All agents episode reward: [-0.37392152]
Agent gate_2 episode reward: [-0.41206415]
All agents episode reward: [-0.41206415]
Agent gate_2 episode reward: [-0.42162569]
All agents episode reward: [-0.42162569]
Agent gate_2 episode reward: [-0.41256387]
All agents episode reward: [-0.41256387]
Agent gate_2 episode reward: [-0.39608659]
All agents episode reward: [-0.39608659]
Agent gate_2 episode reward: [-0.40305047]
All agents episode reward: [-0.40305047]
Iteration 3: 100%|██████████| 10/10 [00:21<00:00,  2.12s/it, episode=40, norm_ret=-0.475, true_ret=-4712.689, steps=600]
Agent gate_2 episode reward: [-0.40397647]
All agents episode reward: [-0.40397647]
Agent gate_2 episode reward: [-0.45493146]
All agents episode reward: [-0.45493146]
Agent gate_2 episode reward: [-0.45895091]
All agents episode reward: [-0.45895091]
Agent gate_2 episode reward: [-0.50835701]
All agents episode reward: [-0.50835701]
Agent gate_2 episode reward: [-0.46651904]
All agents episode reward: [-0.46651904]
Agent gate_2 episode reward: [-0.48733504]
All agents episode reward: [-0.48733504]
Agent gate_2 episode reward: [-0.47781363]
All agents episode reward: [-0.47781363]
Agent gate_2 episode reward: [-0.47387943]
All agents episode reward: [-0.47387943]
Agent gate_2 episode reward: [-0.46314065]
All agents episode reward: [-0.46314065]
Agent gate_2 episode reward: [-0.55708936]
All agents episode reward: [-0.55708936]
Iteration 4: 100%|██████████| 10/10 [00:21<00:00,  2.15s/it, episode=50, norm_ret=-0.517, true_ret=-4078.585, steps=600]
Agent gate_2 episode reward: [-0.49272468]
All agents episode reward: [-0.49272468]
Agent gate_2 episode reward: [-0.48625308]
All agents episode reward: [-0.48625308]
Agent gate_2 episode reward: [-0.48273677]
All agents episode reward: [-0.48273677]
Agent gate_2 episode reward: [-0.50018426]
All agents episode reward: [-0.50018426]
Agent gate_2 episode reward: [-0.52489751]
All agents episode reward: [-0.52489751]
Agent gate_2 episode reward: [-0.5382546]
All agents episode reward: [-0.5382546]
Agent gate_2 episode reward: [-0.54171236]
All agents episode reward: [-0.54171236]
Agent gate_2 episode reward: [-0.52421963]
All agents episode reward: [-0.52421963]
Agent gate_2 episode reward: [-0.54640274]
All agents episode reward: [-0.54640274]
Agent gate_2 episode reward: [-0.53707414]
All agents episode reward: [-0.53707414]
Iteration 5: 100%|██████████| 10/10 [00:21<00:00,  2.13s/it, episode=60, norm_ret=-0.561, true_ret=-4398.483, steps=600]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -4156.239 at episode 51 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.55266819]
All agents episode reward: [-0.55266819]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3917.399 at episode 52 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.52584621]
All agents episode reward: [-0.52584621]
Agent gate_2 episode reward: [-0.53902058]
All agents episode reward: [-0.53902058]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3819.914 at episode 54 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.52222314]
All agents episode reward: [-0.52222314]
Agent gate_2 episode reward: [-0.55681335]
All agents episode reward: [-0.55681335]
Agent gate_2 episode reward: [-0.56329608]
All agents episode reward: [-0.56329608]
Agent gate_2 episode reward: [-0.59026482]
All agents episode reward: [-0.59026482]
Agent gate_2 episode reward: [-0.55718871]
All agents episode reward: [-0.55718871]
Agent gate_2 episode reward: [-0.5709843]
All agents episode reward: [-0.5709843]
Agent gate_2 episode reward: [-0.63293331]
All agents episode reward: [-0.63293331]
Iteration 6: 100%|██████████| 10/10 [00:21<00:00,  2.14s/it, episode=70, norm_ret=-0.598, true_ret=-3956.741, steps=600]
Agent gate_2 episode reward: [-0.59299082]
All agents episode reward: [-0.59299082]
Agent gate_2 episode reward: [-0.58340612]
All agents episode reward: [-0.58340612]
Agent gate_2 episode reward: [-0.60412835]
All agents episode reward: [-0.60412835]
Agent gate_2 episode reward: [-0.56865213]
All agents episode reward: [-0.56865213]
Agent gate_2 episode reward: [-0.59565143]
All agents episode reward: [-0.59565143]
Agent gate_2 episode reward: [-0.58991431]
All agents episode reward: [-0.58991431]
Agent gate_2 episode reward: [-0.62810754]
All agents episode reward: [-0.62810754]
Agent gate_2 episode reward: [-0.60518683]
All agents episode reward: [-0.60518683]
Agent gate_2 episode reward: [-0.59534475]
All agents episode reward: [-0.59534475]
Agent gate_2 episode reward: [-0.61366498]
All agents episode reward: [-0.61366498]
Iteration 7: 100%|██████████| 10/10 [00:21<00:00,  2.17s/it, episode=80, norm_ret=-0.631, true_ret=-3838.741, steps=600]
Agent gate_2 episode reward: [-0.62702485]
All agents episode reward: [-0.62702485]
Agent gate_2 episode reward: [-0.62986916]
All agents episode reward: [-0.62986916]
Agent gate_2 episode reward: [-0.61842758]
All agents episode reward: [-0.61842758]
Agent gate_2 episode reward: [-0.61765312]
All agents episode reward: [-0.61765312]
Agent gate_2 episode reward: [-0.62386076]
All agents episode reward: [-0.62386076]
Agent gate_2 episode reward: [-0.63161174]
All agents episode reward: [-0.63161174]
Agent gate_2 episode reward: [-0.65185917]
All agents episode reward: [-0.65185917]
Agent gate_2 episode reward: [-0.64096632]
All agents episode reward: [-0.64096632]
Agent gate_2 episode reward: [-0.63239788]
All agents episode reward: [-0.63239788]
Agent gate_2 episode reward: [-0.63548856]
All agents episode reward: [-0.63548856]
Iteration 8: 100%|██████████| 10/10 [00:21<00:00,  2.12s/it, episode=90, norm_ret=-0.663, true_ret=-3877.221, steps=600]
Agent gate_2 episode reward: [-0.64447073]
All agents episode reward: [-0.64447073]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3797.502 at episode 82 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.63624498]
All agents episode reward: [-0.63624498]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3774.820 at episode 83 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.63624475]
All agents episode reward: [-0.63624475]
Agent gate_2 episode reward: [-0.69827638]
All agents episode reward: [-0.69827638]
Agent gate_2 episode reward: [-0.64779631]
All agents episode reward: [-0.64779631]
Agent gate_2 episode reward: [-0.66816724]
All agents episode reward: [-0.66816724]
Agent gate_2 episode reward: [-0.66908344]
All agents episode reward: [-0.66908344]
Agent gate_2 episode reward: [-0.65474224]
All agents episode reward: [-0.65474224]
Agent gate_2 episode reward: [-0.6969371]
All agents episode reward: [-0.6969371]
Agent gate_2 episode reward: [-0.67990586]
All agents episode reward: [-0.67990586]
Iteration 9: 100%|██████████| 10/10 [00:21<00:00,  2.14s/it, episode=100, norm_ret=-0.694, true_ret=-3842.792, steps=600]
Agent gate_2 episode reward: [-0.67092095]
All agents episode reward: [-0.67092095]
Agent gate_2 episode reward: [-0.67538574]
All agents episode reward: [-0.67538574]
Agent gate_2 episode reward: [-0.67682997]
All agents episode reward: [-0.67682997]
Agent gate_2 episode reward: [-0.71958846]
All agents episode reward: [-0.71958846]
Agent gate_2 episode reward: [-0.73039497]
All agents episode reward: [-0.73039497]
Agent gate_2 episode reward: [-0.68558106]
All agents episode reward: [-0.68558106]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3718.602 at episode 97 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.6764551]
All agents episode reward: [-0.6764551]
Agent gate_2 episode reward: [-0.69748225]
All agents episode reward: [-0.69748225]
Agent gate_2 episode reward: [-0.70214889]
All agents episode reward: [-0.70214889]
Agent gate_2 episode reward: [-0.70953084]
All agents episode reward: [-0.70953084]
Loaded 1 agents from ppo_agents_butterfly_scB
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scB/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scB/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scB/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scB/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scB/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scB/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scB/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scB/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scB/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scB/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -10414.236 ± 6339.559
  Average reward: -10414.236 ± 6339.559
  Total reward: -10414.236 ± 6339.559
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scB/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scB/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scB/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scB/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scB/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scB/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scB/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scB/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scB/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scB/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -3917.829 ± 126.963
  Average reward: -3917.829 ± 126.963
  Total reward: -3917.829 ± 126.963
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scB/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scB/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scB/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scB/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scB/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scB/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scB/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scB/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scB/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scB/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -3887.620 ± 98.689
  Average reward: -3887.620 ± 98.689
  Total reward: -3887.620 ± 98.689
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -10414.236
Rule-based avg reward: -3917.829
No control avg reward: -3887.620
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
