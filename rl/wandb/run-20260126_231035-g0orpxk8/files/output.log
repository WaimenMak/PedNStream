Iteration 0: 100%|██████████| 10/10 [00:16<00:00,  1.62s/it, episode=10, norm_ret=-8.265, true_ret=-9644.735, steps=600]
Agent gate_2 episode reward: [-64.00377183]
All agents episode reward: [-64.00377183]
Agent gate_2 episode reward: [-2.72787249]
All agents episode reward: [-2.72787249]
Agent gate_2 episode reward: [-2.48428472]
All agents episode reward: [-2.48428472]
Agent gate_2 episode reward: [-1.29543321]
All agents episode reward: [-1.29543321]
Agent gate_2 episode reward: [-2.67358904]
All agents episode reward: [-2.67358904]
Agent gate_2 episode reward: [-1.96364144]
All agents episode reward: [-1.96364144]
Agent gate_2 episode reward: [-1.59362663]
All agents episode reward: [-1.59362663]
Agent gate_2 episode reward: [-1.29445651]
All agents episode reward: [-1.29445651]
Agent gate_2 episode reward: [-1.92448172]
All agents episode reward: [-1.92448172]
Agent gate_2 episode reward: [-2.68930741]
All agents episode reward: [-2.68930741]
Iteration 1: 100%|██████████| 10/10 [00:15<00:00,  1.59s/it, episode=20, norm_ret=-2.338, true_ret=-6115.702, steps=600]
Agent gate_2 episode reward: [-2.24399658]
All agents episode reward: [-2.24399658]
Agent gate_2 episode reward: [-1.33936424]
All agents episode reward: [-1.33936424]
Agent gate_2 episode reward: [-2.15146831]
All agents episode reward: [-2.15146831]
Agent gate_2 episode reward: [-1.58675676]
All agents episode reward: [-1.58675676]
Agent gate_2 episode reward: [-2.21087036]
All agents episode reward: [-2.21087036]
Agent gate_2 episode reward: [-4.3281976]
All agents episode reward: [-4.3281976]
Agent gate_2 episode reward: [-2.33781202]
All agents episode reward: [-2.33781202]
Agent gate_2 episode reward: [-2.30532594]
All agents episode reward: [-2.30532594]
Agent gate_2 episode reward: [-2.54150369]
All agents episode reward: [-2.54150369]
Agent gate_2 episode reward: [-2.33569221]
All agents episode reward: [-2.33569221]
Iteration 2: 100%|██████████| 10/10 [00:15<00:00,  1.57s/it, episode=30, norm_ret=-2.278, true_ret=-5068.741, steps=600]
Agent gate_2 episode reward: [-2.17773989]
All agents episode reward: [-2.17773989]
Agent gate_2 episode reward: [-1.95502618]
All agents episode reward: [-1.95502618]
Agent gate_2 episode reward: [-2.04511456]
All agents episode reward: [-2.04511456]
Agent gate_2 episode reward: [-2.73119086]
All agents episode reward: [-2.73119086]
Agent gate_2 episode reward: [-3.4672447]
All agents episode reward: [-3.4672447]
Agent gate_2 episode reward: [-2.22194261]
All agents episode reward: [-2.22194261]
Agent gate_2 episode reward: [-2.0998531]
All agents episode reward: [-2.0998531]
Agent gate_2 episode reward: [-1.86011107]
All agents episode reward: [-1.86011107]
Agent gate_2 episode reward: [-1.89755818]
All agents episode reward: [-1.89755818]
Agent gate_2 episode reward: [-2.3265241]
All agents episode reward: [-2.3265241]
Iteration 3: 100%|██████████| 10/10 [00:16<00:00,  1.67s/it, episode=40, norm_ret=-2.607, true_ret=-4443.242, steps=600]
Agent gate_2 episode reward: [-2.6270147]
All agents episode reward: [-2.6270147]
Agent gate_2 episode reward: [-2.36990478]
All agents episode reward: [-2.36990478]
Agent gate_2 episode reward: [-2.75598739]
All agents episode reward: [-2.75598739]
Agent gate_2 episode reward: [-2.83362159]
All agents episode reward: [-2.83362159]
Agent gate_2 episode reward: [-2.42238646]
All agents episode reward: [-2.42238646]
Agent gate_2 episode reward: [-2.69479926]
All agents episode reward: [-2.69479926]
Agent gate_2 episode reward: [-2.5895065]
All agents episode reward: [-2.5895065]
Agent gate_2 episode reward: [-3.03221007]
All agents episode reward: [-3.03221007]
Agent gate_2 episode reward: [-2.42774314]
All agents episode reward: [-2.42774314]
Agent gate_2 episode reward: [-2.31733215]
All agents episode reward: [-2.31733215]
Iteration 4: 100%|██████████| 10/10 [00:16<00:00,  1.64s/it, episode=50, norm_ret=-2.314, true_ret=-4407.060, steps=600]
Agent gate_2 episode reward: [-2.18037252]
All agents episode reward: [-2.18037252]
Agent gate_2 episode reward: [-2.42376867]
All agents episode reward: [-2.42376867]
Agent gate_2 episode reward: [-2.25271374]
All agents episode reward: [-2.25271374]
Agent gate_2 episode reward: [-2.22540406]
All agents episode reward: [-2.22540406]
Agent gate_2 episode reward: [-2.27352594]
All agents episode reward: [-2.27352594]
Agent gate_2 episode reward: [-2.45701372]
All agents episode reward: [-2.45701372]
Agent gate_2 episode reward: [-2.24114394]
All agents episode reward: [-2.24114394]
Agent gate_2 episode reward: [-2.25073732]
All agents episode reward: [-2.25073732]
Agent gate_2 episode reward: [-2.29580717]
All agents episode reward: [-2.29580717]
Agent gate_2 episode reward: [-2.53669263]
All agents episode reward: [-2.53669263]
Iteration 5: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it, episode=60, norm_ret=-2.540, true_ret=-4136.396, steps=600]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -4485.152 at episode 51 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-2.60616544]
All agents episode reward: [-2.60616544]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -4106.043 at episode 52 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-2.40603635]
All agents episode reward: [-2.40603635]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3915.824 at episode 53 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-2.31352917]
All agents episode reward: [-2.31352917]
Agent gate_2 episode reward: [-2.44804138]
All agents episode reward: [-2.44804138]
Agent gate_2 episode reward: [-2.43106874]
All agents episode reward: [-2.43106874]
Agent gate_2 episode reward: [-2.72159026]
All agents episode reward: [-2.72159026]
Agent gate_2 episode reward: [-2.53068627]
All agents episode reward: [-2.53068627]
Agent gate_2 episode reward: [-2.74835396]
All agents episode reward: [-2.74835396]
Agent gate_2 episode reward: [-2.61248881]
All agents episode reward: [-2.61248881]
Agent gate_2 episode reward: [-2.58163279]
All agents episode reward: [-2.58163279]
Iteration 6: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it, episode=70, norm_ret=-2.663, true_ret=-4186.536, steps=600]
Agent gate_2 episode reward: [-2.65250377]
All agents episode reward: [-2.65250377]
Agent gate_2 episode reward: [-2.61264092]
All agents episode reward: [-2.61264092]
Agent gate_2 episode reward: [-2.57089755]
All agents episode reward: [-2.57089755]
Agent gate_2 episode reward: [-2.80316712]
All agents episode reward: [-2.80316712]
Agent gate_2 episode reward: [-2.5855708]
All agents episode reward: [-2.5855708]
Agent gate_2 episode reward: [-2.6008472]
All agents episode reward: [-2.6008472]
Agent gate_2 episode reward: [-2.58405902]
All agents episode reward: [-2.58405902]
Agent gate_2 episode reward: [-2.69638883]
All agents episode reward: [-2.69638883]
Agent gate_2 episode reward: [-2.72907168]
All agents episode reward: [-2.72907168]
Agent gate_2 episode reward: [-2.79623964]
All agents episode reward: [-2.79623964]
Iteration 7: 100%|██████████| 10/10 [00:16<00:00,  1.69s/it, episode=80, norm_ret=-2.843, true_ret=-4245.981, steps=600]
Agent gate_2 episode reward: [-2.65722284]
All agents episode reward: [-2.65722284]
Agent gate_2 episode reward: [-2.80464895]
All agents episode reward: [-2.80464895]
Agent gate_2 episode reward: [-2.71805816]
All agents episode reward: [-2.71805816]
Agent gate_2 episode reward: [-2.84798951]
All agents episode reward: [-2.84798951]
Agent gate_2 episode reward: [-2.88520182]
All agents episode reward: [-2.88520182]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3858.240 at episode 76 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-2.67123203]
All agents episode reward: [-2.67123203]
Agent gate_2 episode reward: [-2.79970321]
All agents episode reward: [-2.79970321]
Agent gate_2 episode reward: [-2.81948459]
All agents episode reward: [-2.81948459]
Agent gate_2 episode reward: [-3.21601267]
All agents episode reward: [-3.21601267]
Agent gate_2 episode reward: [-3.00623067]
All agents episode reward: [-3.00623067]
Iteration 8: 100%|██████████| 10/10 [00:16<00:00,  1.63s/it, episode=90, norm_ret=-3.111, true_ret=-4031.302, steps=600]
Agent gate_2 episode reward: [-3.17267551]
All agents episode reward: [-3.17267551]
Agent gate_2 episode reward: [-3.27804085]
All agents episode reward: [-3.27804085]
Agent gate_2 episode reward: [-3.04056717]
All agents episode reward: [-3.04056717]
Agent gate_2 episode reward: [-2.83124835]
All agents episode reward: [-2.83124835]
Agent gate_2 episode reward: [-3.26615118]
All agents episode reward: [-3.26615118]
Agent gate_2 episode reward: [-3.06420418]
All agents episode reward: [-3.06420418]
Agent gate_2 episode reward: [-3.24841218]
All agents episode reward: [-3.24841218]
Agent gate_2 episode reward: [-2.97913314]
All agents episode reward: [-2.97913314]
Agent gate_2 episode reward: [-3.22706433]
All agents episode reward: [-3.22706433]
Agent gate_2 episode reward: [-3.00447283]
All agents episode reward: [-3.00447283]
Iteration 9: 100%|██████████| 10/10 [00:16<00:00,  1.67s/it, episode=100, norm_ret=-3.068, true_ret=-4162.207, steps=600]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3843.207 at episode 91 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-2.87768327]
All agents episode reward: [-2.87768327]
Agent gate_2 episode reward: [-3.05179094]
All agents episode reward: [-3.05179094]
Agent gate_2 episode reward: [-2.99489149]
All agents episode reward: [-2.99489149]
Agent gate_2 episode reward: [-3.28684212]
All agents episode reward: [-3.28684212]
Agent gate_2 episode reward: [-3.0214442]
All agents episode reward: [-3.0214442]
Agent gate_2 episode reward: [-3.08429802]
All agents episode reward: [-3.08429802]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3727.064 at episode 97 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-2.86869391]
All agents episode reward: [-2.86869391]
Agent gate_2 episode reward: [-3.11221784]
All agents episode reward: [-3.11221784]
Agent gate_2 episode reward: [-3.1312698]
All agents episode reward: [-3.1312698]
Agent gate_2 episode reward: [-3.24614202]
All agents episode reward: [-3.24614202]
Loaded 1 agents from ppo_agents_butterfly_scB
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scB/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scB/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scB/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scB/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scB/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scB/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scB/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scB/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scB/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scB/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -43057.746 ± 17511.422
  Average reward: -43057.746 ± 17511.422
  Total reward: -43057.746 ± 17511.422
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scB/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scB/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scB/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scB/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scB/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scB/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scB/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scB/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scB/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scB/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -3900.669 ± 76.049
  Average reward: -3900.669 ± 76.049
  Total reward: -3900.669 ± 76.049
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scB/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scB/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scB/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scB/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scB/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scB/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scB/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scB/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scB/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scB/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -4006.257 ± 106.015
  Average reward: -4006.257 ± 106.015
  Total reward: -4006.257 ± 106.015
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -43057.746
Rule-based avg reward: -3900.669
No control avg reward: -4006.257
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
