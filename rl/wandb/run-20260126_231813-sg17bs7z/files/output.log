Iteration 0: 100%|██████████| 10/10 [00:21<00:00,  2.20s/it, episode=10, norm_ret=-8.487, true_ret=-49223.469, steps=600]
Agent gate_2 episode reward: [-66.43367817]
All agents episode reward: [-66.43367817]
Agent gate_2 episode reward: [-4.11199325]
All agents episode reward: [-4.11199325]
Agent gate_2 episode reward: [-7.46080335]
All agents episode reward: [-7.46080335]
Agent gate_2 episode reward: [-3.27150556]
All agents episode reward: [-3.27150556]
Agent gate_2 episode reward: [-0.67449262]
All agents episode reward: [-0.67449262]
Agent gate_2 episode reward: [-0.56519689]
All agents episode reward: [-0.56519689]
Agent gate_2 episode reward: [-0.55585135]
All agents episode reward: [-0.55585135]
Agent gate_2 episode reward: [-0.58036058]
All agents episode reward: [-0.58036058]
Agent gate_2 episode reward: [-0.59427926]
All agents episode reward: [-0.59427926]
Agent gate_2 episode reward: [-0.62520702]
All agents episode reward: [-0.62520702]
Iteration 1: 100%|██████████| 10/10 [00:21<00:00,  2.13s/it, episode=20, norm_ret=-0.745, true_ret=-49390.227, steps=600]
Agent gate_2 episode reward: [-0.66212281]
All agents episode reward: [-0.66212281]
Agent gate_2 episode reward: [-0.68376303]
All agents episode reward: [-0.68376303]
Agent gate_2 episode reward: [-0.70391783]
All agents episode reward: [-0.70391783]
Agent gate_2 episode reward: [-0.70957383]
All agents episode reward: [-0.70957383]
Agent gate_2 episode reward: [-0.73903173]
All agents episode reward: [-0.73903173]
Agent gate_2 episode reward: [-0.75004489]
All agents episode reward: [-0.75004489]
Agent gate_2 episode reward: [-0.7613088]
All agents episode reward: [-0.7613088]
Agent gate_2 episode reward: [-0.79547957]
All agents episode reward: [-0.79547957]
Agent gate_2 episode reward: [-0.81218521]
All agents episode reward: [-0.81218521]
Agent gate_2 episode reward: [-0.83356246]
All agents episode reward: [-0.83356246]
Iteration 2: 100%|██████████| 10/10 [00:21<00:00,  2.12s/it, episode=30, norm_ret=-0.925, true_ret=-49384.395, steps=600]
Agent gate_2 episode reward: [-0.8550969]
All agents episode reward: [-0.8550969]
Agent gate_2 episode reward: [-0.85536449]
All agents episode reward: [-0.85536449]
Agent gate_2 episode reward: [-0.87917717]
All agents episode reward: [-0.87917717]
Agent gate_2 episode reward: [-0.90176023]
All agents episode reward: [-0.90176023]
Agent gate_2 episode reward: [-0.91104479]
All agents episode reward: [-0.91104479]
Agent gate_2 episode reward: [-0.92926626]
All agents episode reward: [-0.92926626]
Agent gate_2 episode reward: [-0.94961476]
All agents episode reward: [-0.94961476]
Agent gate_2 episode reward: [-0.98332691]
All agents episode reward: [-0.98332691]
Agent gate_2 episode reward: [-0.97886709]
All agents episode reward: [-0.97886709]
Agent gate_2 episode reward: [-1.00159881]
All agents episode reward: [-1.00159881]
Iteration 3: 100%|██████████| 10/10 [00:21<00:00,  2.15s/it, episode=40, norm_ret=-1.083, true_ret=-49421.648, steps=600]
Agent gate_2 episode reward: [-1.02321862]
All agents episode reward: [-1.02321862]
Agent gate_2 episode reward: [-1.0438009]
All agents episode reward: [-1.0438009]
Agent gate_2 episode reward: [-1.04721322]
All agents episode reward: [-1.04721322]
Agent gate_2 episode reward: [-1.0653837]
All agents episode reward: [-1.0653837]
Agent gate_2 episode reward: [-1.07403684]
All agents episode reward: [-1.07403684]
Agent gate_2 episode reward: [-1.08385781]
All agents episode reward: [-1.08385781]
Agent gate_2 episode reward: [-1.09995488]
All agents episode reward: [-1.09995488]
Agent gate_2 episode reward: [-1.09227452]
All agents episode reward: [-1.09227452]
Agent gate_2 episode reward: [-1.15322524]
All agents episode reward: [-1.15322524]
Agent gate_2 episode reward: [-1.14629669]
All agents episode reward: [-1.14629669]
Iteration 4: 100%|██████████| 10/10 [00:21<00:00,  2.12s/it, episode=50, norm_ret=-1.215, true_ret=-48901.051, steps=600]
Agent gate_2 episode reward: [-1.15551922]
All agents episode reward: [-1.15551922]
Agent gate_2 episode reward: [-1.16018498]
All agents episode reward: [-1.16018498]
Agent gate_2 episode reward: [-1.17655497]
All agents episode reward: [-1.17655497]
Agent gate_2 episode reward: [-1.22953025]
All agents episode reward: [-1.22953025]
Agent gate_2 episode reward: [-1.20972065]
All agents episode reward: [-1.20972065]
Agent gate_2 episode reward: [-1.22048574]
All agents episode reward: [-1.22048574]
Agent gate_2 episode reward: [-1.24314434]
All agents episode reward: [-1.24314434]
Agent gate_2 episode reward: [-1.23320995]
All agents episode reward: [-1.23320995]
Agent gate_2 episode reward: [-1.26250348]
All agents episode reward: [-1.26250348]
Agent gate_2 episode reward: [-1.26046135]
All agents episode reward: [-1.26046135]
Iteration 5: 100%|██████████| 10/10 [00:21<00:00,  2.12s/it, episode=60, norm_ret=-1.339, true_ret=-48693.922, steps=600]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -50125.992 at episode 51 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-1.30421638]
All agents episode reward: [-1.30421638]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -48155.523 at episode 52 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-1.26459989]
All agents episode reward: [-1.26459989]
Agent gate_2 episode reward: [-1.31977321]
All agents episode reward: [-1.31977321]
Agent gate_2 episode reward: [-1.32672673]
All agents episode reward: [-1.32672673]
Agent gate_2 episode reward: [-1.33000307]
All agents episode reward: [-1.33000307]
Agent gate_2 episode reward: [-1.37060612]
All agents episode reward: [-1.37060612]
Agent gate_2 episode reward: [-1.35030789]
All agents episode reward: [-1.35030789]
Agent gate_2 episode reward: [-1.37399163]
All agents episode reward: [-1.37399163]
Agent gate_2 episode reward: [-1.37652089]
All agents episode reward: [-1.37652089]
Agent gate_2 episode reward: [-1.36893049]
All agents episode reward: [-1.36893049]
Iteration 6: 100%|██████████| 10/10 [00:21<00:00,  2.17s/it, episode=70, norm_ret=-1.446, true_ret=-49312.172, steps=600]
Agent gate_2 episode reward: [-1.40213448]
All agents episode reward: [-1.40213448]
Agent gate_2 episode reward: [-1.40191553]
All agents episode reward: [-1.40191553]
Agent gate_2 episode reward: [-1.42104429]
All agents episode reward: [-1.42104429]
Agent gate_2 episode reward: [-1.42384298]
All agents episode reward: [-1.42384298]
Agent gate_2 episode reward: [-1.44104535]
All agents episode reward: [-1.44104535]
Agent gate_2 episode reward: [-1.43038433]
All agents episode reward: [-1.43038433]
Agent gate_2 episode reward: [-1.46596638]
All agents episode reward: [-1.46596638]
Agent gate_2 episode reward: [-1.48479191]
All agents episode reward: [-1.48479191]
Agent gate_2 episode reward: [-1.4947549]
All agents episode reward: [-1.4947549]
Agent gate_2 episode reward: [-1.49222062]
All agents episode reward: [-1.49222062]
Iteration 7: 100%|██████████| 10/10 [00:21<00:00,  2.13s/it, episode=80, norm_ret=-1.552, true_ret=-51212.160, steps=600]
Agent gate_2 episode reward: [-1.50679441]
All agents episode reward: [-1.50679441]
Agent gate_2 episode reward: [-1.52297273]
All agents episode reward: [-1.52297273]
Agent gate_2 episode reward: [-1.53931036]
All agents episode reward: [-1.53931036]
Agent gate_2 episode reward: [-1.52262965]
All agents episode reward: [-1.52262965]
Agent gate_2 episode reward: [-1.53523921]
All agents episode reward: [-1.53523921]
Agent gate_2 episode reward: [-1.57271404]
All agents episode reward: [-1.57271404]
Agent gate_2 episode reward: [-1.53529039]
All agents episode reward: [-1.53529039]
Agent gate_2 episode reward: [-1.54936261]
All agents episode reward: [-1.54936261]
Agent gate_2 episode reward: [-1.5805526]
All agents episode reward: [-1.5805526]
Agent gate_2 episode reward: [-1.65184019]
All agents episode reward: [-1.65184019]
Iteration 8: 100%|██████████| 10/10 [00:21<00:00,  2.12s/it, episode=90, norm_ret=-2.227, true_ret=-49255.477, steps=600]
Agent gate_2 episode reward: [-1.59440163]
All agents episode reward: [-1.59440163]
Agent gate_2 episode reward: [-1.6204875]
All agents episode reward: [-1.6204875]
Agent gate_2 episode reward: [-1.61442293]
All agents episode reward: [-1.61442293]
Agent gate_2 episode reward: [-1.61545791]
All agents episode reward: [-1.61545791]
Agent gate_2 episode reward: [-1.65655864]
All agents episode reward: [-1.65655864]
Agent gate_2 episode reward: [-1.64730831]
All agents episode reward: [-1.64730831]
Agent gate_2 episode reward: [-1.66492709]
All agents episode reward: [-1.66492709]
Agent gate_2 episode reward: [-7.46952329]
All agents episode reward: [-7.46952329]
Agent gate_2 episode reward: [-1.7192946]
All agents episode reward: [-1.7192946]
Agent gate_2 episode reward: [-1.66270011]
All agents episode reward: [-1.66270011]
Iteration 9: 100%|██████████| 10/10 [00:21<00:00,  2.18s/it, episode=100, norm_ret=-1.777, true_ret=-50540.766, steps=600]
Agent gate_2 episode reward: [-2.37770635]
All agents episode reward: [-2.37770635]
Agent gate_2 episode reward: [-1.6668132]
All agents episode reward: [-1.6668132]
Agent gate_2 episode reward: [-1.68891022]
All agents episode reward: [-1.68891022]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -47578.535 at episode 94 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-1.64027398]
All agents episode reward: [-1.64027398]
Agent gate_2 episode reward: [-1.68565023]
All agents episode reward: [-1.68565023]
Agent gate_2 episode reward: [-1.70976504]
All agents episode reward: [-1.70976504]
Agent gate_2 episode reward: [-1.72979022]
All agents episode reward: [-1.72979022]
Agent gate_2 episode reward: [-1.72949874]
All agents episode reward: [-1.72949874]
Agent gate_2 episode reward: [-1.75125148]
All agents episode reward: [-1.75125148]
Agent gate_2 episode reward: [-1.79463498]
All agents episode reward: [-1.79463498]
Loaded 1 agents from ppo_agents_butterfly_scA
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scA/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scA/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scA/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scA/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scA/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scA/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scA/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scA/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scA/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scA/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -92571.078 ± 35160.430
  Average reward: -92571.078 ± 35160.430
  Total reward: -92571.078 ± 35160.430
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scA/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scA/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scA/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scA/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scA/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scA/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scA/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scA/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scA/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scA/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -49462.027 ± 394.331
  Average reward: -49462.027 ± 394.331
  Total reward: -49462.027 ± 394.331
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scA/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scA/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scA/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scA/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scA/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scA/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scA/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scA/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scA/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scA/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -49523.430 ± 719.674
  Average reward: -49523.430 ± 719.674
  Total reward: -49523.430 ± 719.674
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -92571.078
Rule-based avg reward: -49462.027
No control avg reward: -49523.430
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
