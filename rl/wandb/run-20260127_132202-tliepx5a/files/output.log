Iteration 0: 100%|██████████| 10/10 [00:21<00:00,  2.16s/it, episode=10, norm_ret=-12.733, true_ret=-230133280.000, steps=600]
Agent gate_2 episode reward: [-79.37619247]
All agents episode reward: [-79.37619247]
Agent gate_2 episode reward: [-21.99161108]
All agents episode reward: [-21.99161108]
Agent gate_2 episode reward: [-3.78537223]
All agents episode reward: [-3.78537223]
Agent gate_2 episode reward: [-12.74087006]
All agents episode reward: [-12.74087006]
Agent gate_2 episode reward: [-0.56628661]
All agents episode reward: [-0.56628661]
Agent gate_2 episode reward: [-1.8808506]
All agents episode reward: [-1.8808506]
Agent gate_2 episode reward: [-4.44876467]
All agents episode reward: [-4.44876467]
Agent gate_2 episode reward: [-0.63262996]
All agents episode reward: [-0.63262996]
Agent gate_2 episode reward: [-1.04041413]
All agents episode reward: [-1.04041413]
Agent gate_2 episode reward: [-0.86562981]
All agents episode reward: [-0.86562981]
Iteration 1: 100%|██████████| 10/10 [00:24<00:00,  2.47s/it, episode=20, norm_ret=-0.947, true_ret=-175893104.000, steps=600]
Agent gate_2 episode reward: [-1.8791698]
All agents episode reward: [-1.8791698]
Agent gate_2 episode reward: [-0.70949823]
All agents episode reward: [-0.70949823]
Agent gate_2 episode reward: [-0.71166404]
All agents episode reward: [-0.71166404]
Agent gate_2 episode reward: [-0.79307607]
All agents episode reward: [-0.79307607]
Agent gate_2 episode reward: [-0.77757436]
All agents episode reward: [-0.77757436]
Agent gate_2 episode reward: [-0.83323737]
All agents episode reward: [-0.83323737]
Agent gate_2 episode reward: [-1.14927085]
All agents episode reward: [-1.14927085]
Agent gate_2 episode reward: [-0.86350502]
All agents episode reward: [-0.86350502]
Agent gate_2 episode reward: [-0.86419544]
All agents episode reward: [-0.86419544]
Agent gate_2 episode reward: [-0.8892643]
All agents episode reward: [-0.8892643]
Iteration 2: 100%|██████████| 10/10 [00:21<00:00,  2.19s/it, episode=30, norm_ret=-1.039, true_ret=-194568272.000, steps=600]
Agent gate_2 episode reward: [-0.9092623]
All agents episode reward: [-0.9092623]
Agent gate_2 episode reward: [-0.96237309]
All agents episode reward: [-0.96237309]
Agent gate_2 episode reward: [-0.96445933]
All agents episode reward: [-0.96445933]
Agent gate_2 episode reward: [-0.97456368]
All agents episode reward: [-0.97456368]
Agent gate_2 episode reward: [-1.06521487]
All agents episode reward: [-1.06521487]
Agent gate_2 episode reward: [-0.96566862]
All agents episode reward: [-0.96566862]
Agent gate_2 episode reward: [-1.15947501]
All agents episode reward: [-1.15947501]
Agent gate_2 episode reward: [-0.9884931]
All agents episode reward: [-0.9884931]
Agent gate_2 episode reward: [-1.21794056]
All agents episode reward: [-1.21794056]
Agent gate_2 episode reward: [-1.18386072]
All agents episode reward: [-1.18386072]
Iteration 3: 100%|██████████| 10/10 [00:21<00:00,  2.18s/it, episode=40, norm_ret=-1.150, true_ret=-174251520.000, steps=600]
Agent gate_2 episode reward: [-1.10193897]
All agents episode reward: [-1.10193897]
Agent gate_2 episode reward: [-1.07696617]
All agents episode reward: [-1.07696617]
Agent gate_2 episode reward: [-1.1089558]
All agents episode reward: [-1.1089558]
Agent gate_2 episode reward: [-1.11324435]
All agents episode reward: [-1.11324435]
Agent gate_2 episode reward: [-1.15162369]
All agents episode reward: [-1.15162369]
Agent gate_2 episode reward: [-1.096127]
All agents episode reward: [-1.096127]
Agent gate_2 episode reward: [-1.24168496]
All agents episode reward: [-1.24168496]
Agent gate_2 episode reward: [-1.18562943]
All agents episode reward: [-1.18562943]
Agent gate_2 episode reward: [-1.20710638]
All agents episode reward: [-1.20710638]
Agent gate_2 episode reward: [-1.21305393]
All agents episode reward: [-1.21305393]
Iteration 4: 100%|██████████| 10/10 [00:23<00:00,  2.35s/it, episode=50, norm_ret=-1.311, true_ret=-176847680.000, steps=600]
Agent gate_2 episode reward: [-1.30067025]
All agents episode reward: [-1.30067025]
Agent gate_2 episode reward: [-1.28259237]
All agents episode reward: [-1.28259237]
Agent gate_2 episode reward: [-1.24832034]
All agents episode reward: [-1.24832034]
Agent gate_2 episode reward: [-1.27473158]
All agents episode reward: [-1.27473158]
Agent gate_2 episode reward: [-1.29251151]
All agents episode reward: [-1.29251151]
Agent gate_2 episode reward: [-1.30904323]
All agents episode reward: [-1.30904323]
Agent gate_2 episode reward: [-1.33364137]
All agents episode reward: [-1.33364137]
Agent gate_2 episode reward: [-1.34788518]
All agents episode reward: [-1.34788518]
Agent gate_2 episode reward: [-1.34912798]
All agents episode reward: [-1.34912798]
Agent gate_2 episode reward: [-1.36822441]
All agents episode reward: [-1.36822441]
Iteration 5: 100%|██████████| 10/10 [00:22<00:00,  2.23s/it, episode=60, norm_ret=-1.453, true_ret=-181881856.000, steps=600]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -177335504.000 at episode 51 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-1.38495771]
All agents episode reward: [-1.38495771]
Agent gate_2 episode reward: [-1.41143428]
All agents episode reward: [-1.41143428]
Agent gate_2 episode reward: [-1.42342937]
All agents episode reward: [-1.42342937]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -175027088.000 at episode 54 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-1.40457199]
All agents episode reward: [-1.40457199]
Agent gate_2 episode reward: [-1.42532463]
All agents episode reward: [-1.42532463]
Agent gate_2 episode reward: [-1.49973043]
All agents episode reward: [-1.49973043]
Agent gate_2 episode reward: [-1.47095364]
All agents episode reward: [-1.47095364]
Agent gate_2 episode reward: [-1.48295704]
All agents episode reward: [-1.48295704]
Agent gate_2 episode reward: [-1.49462716]
All agents episode reward: [-1.49462716]
Agent gate_2 episode reward: [-1.53468775]
All agents episode reward: [-1.53468775]
Iteration 6: 100%|██████████| 10/10 [00:22<00:00,  2.28s/it, episode=70, norm_ret=-1.551, true_ret=-175769344.000, steps=600]
Agent gate_2 episode reward: [-1.4912932]
All agents episode reward: [-1.4912932]
Agent gate_2 episode reward: [-1.55850949]
All agents episode reward: [-1.55850949]
Agent gate_2 episode reward: [-1.52220332]
All agents episode reward: [-1.52220332]
Agent gate_2 episode reward: [-1.52846572]
All agents episode reward: [-1.52846572]
Agent gate_2 episode reward: [-1.5363889]
All agents episode reward: [-1.5363889]
Agent gate_2 episode reward: [-1.55420939]
All agents episode reward: [-1.55420939]
Agent gate_2 episode reward: [-1.55934767]
All agents episode reward: [-1.55934767]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -173558176.000 at episode 68 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-1.55481983]
All agents episode reward: [-1.55481983]
Agent gate_2 episode reward: [-1.6061362]
All agents episode reward: [-1.6061362]
Agent gate_2 episode reward: [-1.59659929]
All agents episode reward: [-1.59659929]
Iteration 7: 100%|██████████| 10/10 [00:22<00:00,  2.24s/it, episode=80, norm_ret=-1.672, true_ret=-176743376.000, steps=600]
Agent gate_2 episode reward: [-1.63359221]
All agents episode reward: [-1.63359221]
Agent gate_2 episode reward: [-1.63990286]
All agents episode reward: [-1.63990286]
Agent gate_2 episode reward: [-1.62231912]
All agents episode reward: [-1.62231912]
Agent gate_2 episode reward: [-1.64798101]
All agents episode reward: [-1.64798101]
Agent gate_2 episode reward: [-1.71666002]
All agents episode reward: [-1.71666002]
Agent gate_2 episode reward: [-1.66024171]
All agents episode reward: [-1.66024171]
Agent gate_2 episode reward: [-1.68594997]
All agents episode reward: [-1.68594997]
Agent gate_2 episode reward: [-1.71179597]
All agents episode reward: [-1.71179597]
Agent gate_2 episode reward: [-1.69122604]
All agents episode reward: [-1.69122604]
Agent gate_2 episode reward: [-1.71126686]
All agents episode reward: [-1.71126686]
Iteration 8: 100%|██████████| 10/10 [00:22<00:00,  2.24s/it, episode=90, norm_ret=-1.785, true_ret=-178453232.000, steps=600]
Agent gate_2 episode reward: [-1.7774213]
All agents episode reward: [-1.7774213]
Agent gate_2 episode reward: [-1.73258618]
All agents episode reward: [-1.73258618]
Agent gate_2 episode reward: [-1.72196195]
All agents episode reward: [-1.72196195]
Agent gate_2 episode reward: [-1.77944634]
All agents episode reward: [-1.77944634]
Agent gate_2 episode reward: [-1.77547465]
All agents episode reward: [-1.77547465]
Agent gate_2 episode reward: [-1.79996753]
All agents episode reward: [-1.79996753]
Agent gate_2 episode reward: [-1.79116685]
All agents episode reward: [-1.79116685]
Agent gate_2 episode reward: [-1.81158572]
All agents episode reward: [-1.81158572]
Agent gate_2 episode reward: [-1.82737717]
All agents episode reward: [-1.82737717]
Agent gate_2 episode reward: [-1.82802423]
All agents episode reward: [-1.82802423]
Iteration 9: 100%|██████████| 10/10 [00:22<00:00,  2.27s/it, episode=100, norm_ret=-1.872, true_ret=-178028304.000, steps=600]
Agent gate_2 episode reward: [-1.84594885]
All agents episode reward: [-1.84594885]
Agent gate_2 episode reward: [-1.83337296]
All agents episode reward: [-1.83337296]
Agent gate_2 episode reward: [-1.84118073]
All agents episode reward: [-1.84118073]
Agent gate_2 episode reward: [-1.85618209]
All agents episode reward: [-1.85618209]
Agent gate_2 episode reward: [-1.84487083]
All agents episode reward: [-1.84487083]
Agent gate_2 episode reward: [-1.89802299]
All agents episode reward: [-1.89802299]
Agent gate_2 episode reward: [-1.88688879]
All agents episode reward: [-1.88688879]
Agent gate_2 episode reward: [-1.90588544]
All agents episode reward: [-1.90588544]
Agent gate_2 episode reward: [-1.89426101]
All agents episode reward: [-1.89426101]
Agent gate_2 episode reward: [-1.91789872]
All agents episode reward: [-1.91789872]
Loaded 1 agents from ppo_agents_butterfly_scA
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scA/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scA/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scA/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scA/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scA/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scA/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scA/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scA/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scA/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scA/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -178211232.000 ± 2461875.500
  Average reward: -178211232.000 ± 2461875.500
  Total reward: -178211232.000 ± 2461875.500
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scA/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scA/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scA/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scA/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scA/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scA/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scA/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scA/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scA/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scA/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -177629808.000 ± 2093861.250
  Average reward: -177629808.000 ± 2093861.250
  Total reward: -177629808.000 ± 2093861.250
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scA/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scA/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scA/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scA/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scA/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scA/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scA/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scA/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scA/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scA/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -178847072.000 ± 2075384.875
  Average reward: -178847072.000 ± 2075384.875
  Total reward: -178847072.000 ± 2075384.875
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -178211232.000
Rule-based avg reward: -177629808.000
No control avg reward: -178847072.000
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
