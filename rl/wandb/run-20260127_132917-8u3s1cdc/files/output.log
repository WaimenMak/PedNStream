Iteration 0: 100%|██████████| 10/10 [00:24<00:00,  2.44s/it, episode=10, norm_ret=-10.496, true_ret=-49826.137, steps=600]
Agent gate_2 episode reward: [-62.0136035]
All agents episode reward: [-62.0136035]
Agent gate_2 episode reward: [-20.92966604]
All agents episode reward: [-20.92966604]
Agent gate_2 episode reward: [-1.8654677]
All agents episode reward: [-1.8654677]
Agent gate_2 episode reward: [-2.06960899]
All agents episode reward: [-2.06960899]
Agent gate_2 episode reward: [-2.50730418]
All agents episode reward: [-2.50730418]
Agent gate_2 episode reward: [-3.08496937]
All agents episode reward: [-3.08496937]
Agent gate_2 episode reward: [-4.41548334]
All agents episode reward: [-4.41548334]
Agent gate_2 episode reward: [-2.51939873]
All agents episode reward: [-2.51939873]
Agent gate_2 episode reward: [-2.68993648]
All agents episode reward: [-2.68993648]
Agent gate_2 episode reward: [-2.86243179]
All agents episode reward: [-2.86243179]
Iteration 1: 100%|██████████| 10/10 [00:21<00:00,  2.17s/it, episode=20, norm_ret=-3.367, true_ret=-45778.930, steps=600]
Agent gate_2 episode reward: [-2.97529449]
All agents episode reward: [-2.97529449]
Agent gate_2 episode reward: [-2.96155453]
All agents episode reward: [-2.96155453]
Agent gate_2 episode reward: [-3.1413975]
All agents episode reward: [-3.1413975]
Agent gate_2 episode reward: [-3.21466997]
All agents episode reward: [-3.21466997]
Agent gate_2 episode reward: [-3.41218107]
All agents episode reward: [-3.41218107]
Agent gate_2 episode reward: [-3.40006756]
All agents episode reward: [-3.40006756]
Agent gate_2 episode reward: [-3.53433528]
All agents episode reward: [-3.53433528]
Agent gate_2 episode reward: [-3.73440192]
All agents episode reward: [-3.73440192]
Agent gate_2 episode reward: [-3.72999881]
All agents episode reward: [-3.72999881]
Agent gate_2 episode reward: [-3.56577846]
All agents episode reward: [-3.56577846]
Iteration 2: 100%|██████████| 10/10 [00:21<00:00,  2.15s/it, episode=30, norm_ret=-4.337, true_ret=-50400.535, steps=600]
Agent gate_2 episode reward: [-3.85655383]
All agents episode reward: [-3.85655383]
Agent gate_2 episode reward: [-3.87270066]
All agents episode reward: [-3.87270066]
Agent gate_2 episode reward: [-4.35137033]
All agents episode reward: [-4.35137033]
Agent gate_2 episode reward: [-4.03260944]
All agents episode reward: [-4.03260944]
Agent gate_2 episode reward: [-4.11106691]
All agents episode reward: [-4.11106691]
Agent gate_2 episode reward: [-5.7563059]
All agents episode reward: [-5.7563059]
Agent gate_2 episode reward: [-4.18173089]
All agents episode reward: [-4.18173089]
Agent gate_2 episode reward: [-4.32477253]
All agents episode reward: [-4.32477253]
Agent gate_2 episode reward: [-4.23531951]
All agents episode reward: [-4.23531951]
Agent gate_2 episode reward: [-4.64532677]
All agents episode reward: [-4.64532677]
Iteration 3: 100%|██████████| 10/10 [00:21<00:00,  2.17s/it, episode=40, norm_ret=-4.868, true_ret=-49783.941, steps=600]
Agent gate_2 episode reward: [-4.44788023]
All agents episode reward: [-4.44788023]
Agent gate_2 episode reward: [-4.80167943]
All agents episode reward: [-4.80167943]
Agent gate_2 episode reward: [-4.70521133]
All agents episode reward: [-4.70521133]
Agent gate_2 episode reward: [-4.79192618]
All agents episode reward: [-4.79192618]
Agent gate_2 episode reward: [-4.91764638]
All agents episode reward: [-4.91764638]
Agent gate_2 episode reward: [-4.84274045]
All agents episode reward: [-4.84274045]
Agent gate_2 episode reward: [-4.91759258]
All agents episode reward: [-4.91759258]
Agent gate_2 episode reward: [-5.0960261]
All agents episode reward: [-5.0960261]
Agent gate_2 episode reward: [-4.99625218]
All agents episode reward: [-4.99625218]
Agent gate_2 episode reward: [-5.16126036]
All agents episode reward: [-5.16126036]
Iteration 4: 100%|██████████| 10/10 [00:21<00:00,  2.20s/it, episode=50, norm_ret=-5.336, true_ret=-48221.527, steps=600]
Agent gate_2 episode reward: [-5.11477183]
All agents episode reward: [-5.11477183]
Agent gate_2 episode reward: [-4.97022928]
All agents episode reward: [-4.97022928]
Agent gate_2 episode reward: [-5.15227852]
All agents episode reward: [-5.15227852]
Agent gate_2 episode reward: [-5.09631261]
All agents episode reward: [-5.09631261]
Agent gate_2 episode reward: [-6.69826207]
All agents episode reward: [-6.69826207]
Agent gate_2 episode reward: [-5.0165972]
All agents episode reward: [-5.0165972]
Agent gate_2 episode reward: [-5.10913597]
All agents episode reward: [-5.10913597]
Agent gate_2 episode reward: [-5.3782841]
All agents episode reward: [-5.3782841]
Agent gate_2 episode reward: [-5.38379137]
All agents episode reward: [-5.38379137]
Agent gate_2 episode reward: [-5.43728702]
All agents episode reward: [-5.43728702]
Iteration 5: 100%|██████████| 10/10 [00:21<00:00,  2.19s/it, episode=60, norm_ret=-5.554, true_ret=-48636.504, steps=600]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -46453.766 at episode 51 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-5.27795108]
All agents episode reward: [-5.27795108]
Agent gate_2 episode reward: [-5.33886247]
All agents episode reward: [-5.33886247]
Agent gate_2 episode reward: [-5.43180746]
All agents episode reward: [-5.43180746]
Agent gate_2 episode reward: [-5.89073896]
All agents episode reward: [-5.89073896]
Agent gate_2 episode reward: [-5.55698834]
All agents episode reward: [-5.55698834]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -43390.707 at episode 56 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-5.10612299]
All agents episode reward: [-5.10612299]
Agent gate_2 episode reward: [-5.46785797]
All agents episode reward: [-5.46785797]
Agent gate_2 episode reward: [-5.78612378]
All agents episode reward: [-5.78612378]
Agent gate_2 episode reward: [-5.81136688]
All agents episode reward: [-5.81136688]
Agent gate_2 episode reward: [-5.87513649]
All agents episode reward: [-5.87513649]
Iteration 6: 100%|██████████| 10/10 [00:23<00:00,  2.32s/it, episode=70, norm_ret=-5.996, true_ret=-46838.680, steps=600]
Agent gate_2 episode reward: [-5.84513783]
All agents episode reward: [-5.84513783]
Agent gate_2 episode reward: [-5.79849816]
All agents episode reward: [-5.79849816]
Agent gate_2 episode reward: [-5.83001422]
All agents episode reward: [-5.83001422]
Agent gate_2 episode reward: [-5.84391904]
All agents episode reward: [-5.84391904]
Agent gate_2 episode reward: [-6.03890358]
All agents episode reward: [-6.03890358]
Agent gate_2 episode reward: [-6.04240106]
All agents episode reward: [-6.04240106]
Agent gate_2 episode reward: [-5.97214023]
All agents episode reward: [-5.97214023]
Agent gate_2 episode reward: [-6.18260959]
All agents episode reward: [-6.18260959]
Agent gate_2 episode reward: [-6.42512133]
All agents episode reward: [-6.42512133]
Agent gate_2 episode reward: [-5.98216207]
All agents episode reward: [-5.98216207]
Iteration 7: 100%|██████████| 10/10 [00:21<00:00,  2.15s/it, episode=80, norm_ret=-6.307, true_ret=-47754.148, steps=600]
Agent gate_2 episode reward: [-6.3101429]
All agents episode reward: [-6.3101429]
Agent gate_2 episode reward: [-5.99036531]
All agents episode reward: [-5.99036531]
Agent gate_2 episode reward: [-6.30312705]
All agents episode reward: [-6.30312705]
Agent gate_2 episode reward: [-6.34400541]
All agents episode reward: [-6.34400541]
Agent gate_2 episode reward: [-6.28575284]
All agents episode reward: [-6.28575284]
Agent gate_2 episode reward: [-6.34754357]
All agents episode reward: [-6.34754357]
Agent gate_2 episode reward: [-6.362484]
All agents episode reward: [-6.362484]
Agent gate_2 episode reward: [-6.17706799]
All agents episode reward: [-6.17706799]
Agent gate_2 episode reward: [-6.55898651]
All agents episode reward: [-6.55898651]
Agent gate_2 episode reward: [-6.39118138]
All agents episode reward: [-6.39118138]
Iteration 8: 100%|██████████| 10/10 [00:23<00:00,  2.31s/it, episode=90, norm_ret=-6.282, true_ret=-49190.387, steps=600]
Agent gate_2 episode reward: [-6.34249347]
All agents episode reward: [-6.34249347]
Agent gate_2 episode reward: [-6.59751473]
All agents episode reward: [-6.59751473]
Agent gate_2 episode reward: [-6.32151102]
All agents episode reward: [-6.32151102]
Agent gate_2 episode reward: [-5.93263809]
All agents episode reward: [-5.93263809]
Saved 1 agents to ppo_agents_butterfly_scA
New best average return achieved: -38765.867 at episode 85 (saved all agents to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-5.30110129]
All agents episode reward: [-5.30110129]
Agent gate_2 episode reward: [-5.94402184]
All agents episode reward: [-5.94402184]
Agent gate_2 episode reward: [-7.04305453]
All agents episode reward: [-7.04305453]
Agent gate_2 episode reward: [-6.00526254]
All agents episode reward: [-6.00526254]
Agent gate_2 episode reward: [-6.4761386]
All agents episode reward: [-6.4761386]
Agent gate_2 episode reward: [-6.86038619]
All agents episode reward: [-6.86038619]
Iteration 9: 100%|██████████| 10/10 [00:25<00:00,  2.55s/it, episode=100, norm_ret=-6.726, true_ret=-46230.180, steps=600]
Agent gate_2 episode reward: [-6.70940473]
All agents episode reward: [-6.70940473]
Agent gate_2 episode reward: [-5.98396299]
All agents episode reward: [-5.98396299]
Agent gate_2 episode reward: [-6.70561531]
All agents episode reward: [-6.70561531]
Agent gate_2 episode reward: [-6.77289966]
All agents episode reward: [-6.77289966]
Agent gate_2 episode reward: [-6.72395228]
All agents episode reward: [-6.72395228]
Agent gate_2 episode reward: [-6.90873442]
All agents episode reward: [-6.90873442]
Agent gate_2 episode reward: [-6.99256744]
All agents episode reward: [-6.99256744]
Agent gate_2 episode reward: [-6.96803916]
All agents episode reward: [-6.96803916]
Agent gate_2 episode reward: [-6.82051724]
All agents episode reward: [-6.82051724]
Agent gate_2 episode reward: [-6.67225698]
All agents episode reward: [-6.67225698]
Loaded 1 agents from ppo_agents_butterfly_scA
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scA/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scA/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scA/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scA/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scA/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scA/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scA/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scA/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scA/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scA/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -122632.391 ± 52356.805
  Average reward: -122632.391 ± 52356.805
  Total reward: -122632.391 ± 52356.805
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scA/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scA/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scA/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scA/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scA/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scA/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scA/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scA/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scA/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scA/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -49392.738 ± 428.117
  Average reward: -49392.738 ± 428.117
  Total reward: -49392.738 ± 428.117
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scA/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scA/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scA/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scA/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scA/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scA/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scA/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scA/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scA/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scA/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -49602.078 ± 725.482
  Average reward: -49602.078 ± 725.482
  Total reward: -49602.078 ± 725.482
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -122632.391
Rule-based avg reward: -49392.738
No control avg reward: -49602.078
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
