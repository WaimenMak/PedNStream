Iteration 0: 100%|██████████| 10/10 [00:23<00:00,  2.38s/it, episode=10, norm_ret=-9.299, true_ret=-7185.245, steps=600]
Agent gate_2 episode reward: [-68.71506987]
All agents episode reward: [-68.71506987]
Agent gate_2 episode reward: [-13.72038774]
All agents episode reward: [-13.72038774]
Agent gate_2 episode reward: [-4.1419669]
All agents episode reward: [-4.1419669]
Agent gate_2 episode reward: [-1.93091966]
All agents episode reward: [-1.93091966]
Agent gate_2 episode reward: [-1.51457406]
All agents episode reward: [-1.51457406]
Agent gate_2 episode reward: [-0.68140044]
All agents episode reward: [-0.68140044]
Agent gate_2 episode reward: [-0.55256613]
All agents episode reward: [-0.55256613]
Agent gate_2 episode reward: [-0.7863401]
All agents episode reward: [-0.7863401]
Agent gate_2 episode reward: [-0.53222951]
All agents episode reward: [-0.53222951]
Agent gate_2 episode reward: [-0.41134048]
All agents episode reward: [-0.41134048]
Iteration 1: 100%|██████████| 10/10 [00:22<00:00,  2.21s/it, episode=20, norm_ret=-0.475, true_ret=-4633.748, steps=600]
Agent gate_2 episode reward: [-0.36539198]
All agents episode reward: [-0.36539198]
Agent gate_2 episode reward: [-0.45112976]
All agents episode reward: [-0.45112976]
Agent gate_2 episode reward: [-0.39559461]
All agents episode reward: [-0.39559461]
Agent gate_2 episode reward: [-0.38148311]
All agents episode reward: [-0.38148311]
Agent gate_2 episode reward: [-0.71780029]
All agents episode reward: [-0.71780029]
Agent gate_2 episode reward: [-0.58451413]
All agents episode reward: [-0.58451413]
Agent gate_2 episode reward: [-0.48885403]
All agents episode reward: [-0.48885403]
Agent gate_2 episode reward: [-0.52965912]
All agents episode reward: [-0.52965912]
Agent gate_2 episode reward: [-0.48469675]
All agents episode reward: [-0.48469675]
Agent gate_2 episode reward: [-0.35234829]
All agents episode reward: [-0.35234829]
Iteration 2: 100%|██████████| 10/10 [00:22<00:00,  2.22s/it, episode=30, norm_ret=-0.372, true_ret=-4080.229, steps=600]
Agent gate_2 episode reward: [-0.37692597]
All agents episode reward: [-0.37692597]
Agent gate_2 episode reward: [-0.34577543]
All agents episode reward: [-0.34577543]
Agent gate_2 episode reward: [-0.36297218]
All agents episode reward: [-0.36297218]
Agent gate_2 episode reward: [-0.3276183]
All agents episode reward: [-0.3276183]
Agent gate_2 episode reward: [-0.35478203]
All agents episode reward: [-0.35478203]
Agent gate_2 episode reward: [-0.3788358]
All agents episode reward: [-0.3788358]
Agent gate_2 episode reward: [-0.39810395]
All agents episode reward: [-0.39810395]
Agent gate_2 episode reward: [-0.41414529]
All agents episode reward: [-0.41414529]
Agent gate_2 episode reward: [-0.38621009]
All agents episode reward: [-0.38621009]
Agent gate_2 episode reward: [-0.37202526]
All agents episode reward: [-0.37202526]
Iteration 3: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it, episode=40, norm_ret=-0.393, true_ret=-3850.482, steps=600]
Agent gate_2 episode reward: [-0.35957843]
All agents episode reward: [-0.35957843]
Agent gate_2 episode reward: [-0.36372674]
All agents episode reward: [-0.36372674]
Agent gate_2 episode reward: [-0.39786032]
All agents episode reward: [-0.39786032]
Agent gate_2 episode reward: [-0.38646839]
All agents episode reward: [-0.38646839]
Agent gate_2 episode reward: [-0.3899456]
All agents episode reward: [-0.3899456]
Agent gate_2 episode reward: [-0.39762918]
All agents episode reward: [-0.39762918]
Agent gate_2 episode reward: [-0.40003335]
All agents episode reward: [-0.40003335]
Agent gate_2 episode reward: [-0.43752266]
All agents episode reward: [-0.43752266]
Agent gate_2 episode reward: [-0.39363509]
All agents episode reward: [-0.39363509]
Agent gate_2 episode reward: [-0.40110676]
All agents episode reward: [-0.40110676]
Iteration 4: 100%|██████████| 10/10 [00:21<00:00,  2.16s/it, episode=50, norm_ret=-0.451, true_ret=-3947.809, steps=600]
Agent gate_2 episode reward: [-0.41654232]
All agents episode reward: [-0.41654232]
Agent gate_2 episode reward: [-0.40902967]
All agents episode reward: [-0.40902967]
Agent gate_2 episode reward: [-0.45276497]
All agents episode reward: [-0.45276497]
Agent gate_2 episode reward: [-0.42313794]
All agents episode reward: [-0.42313794]
Agent gate_2 episode reward: [-0.45588483]
All agents episode reward: [-0.45588483]
Agent gate_2 episode reward: [-0.50875988]
All agents episode reward: [-0.50875988]
Agent gate_2 episode reward: [-0.45981781]
All agents episode reward: [-0.45981781]
Agent gate_2 episode reward: [-0.44544638]
All agents episode reward: [-0.44544638]
Agent gate_2 episode reward: [-0.48184737]
All agents episode reward: [-0.48184737]
Agent gate_2 episode reward: [-0.45691181]
All agents episode reward: [-0.45691181]
Iteration 5: 100%|██████████| 10/10 [00:22<00:00,  2.23s/it, episode=60, norm_ret=-0.499, true_ret=-4084.597, steps=600]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3829.352 at episode 51 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.44738699]
All agents episode reward: [-0.44738699]
Agent gate_2 episode reward: [-0.50436174]
All agents episode reward: [-0.50436174]
Agent gate_2 episode reward: [-0.49433976]
All agents episode reward: [-0.49433976]
Agent gate_2 episode reward: [-0.49167319]
All agents episode reward: [-0.49167319]
Agent gate_2 episode reward: [-0.48369198]
All agents episode reward: [-0.48369198]
Agent gate_2 episode reward: [-0.50110574]
All agents episode reward: [-0.50110574]
Agent gate_2 episode reward: [-0.51492388]
All agents episode reward: [-0.51492388]
Agent gate_2 episode reward: [-0.50510982]
All agents episode reward: [-0.50510982]
Agent gate_2 episode reward: [-0.53615637]
All agents episode reward: [-0.53615637]
Agent gate_2 episode reward: [-0.51566363]
All agents episode reward: [-0.51566363]
Iteration 6: 100%|██████████| 10/10 [00:21<00:00,  2.18s/it, episode=70, norm_ret=-0.526, true_ret=-3846.928, steps=600]
Agent gate_2 episode reward: [-0.53931906]
All agents episode reward: [-0.53931906]
Agent gate_2 episode reward: [-0.49621796]
All agents episode reward: [-0.49621796]
Agent gate_2 episode reward: [-0.54394956]
All agents episode reward: [-0.54394956]
Agent gate_2 episode reward: [-0.52442938]
All agents episode reward: [-0.52442938]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3747.961 at episode 65 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.49170318]
All agents episode reward: [-0.49170318]
Agent gate_2 episode reward: [-0.51752756]
All agents episode reward: [-0.51752756]
Agent gate_2 episode reward: [-0.52131748]
All agents episode reward: [-0.52131748]
Agent gate_2 episode reward: [-0.53468429]
All agents episode reward: [-0.53468429]
Agent gate_2 episode reward: [-0.56795355]
All agents episode reward: [-0.56795355]
Agent gate_2 episode reward: [-0.52304596]
All agents episode reward: [-0.52304596]
Iteration 7: 100%|██████████| 10/10 [00:21<00:00,  2.19s/it, episode=80, norm_ret=-0.567, true_ret=-4075.264, steps=600]
Agent gate_2 episode reward: [-0.55402426]
All agents episode reward: [-0.55402426]
Agent gate_2 episode reward: [-0.56761303]
All agents episode reward: [-0.56761303]
Agent gate_2 episode reward: [-0.54682098]
All agents episode reward: [-0.54682098]
Agent gate_2 episode reward: [-0.53908309]
All agents episode reward: [-0.53908309]
Agent gate_2 episode reward: [-0.54828688]
All agents episode reward: [-0.54828688]
Agent gate_2 episode reward: [-0.5474264]
All agents episode reward: [-0.5474264]
Agent gate_2 episode reward: [-0.6046115]
All agents episode reward: [-0.6046115]
Agent gate_2 episode reward: [-0.57178717]
All agents episode reward: [-0.57178717]
Agent gate_2 episode reward: [-0.59620579]
All agents episode reward: [-0.59620579]
Agent gate_2 episode reward: [-0.59093553]
All agents episode reward: [-0.59093553]
Iteration 8: 100%|██████████| 10/10 [00:26<00:00,  2.65s/it, episode=90, norm_ret=-0.593, true_ret=-3945.412, steps=600]
Agent gate_2 episode reward: [-0.58643375]
All agents episode reward: [-0.58643375]
Agent gate_2 episode reward: [-0.58558395]
All agents episode reward: [-0.58558395]
Agent gate_2 episode reward: [-0.58035922]
All agents episode reward: [-0.58035922]
Agent gate_2 episode reward: [-0.60284156]
All agents episode reward: [-0.60284156]
Agent gate_2 episode reward: [-0.59945871]
All agents episode reward: [-0.59945871]
Agent gate_2 episode reward: [-0.56562264]
All agents episode reward: [-0.56562264]
Agent gate_2 episode reward: [-0.59786347]
All agents episode reward: [-0.59786347]
Agent gate_2 episode reward: [-0.59958793]
All agents episode reward: [-0.59958793]
Agent gate_2 episode reward: [-0.60743687]
All agents episode reward: [-0.60743687]
Agent gate_2 episode reward: [-0.60571909]
All agents episode reward: [-0.60571909]
Iteration 9: 100%|██████████| 10/10 [00:21<00:00,  2.17s/it, episode=100, norm_ret=-0.617, true_ret=-3834.131, steps=600]
Agent gate_2 episode reward: [-0.64173367]
All agents episode reward: [-0.64173367]
Agent gate_2 episode reward: [-0.58466135]
All agents episode reward: [-0.58466135]
Saved 1 agents to ppo_agents_butterfly_scB
New best average return achieved: -3745.406 at episode 93 (saved all agents to ppo_agents_butterfly_scB)
Agent gate_2 episode reward: [-0.58421961]
All agents episode reward: [-0.58421961]
Agent gate_2 episode reward: [-0.61863072]
All agents episode reward: [-0.61863072]
Agent gate_2 episode reward: [-0.61148348]
All agents episode reward: [-0.61148348]
Agent gate_2 episode reward: [-0.63502946]
All agents episode reward: [-0.63502946]
Agent gate_2 episode reward: [-0.61887953]
All agents episode reward: [-0.61887953]
Agent gate_2 episode reward: [-0.60026848]
All agents episode reward: [-0.60026848]
Agent gate_2 episode reward: [-0.65486028]
All agents episode reward: [-0.65486028]
Agent gate_2 episode reward: [-0.61949353]
All agents episode reward: [-0.61949353]
Loaded 1 agents from ppo_agents_butterfly_scB
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scB/ppo_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scB/ppo_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scB/ppo_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scB/ppo_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scB/ppo_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scB/ppo_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scB/ppo_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scB/ppo_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scB/ppo_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scB/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -5021.928 ± 188.862
  Average reward: -5021.928 ± 188.862
  Total reward: -5021.928 ± 188.862
============================================================
Running 10 evaluation runs...
  Run 1/10... Saved run 1 to rl_training/butterfly_scB/rule_based_run1
  Run 2/10... Saved run 2 to rl_training/butterfly_scB/rule_based_run2
  Run 3/10... Saved run 3 to rl_training/butterfly_scB/rule_based_run3
  Run 4/10... Saved run 4 to rl_training/butterfly_scB/rule_based_run4
  Run 5/10... Saved run 5 to rl_training/butterfly_scB/rule_based_run5
  Run 6/10... Saved run 6 to rl_training/butterfly_scB/rule_based_run6
  Run 7/10... Saved run 7 to rl_training/butterfly_scB/rule_based_run7
  Run 8/10... Saved run 8 to rl_training/butterfly_scB/rule_based_run8
  Run 9/10... Saved run 9 to rl_training/butterfly_scB/rule_based_run9
  Run 10/10... Saved run 10 to rl_training/butterfly_scB/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -3886.159 ± 82.609
  Average reward: -3886.159 ± 82.609
  Total reward: -3886.159 ± 82.609
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Saved run 1 to rl_training/butterfly_scB/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Saved run 2 to rl_training/butterfly_scB/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Saved run 3 to rl_training/butterfly_scB/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Saved run 4 to rl_training/butterfly_scB/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Saved run 5 to rl_training/butterfly_scB/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Saved run 6 to rl_training/butterfly_scB/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Saved run 7 to rl_training/butterfly_scB/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Saved run 8 to rl_training/butterfly_scB/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Saved run 9 to rl_training/butterfly_scB/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Saved run 10 to rl_training/butterfly_scB/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -3904.970 ± 139.461
  Average reward: -3904.970 ± 139.461
  Total reward: -3904.970 ± 139.461
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -5021.928
Rule-based avg reward: -3886.159
No control avg reward: -3904.970
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
