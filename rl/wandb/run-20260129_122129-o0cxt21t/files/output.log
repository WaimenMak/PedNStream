Iteration 0:  80%|████████  | 16/20 [01:05<00:17,  4.27s/it, episode=10, norm_ret=-7.646, true_ret=-283627008.000, steps=600]
Agent gate_2 episode reward: [-75.08342424]
All agents episode reward: [-75.08342424]
Agent gate_2 episode reward: [-0.11650773]
All agents episode reward: [-0.11650773]
Agent gate_2 episode reward: [-0.10816877]
All agents episode reward: [-0.10816877]
Agent gate_2 episode reward: [-0.13312686]
All agents episode reward: [-0.13312686]
Agent gate_2 episode reward: [-0.1494683]
All agents episode reward: [-0.1494683]
Agent gate_2 episode reward: [-0.15177414]
All agents episode reward: [-0.15177414]
Agent gate_2 episode reward: [-0.15479222]
All agents episode reward: [-0.15479222]
Agent gate_2 episode reward: [-0.1627809]
All agents episode reward: [-0.1627809]
Agent gate_2 episode reward: [-0.20685793]
All agents episode reward: [-0.20685793]
Agent gate_2 episode reward: [-0.19299672]
All agents episode reward: [-0.19299672]
Agent gate_2 episode reward: [-0.19639093]
All agents episode reward: [-0.19639093]
Agent gate_2 episode reward: [-0.23119728]
All agents episode reward: [-0.23119728]
Agent gate_2 episode reward: [-0.21782839]
All agents episode reward: [-0.21782839]
Agent gate_2 episode reward: [-0.24151749]
All agents episode reward: [-0.24151749]
Agent gate_2 episode reward: [-0.23977612]
All agents episode reward: [-0.23977612]
Agent gate_2 episode reward: [-0.26159543]
All agents episode reward: [-0.26159543]
Agent gate_2 episode reward: [-0.25450318]
All agents episode reward: [-0.25450318]
Agent gate_2 episode reward: [-0.26640803]
All agents episode reward: [-0.26640803]
Agent gate_2 episode reward: [-0.26113383]
All agents episode reward: [-0.26113383]
Agent gate_2 episode reward: [-0.2900318]
All agents episode reward: [-0.2900318]
Iteration 1:  80%|████████  | 16/20 [01:06<00:16,  4.08s/it, episode=30, norm_ret=-0.310, true_ret=-272797536.000, steps=600]
Agent gate_2 episode reward: [-0.28337966]
All agents episode reward: [-0.28337966]
Agent gate_2 episode reward: [-0.28118039]
All agents episode reward: [-0.28118039]
Agent gate_2 episode reward: [-0.29434781]
All agents episode reward: [-0.29434781]
Agent gate_2 episode reward: [-0.32570483]
All agents episode reward: [-0.32570483]
Agent gate_2 episode reward: [-0.29604274]
All agents episode reward: [-0.29604274]
Agent gate_2 episode reward: [-0.31627271]
All agents episode reward: [-0.31627271]
Agent gate_2 episode reward: [-0.32723099]
All agents episode reward: [-0.32723099]
Agent gate_2 episode reward: [-0.33391086]
All agents episode reward: [-0.33391086]
Agent gate_2 episode reward: [-0.32718454]
All agents episode reward: [-0.32718454]
Agent gate_2 episode reward: [-0.31684257]
All agents episode reward: [-0.31684257]
Agent gate_2 episode reward: [-0.33716154]
All agents episode reward: [-0.33716154]
Agent gate_2 episode reward: [-0.35444873]
All agents episode reward: [-0.35444873]
Agent gate_2 episode reward: [-0.34791135]
All agents episode reward: [-0.34791135]
Agent gate_2 episode reward: [-0.35554332]
All agents episode reward: [-0.35554332]
Agent gate_2 episode reward: [-0.35455126]
All agents episode reward: [-0.35455126]
Agent gate_2 episode reward: [-0.36521053]
All agents episode reward: [-0.36521053]
Agent gate_2 episode reward: [-0.36874006]
All agents episode reward: [-0.36874006]
Agent gate_2 episode reward: [-0.3843103]
All agents episode reward: [-0.3843103]
Agent gate_2 episode reward: [-0.36822052]
All agents episode reward: [-0.36822052]
Agent gate_2 episode reward: [-0.4092706]
All agents episode reward: [-0.4092706]
Iteration 2:  80%|████████  | 16/20 [01:06<00:16,  4.17s/it, episode=50, norm_ret=-0.415, true_ret=-286778752.000, steps=600]
Agent gate_2 episode reward: [-0.3891893]
All agents episode reward: [-0.3891893]
Agent gate_2 episode reward: [-0.44338962]
All agents episode reward: [-0.44338962]
Agent gate_2 episode reward: [-0.39108985]
All agents episode reward: [-0.39108985]
Agent gate_2 episode reward: [-0.40072743]
All agents episode reward: [-0.40072743]
Agent gate_2 episode reward: [-0.40387683]
All agents episode reward: [-0.40387683]
Agent gate_2 episode reward: [-0.43381321]
All agents episode reward: [-0.43381321]
Agent gate_2 episode reward: [-0.40977525]
All agents episode reward: [-0.40977525]
Agent gate_2 episode reward: [-0.40280343]
All agents episode reward: [-0.40280343]
Agent gate_2 episode reward: [-0.44368956]
All agents episode reward: [-0.44368956]
Agent gate_2 episode reward: [-0.4287685]
All agents episode reward: [-0.4287685]
Agent gate_2 episode reward: [-0.43241237]
All agents episode reward: [-0.43241237]
Agent gate_2 episode reward: [-0.45999815]
All agents episode reward: [-0.45999815]
Agent gate_2 episode reward: [-0.45722276]
All agents episode reward: [-0.45722276]
Agent gate_2 episode reward: [-0.45857222]
All agents episode reward: [-0.45857222]
Agent gate_2 episode reward: [-0.47283408]
All agents episode reward: [-0.47283408]
Agent gate_2 episode reward: [-0.44299797]
All agents episode reward: [-0.44299797]
Agent gate_2 episode reward: [-0.45100903]
All agents episode reward: [-0.45100903]
Agent gate_2 episode reward: [-0.46562232]
All agents episode reward: [-0.46562232]
Agent gate_2 episode reward: [-0.45325413]
All agents episode reward: [-0.45325413]
Agent gate_2 episode reward: [-0.47752625]
All agents episode reward: [-0.47752625]
Iteration 3:  80%|████████  | 16/20 [01:07<00:16,  4.18s/it, episode=70, norm_ret=-0.510, true_ret=-304682496.000, steps=600]
Agent gate_2 episode reward: [-0.47337381]
All agents episode reward: [-0.47337381]
Agent gate_2 episode reward: [-0.51513431]
All agents episode reward: [-0.51513431]
Agent gate_2 episode reward: [-0.51718529]
All agents episode reward: [-0.51718529]
Agent gate_2 episode reward: [-0.49468547]
All agents episode reward: [-0.49468547]
Agent gate_2 episode reward: [-0.51299948]
All agents episode reward: [-0.51299948]
Agent gate_2 episode reward: [-0.51674593]
All agents episode reward: [-0.51674593]
Agent gate_2 episode reward: [-0.49878753]
All agents episode reward: [-0.49878753]
Agent gate_2 episode reward: [-0.51271247]
All agents episode reward: [-0.51271247]
Agent gate_2 episode reward: [-0.52180848]
All agents episode reward: [-0.52180848]
Agent gate_2 episode reward: [-0.53820097]
All agents episode reward: [-0.53820097]
Agent gate_2 episode reward: [-0.58390274]
All agents episode reward: [-0.58390274]
Agent gate_2 episode reward: [-0.5002692]
All agents episode reward: [-0.5002692]
Agent gate_2 episode reward: [-0.54934251]
All agents episode reward: [-0.54934251]
Agent gate_2 episode reward: [-0.54482606]
All agents episode reward: [-0.54482606]
Agent gate_2 episode reward: [-0.55960232]
All agents episode reward: [-0.55960232]
Agent gate_2 episode reward: [-0.5412762]
All agents episode reward: [-0.5412762]
Agent gate_2 episode reward: [-0.58226308]
All agents episode reward: [-0.58226308]
Agent gate_2 episode reward: [-0.53422778]
All agents episode reward: [-0.53422778]
Agent gate_2 episode reward: [-0.5365975]
All agents episode reward: [-0.5365975]
Agent gate_2 episode reward: [-0.5379684]
All agents episode reward: [-0.5379684]
Iteration 4:  80%|████████  | 16/20 [01:10<00:17,  4.41s/it, episode=90, norm_ret=-0.579, true_ret=-279705056.000, steps=600]
Agent gate_2 episode reward: [-0.56043917]
All agents episode reward: [-0.56043917]
Agent gate_2 episode reward: [-0.58518126]
All agents episode reward: [-0.58518126]
Agent gate_2 episode reward: [-0.5880456]
All agents episode reward: [-0.5880456]
Agent gate_2 episode reward: [-0.55619988]
All agents episode reward: [-0.55619988]
Agent gate_2 episode reward: [-0.58168234]
All agents episode reward: [-0.58168234]
Agent gate_2 episode reward: [-0.55338342]
All agents episode reward: [-0.55338342]
Agent gate_2 episode reward: [-0.58420797]
All agents episode reward: [-0.58420797]
Agent gate_2 episode reward: [-0.58426609]
All agents episode reward: [-0.58426609]
Agent gate_2 episode reward: [-0.63833545]
All agents episode reward: [-0.63833545]
Agent gate_2 episode reward: [-0.55974089]
All agents episode reward: [-0.55974089]
Agent gate_2 episode reward: [-0.59064583]
All agents episode reward: [-0.59064583]
Agent gate_2 episode reward: [-0.59542588]
All agents episode reward: [-0.59542588]
Agent gate_2 episode reward: [-0.59943464]
All agents episode reward: [-0.59943464]
Agent gate_2 episode reward: [-0.58928155]
All agents episode reward: [-0.58928155]
Agent gate_2 episode reward: [-0.64484487]
All agents episode reward: [-0.64484487]
Agent gate_2 episode reward: [-0.60234201]
All agents episode reward: [-0.60234201]
Agent gate_2 episode reward: [-0.65324236]
All agents episode reward: [-0.65324236]
Agent gate_2 episode reward: [-0.58068876]
All agents episode reward: [-0.58068876]
Agent gate_2 episode reward: [-0.64190177]
All agents episode reward: [-0.64190177]
Agent gate_2 episode reward: [-0.5904213]
All agents episode reward: [-0.5904213]
Iteration 5:  75%|███████▌  | 15/20 [01:06<00:20,  4.13s/it, episode=110, norm_ret=-0.623, true_ret=-292734624.000, steps=600]
Agent gate_2 episode reward: [-0.591394]
All agents episode reward: [-0.591394]
Agent gate_2 episode reward: [-0.61116008]
All agents episode reward: [-0.61116008]
Agent gate_2 episode reward: [-0.61942494]
All agents episode reward: [-0.61942494]
Agent gate_2 episode reward: [-0.620425]
All agents episode reward: [-0.620425]
Agent gate_2 episode reward: [-0.63952776]
All agents episode reward: [-0.63952776]
Agent gate_2 episode reward: [-0.639614]
All agents episode reward: [-0.639614]
Agent gate_2 episode reward: [-0.60442289]
All agents episode reward: [-0.60442289]
Agent gate_2 episode reward: [-0.60608763]
All agents episode reward: [-0.60608763]
Agent gate_2 episode reward: [-0.65051847]
All agents episode reward: [-0.65051847]
Saved 1 agents to sac_agents_butterfly_scC
[Validation] New best avg return: -288456416.000 at episode 110 (over 3 val episodes, saved to sac_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.64718246]
All agents episode reward: [-0.64718246]
Agent gate_2 episode reward: [-0.66393698]
All agents episode reward: [-0.66393698]
Agent gate_2 episode reward: [-0.68026739]
All agents episode reward: [-0.68026739]
Agent gate_2 episode reward: [-0.69576994]
All agents episode reward: [-0.69576994]
Agent gate_2 episode reward: [-0.65678814]
All agents episode reward: [-0.65678814]
Agent gate_2 episode reward: [-0.71999703]
All agents episode reward: [-0.71999703]
Agent gate_2 episode reward: [-0.69241764]
All agents episode reward: [-0.69241764]
Agent gate_2 episode reward: [-0.67713125]
All agents episode reward: [-0.67713125]
Agent gate_2 episode reward: [-0.61770052]
All agents episode reward: [-0.61770052]
Agent gate_2 episode reward: [-0.66093546]
All agents episode reward: [-0.66093546]
Agent gate_2 episode reward: [-0.69716336]
All agents episode reward: [-0.69716336]
Iteration 6:  75%|███████▌  | 15/20 [01:05<00:22,  4.42s/it, episode=130, norm_ret=-0.697, true_ret=-296661664.000, steps=600]
Agent gate_2 episode reward: [-0.74769218]
All agents episode reward: [-0.74769218]
Agent gate_2 episode reward: [-0.69280915]
All agents episode reward: [-0.69280915]
Agent gate_2 episode reward: [-0.69244685]
All agents episode reward: [-0.69244685]
Agent gate_2 episode reward: [-0.67511089]
All agents episode reward: [-0.67511089]
Agent gate_2 episode reward: [-0.67064119]
All agents episode reward: [-0.67064119]
Agent gate_2 episode reward: [-0.66254648]
All agents episode reward: [-0.66254648]
Agent gate_2 episode reward: [-0.69008184]
All agents episode reward: [-0.69008184]
Agent gate_2 episode reward: [-0.72360675]
All agents episode reward: [-0.72360675]
Agent gate_2 episode reward: [-0.68391973]
All agents episode reward: [-0.68391973]
Saved 1 agents to sac_agents_butterfly_scC
[Validation] New best avg return: -283071200.000 at episode 130 (over 3 val episodes, saved to sac_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.72872616]
All agents episode reward: [-0.72872616]
Agent gate_2 episode reward: [-0.72378032]
All agents episode reward: [-0.72378032]
Agent gate_2 episode reward: [-0.69410317]
All agents episode reward: [-0.69410317]
Agent gate_2 episode reward: [-0.77478428]
All agents episode reward: [-0.77478428]
Agent gate_2 episode reward: [-0.75569917]
All agents episode reward: [-0.75569917]
Agent gate_2 episode reward: [-0.71452341]
All agents episode reward: [-0.71452341]
Agent gate_2 episode reward: [-0.67900579]
All agents episode reward: [-0.67900579]
Agent gate_2 episode reward: [-0.7409148]
All agents episode reward: [-0.7409148]
Agent gate_2 episode reward: [-0.75929206]
All agents episode reward: [-0.75929206]
Agent gate_2 episode reward: [-0.71502748]
All agents episode reward: [-0.71502748]
Agent gate_2 episode reward: [-0.74942999]
All agents episode reward: [-0.74942999]
Iteration 7:  80%|████████  | 16/20 [01:09<00:17,  4.34s/it, episode=150, norm_ret=-0.791, true_ret=-301268064.000, steps=600]
Agent gate_2 episode reward: [-0.8067938]
All agents episode reward: [-0.8067938]
Agent gate_2 episode reward: [-0.79593627]
All agents episode reward: [-0.79593627]
Agent gate_2 episode reward: [-0.79513755]
All agents episode reward: [-0.79513755]
Agent gate_2 episode reward: [-0.79979649]
All agents episode reward: [-0.79979649]
Agent gate_2 episode reward: [-0.78391847]
All agents episode reward: [-0.78391847]
Agent gate_2 episode reward: [-0.77644275]
All agents episode reward: [-0.77644275]
Agent gate_2 episode reward: [-0.78026251]
All agents episode reward: [-0.78026251]
Agent gate_2 episode reward: [-0.75967292]
All agents episode reward: [-0.75967292]
Agent gate_2 episode reward: [-0.80916938]
All agents episode reward: [-0.80916938]
Agent gate_2 episode reward: [-0.80715129]
All agents episode reward: [-0.80715129]
Agent gate_2 episode reward: [-0.75516911]
All agents episode reward: [-0.75516911]
Agent gate_2 episode reward: [-0.79896527]
All agents episode reward: [-0.79896527]
Agent gate_2 episode reward: [-0.80533252]
All agents episode reward: [-0.80533252]
Agent gate_2 episode reward: [-0.82834632]
All agents episode reward: [-0.82834632]
Agent gate_2 episode reward: [-0.87405324]
All agents episode reward: [-0.87405324]
Agent gate_2 episode reward: [-0.77333221]
All agents episode reward: [-0.77333221]
Agent gate_2 episode reward: [-0.78453028]
All agents episode reward: [-0.78453028]
Agent gate_2 episode reward: [-0.76861183]
All agents episode reward: [-0.76861183]
Agent gate_2 episode reward: [-0.78805834]
All agents episode reward: [-0.78805834]
Agent gate_2 episode reward: [-0.82496608]
All agents episode reward: [-0.82496608]
Iteration 8:  80%|████████  | 16/20 [01:11<00:18,  4.53s/it, episode=170, norm_ret=-0.828, true_ret=-298970848.000, steps=600]
Agent gate_2 episode reward: [-0.80064108]
All agents episode reward: [-0.80064108]
Agent gate_2 episode reward: [-0.78636489]
All agents episode reward: [-0.78636489]
Agent gate_2 episode reward: [-0.82187936]
All agents episode reward: [-0.82187936]
Agent gate_2 episode reward: [-0.83798382]
All agents episode reward: [-0.83798382]
Agent gate_2 episode reward: [-0.83415556]
All agents episode reward: [-0.83415556]
Agent gate_2 episode reward: [-0.82482912]
All agents episode reward: [-0.82482912]
Agent gate_2 episode reward: [-0.83500133]
All agents episode reward: [-0.83500133]
Agent gate_2 episode reward: [-0.85145533]
All agents episode reward: [-0.85145533]
Agent gate_2 episode reward: [-0.82131657]
All agents episode reward: [-0.82131657]
Agent gate_2 episode reward: [-0.8623891]
All agents episode reward: [-0.8623891]
Agent gate_2 episode reward: [-0.87265701]
All agents episode reward: [-0.87265701]
Agent gate_2 episode reward: [-0.93854049]
All agents episode reward: [-0.93854049]
Agent gate_2 episode reward: [-0.81455819]
All agents episode reward: [-0.81455819]
Agent gate_2 episode reward: [-0.8304524]
All agents episode reward: [-0.8304524]
Agent gate_2 episode reward: [-0.87170643]
All agents episode reward: [-0.87170643]
Agent gate_2 episode reward: [-0.86009204]
All agents episode reward: [-0.86009204]
Agent gate_2 episode reward: [-0.8891872]
All agents episode reward: [-0.8891872]
Agent gate_2 episode reward: [-0.86363259]
All agents episode reward: [-0.86363259]
Agent gate_2 episode reward: [-0.935752]
All agents episode reward: [-0.935752]
Agent gate_2 episode reward: [-0.8936852]
All agents episode reward: [-0.8936852]
Iteration 9:  80%|████████  | 16/20 [01:09<00:16,  4.23s/it, episode=190, norm_ret=-0.904, true_ret=-289585632.000, steps=600]
Agent gate_2 episode reward: [-0.87733773]
All agents episode reward: [-0.87733773]
Agent gate_2 episode reward: [-0.82928366]
All agents episode reward: [-0.82928366]
Agent gate_2 episode reward: [-0.90876466]
All agents episode reward: [-0.90876466]
Agent gate_2 episode reward: [-0.88645541]
All agents episode reward: [-0.88645541]
Agent gate_2 episode reward: [-0.93504475]
All agents episode reward: [-0.93504475]
Agent gate_2 episode reward: [-0.95993227]
All agents episode reward: [-0.95993227]
Agent gate_2 episode reward: [-0.94820983]
All agents episode reward: [-0.94820983]
Agent gate_2 episode reward: [-0.92526483]
All agents episode reward: [-0.92526483]
Agent gate_2 episode reward: [-0.87775788]
All agents episode reward: [-0.87775788]
Agent gate_2 episode reward: [-0.8907288]
All agents episode reward: [-0.8907288]
Agent gate_2 episode reward: [-0.89691228]
All agents episode reward: [-0.89691228]
Agent gate_2 episode reward: [-0.94542248]
All agents episode reward: [-0.94542248]
Agent gate_2 episode reward: [-0.86470701]
All agents episode reward: [-0.86470701]
Agent gate_2 episode reward: [-0.9120545]
All agents episode reward: [-0.9120545]
Agent gate_2 episode reward: [-0.94323005]
All agents episode reward: [-0.94323005]
Agent gate_2 episode reward: [-0.86760075]
All agents episode reward: [-0.86760075]
Agent gate_2 episode reward: [-0.94336585]
All agents episode reward: [-0.94336585]
Agent gate_2 episode reward: [-0.99682137]
All agents episode reward: [-0.99682137]
Agent gate_2 episode reward: [-0.89598961]
All agents episode reward: [-0.89598961]
Saved 1 agents to sac_agents_butterfly_scC
[Validation] New best avg return: -280460800.000 at episode 200 (over 3 val episodes, saved to sac_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.93737782]
All agents episode reward: [-0.93737782]
Loaded 1 agents from sac_agents_butterfly_scC
Running 5 evaluation runs...
  Run 1/5... Avg agent reward (episode): -293524576.000 | Total reward: -293524576.000
Saved run 1 to rl_training/butterfly_scC/sac_run1
  Run 2/5... Avg agent reward (episode): -447407616.000 | Total reward: -447407616.000
Saved run 2 to rl_training/butterfly_scC/sac_run2
  Run 3/5... Avg agent reward (episode): -2366724.750 | Total reward: -2366724.750
Saved run 3 to rl_training/butterfly_scC/sac_run3
  Run 4/5... Avg agent reward (episode): -472131552.000 | Total reward: -472131552.000
Saved run 4 to rl_training/butterfly_scC/sac_run4
  Run 5/5... Avg agent reward (episode): -407322400.000 | Total reward: -407322400.000
Saved run 5 to rl_training/butterfly_scC/sac_run5
============================================================
Evaluation Results
  Number of runs: 5
============================================================
  Agent gate_2: -324550592.000 ± 172331200.000
  Average reward: -324550592.000 ± 172331200.000
  Total reward: -324550592.000 ± 172331200.000
============================================================
Running 5 evaluation runs...
  Run 1/5... Avg agent reward (episode): -293524576.000 | Total reward: -293524576.000
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/5... Avg agent reward (episode): -447407616.000 | Total reward: -447407616.000
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/5... Avg agent reward (episode): -2366724.750 | Total reward: -2366724.750
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/5... Avg agent reward (episode): -472131552.000 | Total reward: -472131552.000
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/5... Avg agent reward (episode): -407322400.000 | Total reward: -407322400.000
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
============================================================
Evaluation Results
  Number of runs: 5
============================================================
  Agent gate_2: -324550592.000 ± 172331200.000
  Average reward: -324550592.000 ± 172331200.000
  Total reward: -324550592.000 ± 172331200.000
============================================================
Running 5 evaluation runs...
  Run 1/5... No actions provided, skipping action application.
Avg agent reward (episode): -293524576.000 | Total reward: -293524576.000
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/5... No actions provided, skipping action application.
Avg agent reward (episode): -447407616.000 | Total reward: -447407616.000
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/5... No actions provided, skipping action application.
Avg agent reward (episode): -2366724.750 | Total reward: -2366724.750
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/5... No actions provided, skipping action application.
Avg agent reward (episode): -472131552.000 | Total reward: -472131552.000
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/5... No actions provided, skipping action application.
Avg agent reward (episode): -407322400.000 | Total reward: -407322400.000
Saved run 5 to rl_training/butterfly_scC/no_control_run5
============================================================
Evaluation Results
  Number of runs: 5
============================================================
  Agent gate_2: -324550592.000 ± 172331200.000
  Average reward: -324550592.000 ± 172331200.000
  Total reward: -324550592.000 ± 172331200.000
============================================================

============================================================
Comparison of All Methods
============================================================
sac avg reward:        -324550592.000
Rule-based avg reward: -324550592.000
No control avg reward: -324550592.000
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
