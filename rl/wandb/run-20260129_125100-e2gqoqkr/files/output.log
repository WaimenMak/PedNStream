Iteration 0: 100%|██████████| 10/10 [00:22<00:00,  2.28s/it, episode=10, norm_ret=-11.131, true_ret=-310492576.000, steps=600]
Agent gate_2 episode reward: [-78.09002062]
All agents episode reward: [-78.09002062]
Agent gate_2 episode reward: [-12.71663039]
All agents episode reward: [-12.71663039]
Agent gate_2 episode reward: [-15.17513293]
All agents episode reward: [-15.17513293]
Agent gate_2 episode reward: [-3.08127101]
All agents episode reward: [-3.08127101]
Agent gate_2 episode reward: [-1.12948705]
All agents episode reward: [-1.12948705]
Agent gate_2 episode reward: [-0.57528195]
All agents episode reward: [-0.57528195]
Agent gate_2 episode reward: [-0.13246343]
All agents episode reward: [-0.13246343]
Agent gate_2 episode reward: [-0.14445047]
All agents episode reward: [-0.14445047]
Agent gate_2 episode reward: [-0.12580071]
All agents episode reward: [-0.12580071]
Agent gate_2 episode reward: [-0.1418424]
All agents episode reward: [-0.1418424]
Iteration 1: 100%|██████████| 10/10 [00:22<00:00,  2.23s/it, episode=20, norm_ret=-0.244, true_ret=-315404448.000, steps=600]
Agent gate_2 episode reward: [-0.13487309]
All agents episode reward: [-0.13487309]
Agent gate_2 episode reward: [-0.26955113]
All agents episode reward: [-0.26955113]
Agent gate_2 episode reward: [-0.23837736]
All agents episode reward: [-0.23837736]
Agent gate_2 episode reward: [-0.41984638]
All agents episode reward: [-0.41984638]
Agent gate_2 episode reward: [-0.46125618]
All agents episode reward: [-0.46125618]
Agent gate_2 episode reward: [-0.22616614]
All agents episode reward: [-0.22616614]
Agent gate_2 episode reward: [-0.16860103]
All agents episode reward: [-0.16860103]
Agent gate_2 episode reward: [-0.17358843]
All agents episode reward: [-0.17358843]
Agent gate_2 episode reward: [-0.15819114]
All agents episode reward: [-0.15819114]
Agent gate_2 episode reward: [-0.18803456]
All agents episode reward: [-0.18803456]
Iteration 2: 100%|██████████| 10/10 [00:23<00:00,  2.33s/it, episode=30, norm_ret=-0.217, true_ret=-320900064.000, steps=600]
Agent gate_2 episode reward: [-0.34497418]
All agents episode reward: [-0.34497418]
Agent gate_2 episode reward: [-0.21642424]
All agents episode reward: [-0.21642424]
Agent gate_2 episode reward: [-0.1873741]
All agents episode reward: [-0.1873741]
Agent gate_2 episode reward: [-0.18490794]
All agents episode reward: [-0.18490794]
Agent gate_2 episode reward: [-0.21033114]
All agents episode reward: [-0.21033114]
Agent gate_2 episode reward: [-0.19040634]
All agents episode reward: [-0.19040634]
Agent gate_2 episode reward: [-0.20549162]
All agents episode reward: [-0.20549162]
Agent gate_2 episode reward: [-0.21784967]
All agents episode reward: [-0.21784967]
Agent gate_2 episode reward: [-0.1878516]
All agents episode reward: [-0.1878516]
Agent gate_2 episode reward: [-0.22839915]
All agents episode reward: [-0.22839915]
Iteration 3: 100%|██████████| 10/10 [00:23<00:00,  2.37s/it, episode=40, norm_ret=-0.271, true_ret=-276704352.000, steps=600]
Agent gate_2 episode reward: [-0.21315002]
All agents episode reward: [-0.21315002]
Agent gate_2 episode reward: [-0.2432164]
All agents episode reward: [-0.2432164]
Agent gate_2 episode reward: [-0.21128175]
All agents episode reward: [-0.21128175]
Agent gate_2 episode reward: [-0.22312617]
All agents episode reward: [-0.22312617]
Agent gate_2 episode reward: [-0.24944754]
All agents episode reward: [-0.24944754]
Agent gate_2 episode reward: [-0.26743887]
All agents episode reward: [-0.26743887]
Agent gate_2 episode reward: [-0.33763975]
All agents episode reward: [-0.33763975]
Agent gate_2 episode reward: [-0.44423024]
All agents episode reward: [-0.44423024]
Agent gate_2 episode reward: [-0.29807223]
All agents episode reward: [-0.29807223]
Agent gate_2 episode reward: [-0.22494629]
All agents episode reward: [-0.22494629]
Iteration 4: 100%|██████████| 10/10 [00:23<00:00,  2.32s/it, episode=50, norm_ret=-0.355, true_ret=-412309920.000, steps=600]
Agent gate_2 episode reward: [-0.23833115]
All agents episode reward: [-0.23833115]
Agent gate_2 episode reward: [-0.22427003]
All agents episode reward: [-0.22427003]
Agent gate_2 episode reward: [-0.24806479]
All agents episode reward: [-0.24806479]
Agent gate_2 episode reward: [-0.26399782]
All agents episode reward: [-0.26399782]
Agent gate_2 episode reward: [-0.27012418]
All agents episode reward: [-0.27012418]
Agent gate_2 episode reward: [-0.27681317]
All agents episode reward: [-0.27681317]
Agent gate_2 episode reward: [-0.24114007]
All agents episode reward: [-0.24114007]
Agent gate_2 episode reward: [-0.25851442]
All agents episode reward: [-0.25851442]
Agent gate_2 episode reward: [-1.1574163]
All agents episode reward: [-1.1574163]
Agent gate_2 episode reward: [-0.37192076]
All agents episode reward: [-0.37192076]
Iteration 5: 100%|██████████| 10/10 [00:31<00:00,  3.20s/it, episode=60, norm_ret=-0.260, true_ret=-329137248.000, steps=600]
Agent gate_2 episode reward: [-0.24336204]
All agents episode reward: [-0.24336204]
Agent gate_2 episode reward: [-0.27322304]
All agents episode reward: [-0.27322304]
Agent gate_2 episode reward: [-0.28919652]
All agents episode reward: [-0.28919652]
Agent gate_2 episode reward: [-0.27327243]
All agents episode reward: [-0.27327243]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -318827072.000 at episode 55 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.30289996]
All agents episode reward: [-0.30289996]
Agent gate_2 episode reward: [-0.18706533]
All agents episode reward: [-0.18706533]
Agent gate_2 episode reward: [-0.22672304]
All agents episode reward: [-0.22672304]
Agent gate_2 episode reward: [-0.23529468]
All agents episode reward: [-0.23529468]
Agent gate_2 episode reward: [-0.23136546]
All agents episode reward: [-0.23136546]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -210818416.000 at episode 60 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.33641537]
All agents episode reward: [-0.33641537]
Iteration 6: 100%|██████████| 10/10 [00:30<00:00,  3.06s/it, episode=70, norm_ret=-0.123, true_ret=-204038432.000, steps=600]
Agent gate_2 episode reward: [-0.00095611]
All agents episode reward: [-0.00095611]
Agent gate_2 episode reward: [-0.00035103]
All agents episode reward: [-0.00035103]
Agent gate_2 episode reward: [0.]
All agents episode reward: [0.]
Agent gate_2 episode reward: [-0.00036223]
All agents episode reward: [-0.00036223]
Agent gate_2 episode reward: [-4.36480567e-06]
All agents episode reward: [-4.36480567e-06]
Agent gate_2 episode reward: [-0.26309339]
All agents episode reward: [-0.26309339]
Agent gate_2 episode reward: [-0.23483353]
All agents episode reward: [-0.23483353]
Agent gate_2 episode reward: [-0.22086326]
All agents episode reward: [-0.22086326]
Agent gate_2 episode reward: [-0.27160691]
All agents episode reward: [-0.27160691]
Agent gate_2 episode reward: [-0.23722672]
All agents episode reward: [-0.23722672]
Iteration 7: 100%|██████████| 10/10 [00:30<00:00,  3.07s/it, episode=80, norm_ret=-0.276, true_ret=-183566832.000, steps=600]
Agent gate_2 episode reward: [-0.33595427]
All agents episode reward: [-0.33595427]
Agent gate_2 episode reward: [-0.32247518]
All agents episode reward: [-0.32247518]
Agent gate_2 episode reward: [-0.30555793]
All agents episode reward: [-0.30555793]
Agent gate_2 episode reward: [-0.3211331]
All agents episode reward: [-0.3211331]
Agent gate_2 episode reward: [-0.33099744]
All agents episode reward: [-0.33099744]
Agent gate_2 episode reward: [-0.22655458]
All agents episode reward: [-0.22655458]
Agent gate_2 episode reward: [-0.24241138]
All agents episode reward: [-0.24241138]
Agent gate_2 episode reward: [-0.21995346]
All agents episode reward: [-0.21995346]
Agent gate_2 episode reward: [-0.21654228]
All agents episode reward: [-0.21654228]
Agent gate_2 episode reward: [-0.23633117]
All agents episode reward: [-0.23633117]
Iteration 8: 100%|██████████| 10/10 [00:30<00:00,  3.01s/it, episode=90, norm_ret=-0.534, true_ret=-451277216.000, steps=600]
Agent gate_2 episode reward: [-0.42203574]
All agents episode reward: [-0.42203574]
Agent gate_2 episode reward: [-0.41365549]
All agents episode reward: [-0.41365549]
Agent gate_2 episode reward: [-0.412695]
All agents episode reward: [-0.412695]
Agent gate_2 episode reward: [-0.45434434]
All agents episode reward: [-0.45434434]
Agent gate_2 episode reward: [-0.43239846]
All agents episode reward: [-0.43239846]
Agent gate_2 episode reward: [-0.62444237]
All agents episode reward: [-0.62444237]
Agent gate_2 episode reward: [-0.6412155]
All agents episode reward: [-0.6412155]
Agent gate_2 episode reward: [-0.6457149]
All agents episode reward: [-0.6457149]
Agent gate_2 episode reward: [-0.65909764]
All agents episode reward: [-0.65909764]
Agent gate_2 episode reward: [-0.6326375]
All agents episode reward: [-0.6326375]
Iteration 9: 100%|██████████| 10/10 [00:30<00:00,  3.02s/it, episode=100, norm_ret=-0.598, true_ret=-504782944.000, steps=600]
Agent gate_2 episode reward: [-0.43105853]
All agents episode reward: [-0.43105853]
Agent gate_2 episode reward: [-0.52277881]
All agents episode reward: [-0.52277881]
Agent gate_2 episode reward: [-0.53322277]
All agents episode reward: [-0.53322277]
Agent gate_2 episode reward: [-0.51675688]
All agents episode reward: [-0.51675688]
Agent gate_2 episode reward: [-0.49535538]
All agents episode reward: [-0.49535538]
Agent gate_2 episode reward: [-0.66553582]
All agents episode reward: [-0.66553582]
Agent gate_2 episode reward: [-0.66492958]
All agents episode reward: [-0.66492958]
Agent gate_2 episode reward: [-0.68894782]
All agents episode reward: [-0.68894782]
Agent gate_2 episode reward: [-0.6978249]
All agents episode reward: [-0.6978249]
Agent gate_2 episode reward: [-0.76105717]
All agents episode reward: [-0.76105717]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -276599616.000 | Total reward: -276599616.000
Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Avg agent reward (episode): -455057696.000 | Total reward: -455057696.000
Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Avg agent reward (episode): -2366724.750 | Total reward: -2366724.750
Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Avg agent reward (episode): -470661344.000 | Total reward: -470661344.000
Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Avg agent reward (episode): -415107520.000 | Total reward: -415107520.000
Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Avg agent reward (episode): -77408008.000 | Total reward: -77408008.000
Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Avg agent reward (episode): -201851872.000 | Total reward: -201851872.000
Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Avg agent reward (episode): -77273648.000 | Total reward: -77273648.000
Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Avg agent reward (episode): -250919888.000 | Total reward: -250919888.000
Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Avg agent reward (episode): -350967456.000 | Total reward: -350967456.000
Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -257821392.000 ± 158604704.000
  Average reward: -257821392.000 ± 158604704.000
  Total reward: -257821392.000 ± 158604704.000
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -293524576.000 | Total reward: -293524576.000
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Avg agent reward (episode): -447407616.000 | Total reward: -447407616.000
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Avg agent reward (episode): -2366724.750 | Total reward: -2366724.750
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Avg agent reward (episode): -472131552.000 | Total reward: -472131552.000
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Avg agent reward (episode): -407322400.000 | Total reward: -407322400.000
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Avg agent reward (episode): -77286696.000 | Total reward: -77286696.000
Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Avg agent reward (episode): -204496800.000 | Total reward: -204496800.000
Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Avg agent reward (episode): -66623696.000 | Total reward: -66623696.000
Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Avg agent reward (episode): -251488432.000 | Total reward: -251488432.000
Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Avg agent reward (episode): -355641824.000 | Total reward: -355641824.000
Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -257829040.000 ± 158857136.000
  Average reward: -257829040.000 ± 158857136.000
  Total reward: -257829040.000 ± 158857136.000
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -293524576.000 | Total reward: -293524576.000
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -447407616.000 | Total reward: -447407616.000
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -2366724.750 | Total reward: -2366724.750
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -472131552.000 | Total reward: -472131552.000
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -407322400.000 | Total reward: -407322400.000
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -77286696.000 | Total reward: -77286696.000
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -204496800.000 | Total reward: -204496800.000
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -66623696.000 | Total reward: -66623696.000
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -251488432.000 | Total reward: -251488432.000
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -355641824.000 | Total reward: -355641824.000
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -257829040.000 ± 158857136.000
  Average reward: -257829040.000 ± 158857136.000
  Total reward: -257829040.000 ± 158857136.000
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -257821392.000
Rule-based avg reward: -257829040.000
No control avg reward: -257829040.000
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
