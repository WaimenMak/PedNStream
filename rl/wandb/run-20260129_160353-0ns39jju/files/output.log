Iteration 0: 100%|██████████| 10/10 [00:21<00:00,  2.19s/it, episode=10, norm_ret=-20.608, true_ret=-111.055, steps=600]
Agent gate_2 episode reward: [-41.18109896]
All agents episode reward: [-41.18109896]
Agent gate_2 episode reward: [-12.25421674]
All agents episode reward: [-12.25421674]
Agent gate_2 episode reward: [-21.57391921]
All agents episode reward: [-21.57391921]
Agent gate_2 episode reward: [-14.6935769]
All agents episode reward: [-14.6935769]
Agent gate_2 episode reward: [-20.47423828]
All agents episode reward: [-20.47423828]
Agent gate_2 episode reward: [-21.57814008]
All agents episode reward: [-21.57814008]
Agent gate_2 episode reward: [-22.1117403]
All agents episode reward: [-22.1117403]
Agent gate_2 episode reward: [-19.0088073]
All agents episode reward: [-19.0088073]
Agent gate_2 episode reward: [-18.53229323]
All agents episode reward: [-18.53229323]
Agent gate_2 episode reward: [-14.67336921]
All agents episode reward: [-14.67336921]
Iteration 1: 100%|██████████| 10/10 [00:21<00:00,  2.18s/it, episode=20, norm_ret=-14.082, true_ret=-99.830, steps=600]
Agent gate_2 episode reward: [-13.97812783]
All agents episode reward: [-13.97812783]
Agent gate_2 episode reward: [-11.09956314]
All agents episode reward: [-11.09956314]
Agent gate_2 episode reward: [-13.85066824]
All agents episode reward: [-13.85066824]
Agent gate_2 episode reward: [-14.10110425]
All agents episode reward: [-14.10110425]
Agent gate_2 episode reward: [-8.0568939]
All agents episode reward: [-8.0568939]
Agent gate_2 episode reward: [-13.19679221]
All agents episode reward: [-13.19679221]
Agent gate_2 episode reward: [-19.80329174]
All agents episode reward: [-19.80329174]
Agent gate_2 episode reward: [-18.45137662]
All agents episode reward: [-18.45137662]
Agent gate_2 episode reward: [-14.3541038]
All agents episode reward: [-14.3541038]
Agent gate_2 episode reward: [-13.92330976]
All agents episode reward: [-13.92330976]
Iteration 2: 100%|██████████| 10/10 [00:21<00:00,  2.13s/it, episode=30, norm_ret=-15.955, true_ret=-101.723, steps=600]
Agent gate_2 episode reward: [-15.7841713]
All agents episode reward: [-15.7841713]
Agent gate_2 episode reward: [-14.234777]
All agents episode reward: [-14.234777]
Agent gate_2 episode reward: [-16.18845409]
All agents episode reward: [-16.18845409]
Agent gate_2 episode reward: [-15.95037476]
All agents episode reward: [-15.95037476]
Agent gate_2 episode reward: [-16.20941007]
All agents episode reward: [-16.20941007]
Agent gate_2 episode reward: [-15.8057989]
All agents episode reward: [-15.8057989]
Agent gate_2 episode reward: [-16.25918527]
All agents episode reward: [-16.25918527]
Agent gate_2 episode reward: [-16.69628003]
All agents episode reward: [-16.69628003]
Agent gate_2 episode reward: [-17.14816233]
All agents episode reward: [-17.14816233]
Agent gate_2 episode reward: [-15.27200188]
All agents episode reward: [-15.27200188]
Iteration 3: 100%|██████████| 10/10 [00:22<00:00,  2.23s/it, episode=40, norm_ret=-16.895, true_ret=-107.087, steps=600]
Agent gate_2 episode reward: [-15.80431591]
All agents episode reward: [-15.80431591]
Agent gate_2 episode reward: [-16.74299105]
All agents episode reward: [-16.74299105]
Agent gate_2 episode reward: [-16.71936617]
All agents episode reward: [-16.71936617]
Agent gate_2 episode reward: [-16.78523139]
All agents episode reward: [-16.78523139]
Agent gate_2 episode reward: [-18.36210942]
All agents episode reward: [-18.36210942]
Agent gate_2 episode reward: [-17.51773153]
All agents episode reward: [-17.51773153]
Agent gate_2 episode reward: [-11.47107344]
All agents episode reward: [-11.47107344]
Agent gate_2 episode reward: [-20.48386785]
All agents episode reward: [-20.48386785]
Agent gate_2 episode reward: [-18.66208776]
All agents episode reward: [-18.66208776]
Agent gate_2 episode reward: [-16.40455553]
All agents episode reward: [-16.40455553]
Iteration 4: 100%|██████████| 10/10 [00:21<00:00,  2.13s/it, episode=50, norm_ret=-19.411, true_ret=-133.238, steps=600]
Agent gate_2 episode reward: [-17.9159875]
All agents episode reward: [-17.9159875]
Agent gate_2 episode reward: [-17.99417728]
All agents episode reward: [-17.99417728]
Agent gate_2 episode reward: [-18.19902332]
All agents episode reward: [-18.19902332]
Agent gate_2 episode reward: [-21.53751773]
All agents episode reward: [-21.53751773]
Agent gate_2 episode reward: [-18.87281362]
All agents episode reward: [-18.87281362]
Agent gate_2 episode reward: [-15.36441234]
All agents episode reward: [-15.36441234]
Agent gate_2 episode reward: [-21.81381819]
All agents episode reward: [-21.81381819]
Agent gate_2 episode reward: [-21.27582794]
All agents episode reward: [-21.27582794]
Agent gate_2 episode reward: [-21.84110511]
All agents episode reward: [-21.84110511]
Agent gate_2 episode reward: [-19.2983162]
All agents episode reward: [-19.2983162]
Iteration 5: 100%|██████████| 10/10 [00:28<00:00,  2.86s/it, episode=60, norm_ret=-18.836, true_ret=-151.209, steps=600]
Agent gate_2 episode reward: [-19.77186016]
All agents episode reward: [-19.77186016]
Agent gate_2 episode reward: [-21.35286502]
All agents episode reward: [-21.35286502]
Agent gate_2 episode reward: [-21.50709183]
All agents episode reward: [-21.50709183]
Agent gate_2 episode reward: [-19.32773328]
All agents episode reward: [-19.32773328]
Saved 1 agents to ppo_agents_butterfly_scA
[Validation] New best avg return: -132.295 at episode 55 (over 5 val episodes, saved to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-21.97736295]
All agents episode reward: [-21.97736295]
Agent gate_2 episode reward: [-18.75907698]
All agents episode reward: [-18.75907698]
Agent gate_2 episode reward: [-19.64068531]
All agents episode reward: [-19.64068531]
Agent gate_2 episode reward: [-6.11619838]
All agents episode reward: [-6.11619838]
Agent gate_2 episode reward: [-20.02661128]
All agents episode reward: [-20.02661128]
Saved 1 agents to ppo_agents_butterfly_scA
[Validation] New best avg return: -125.739 at episode 60 (over 5 val episodes, saved to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-19.87977367]
All agents episode reward: [-19.87977367]
Iteration 6: 100%|██████████| 10/10 [00:29<00:00,  2.94s/it, episode=70, norm_ret=-15.174, true_ret=-109.827, steps=600]
Agent gate_2 episode reward: [-16.74012664]
All agents episode reward: [-16.74012664]
Agent gate_2 episode reward: [-20.01571781]
All agents episode reward: [-20.01571781]
Agent gate_2 episode reward: [-17.04479549]
All agents episode reward: [-17.04479549]
Agent gate_2 episode reward: [-14.94465991]
All agents episode reward: [-14.94465991]
Agent gate_2 episode reward: [-4.70531945]
All agents episode reward: [-4.70531945]
Agent gate_2 episode reward: [-18.36805612]
All agents episode reward: [-18.36805612]
Agent gate_2 episode reward: [-17.58288229]
All agents episode reward: [-17.58288229]
Agent gate_2 episode reward: [-13.84186405]
All agents episode reward: [-13.84186405]
Agent gate_2 episode reward: [-14.88584963]
All agents episode reward: [-14.88584963]
Agent gate_2 episode reward: [-13.60660191]
All agents episode reward: [-13.60660191]
Iteration 7: 100%|██████████| 10/10 [00:29<00:00,  2.90s/it, episode=80, norm_ret=-17.657, true_ret=-131.761, steps=600]
Agent gate_2 episode reward: [-15.45341401]
All agents episode reward: [-15.45341401]
Agent gate_2 episode reward: [-17.35019423]
All agents episode reward: [-17.35019423]
Agent gate_2 episode reward: [-18.78646661]
All agents episode reward: [-18.78646661]
Agent gate_2 episode reward: [-17.70190869]
All agents episode reward: [-17.70190869]
Agent gate_2 episode reward: [-18.70209379]
All agents episode reward: [-18.70209379]
Agent gate_2 episode reward: [-18.81958545]
All agents episode reward: [-18.81958545]
Agent gate_2 episode reward: [-18.90495995]
All agents episode reward: [-18.90495995]
Agent gate_2 episode reward: [-17.19080415]
All agents episode reward: [-17.19080415]
Agent gate_2 episode reward: [-17.80545861]
All agents episode reward: [-17.80545861]
Agent gate_2 episode reward: [-15.85059048]
All agents episode reward: [-15.85059048]
Iteration 8: 100%|██████████| 10/10 [00:28<00:00,  2.88s/it, episode=90, norm_ret=-16.626, true_ret=-137.216, steps=600]
Agent gate_2 episode reward: [-17.49990897]
All agents episode reward: [-17.49990897]
Agent gate_2 episode reward: [-17.18926807]
All agents episode reward: [-17.18926807]
Agent gate_2 episode reward: [-16.87945558]
All agents episode reward: [-16.87945558]
Agent gate_2 episode reward: [-13.26418523]
All agents episode reward: [-13.26418523]
Saved 1 agents to ppo_agents_butterfly_scA
[Validation] New best avg return: -119.063 at episode 85 (over 5 val episodes, saved to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-18.32732749]
All agents episode reward: [-18.32732749]
Agent gate_2 episode reward: [-17.39884549]
All agents episode reward: [-17.39884549]
Agent gate_2 episode reward: [-17.65495127]
All agents episode reward: [-17.65495127]
Agent gate_2 episode reward: [-14.13938175]
All agents episode reward: [-14.13938175]
Agent gate_2 episode reward: [-17.73798734]
All agents episode reward: [-17.73798734]
Saved 1 agents to ppo_agents_butterfly_scA
[Validation] New best avg return: -118.931 at episode 90 (over 5 val episodes, saved to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-16.17157242]
All agents episode reward: [-16.17157242]
Iteration 9: 100%|██████████| 10/10 [00:28<00:00,  2.86s/it, episode=100, norm_ret=-15.834, true_ret=-131.596, steps=600]
Agent gate_2 episode reward: [-15.12676722]
All agents episode reward: [-15.12676722]
Agent gate_2 episode reward: [-13.27358532]
All agents episode reward: [-13.27358532]
Agent gate_2 episode reward: [-18.17090187]
All agents episode reward: [-18.17090187]
Agent gate_2 episode reward: [-16.92558085]
All agents episode reward: [-16.92558085]
Agent gate_2 episode reward: [-16.88827486]
All agents episode reward: [-16.88827486]
Agent gate_2 episode reward: [-12.58284954]
All agents episode reward: [-12.58284954]
Agent gate_2 episode reward: [-17.7609396]
All agents episode reward: [-17.7609396]
Agent gate_2 episode reward: [-14.58379419]
All agents episode reward: [-14.58379419]
Agent gate_2 episode reward: [-17.69646692]
All agents episode reward: [-17.69646692]
Agent gate_2 episode reward: [-15.32974235]
All agents episode reward: [-15.32974235]
Loaded 1 agents from ppo_agents_butterfly_scA
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -40.780 | Total reward: -40.780
Saved run 1 to rl_training/butterfly_scA/ppo_run1
  Run 2/10... Avg agent reward (episode): -124.147 | Total reward: -124.147
Saved run 2 to rl_training/butterfly_scA/ppo_run2
  Run 3/10... Avg agent reward (episode): -149.819 | Total reward: -149.819
Saved run 3 to rl_training/butterfly_scA/ppo_run3
  Run 4/10... Avg agent reward (episode): -136.588 | Total reward: -136.588
Saved run 4 to rl_training/butterfly_scA/ppo_run4
  Run 5/10... Avg agent reward (episode): -157.496 | Total reward: -157.496
Saved run 5 to rl_training/butterfly_scA/ppo_run5
  Run 6/10... Avg agent reward (episode): -147.575 | Total reward: -147.575
Saved run 6 to rl_training/butterfly_scA/ppo_run6
  Run 7/10... Avg agent reward (episode): -149.647 | Total reward: -149.647
Saved run 7 to rl_training/butterfly_scA/ppo_run7
  Run 8/10... Avg agent reward (episode): -137.389 | Total reward: -137.389
Saved run 8 to rl_training/butterfly_scA/ppo_run8
  Run 9/10... Avg agent reward (episode): -147.099 | Total reward: -147.099
Saved run 9 to rl_training/butterfly_scA/ppo_run9
  Run 10/10... Avg agent reward (episode): -155.972 | Total reward: -155.972
Saved run 10 to rl_training/butterfly_scA/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -134.651 ± 32.691
  Average reward: -134.651 ± 32.691
  Total reward: -134.651 ± 32.691
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -40.780 | Total reward: -40.780
Saved run 1 to rl_training/butterfly_scA/rule_based_run1
  Run 2/10... Avg agent reward (episode): -124.147 | Total reward: -124.147
Saved run 2 to rl_training/butterfly_scA/rule_based_run2
  Run 3/10... Avg agent reward (episode): -149.819 | Total reward: -149.819
Saved run 3 to rl_training/butterfly_scA/rule_based_run3
  Run 4/10... Avg agent reward (episode): -136.588 | Total reward: -136.588
Saved run 4 to rl_training/butterfly_scA/rule_based_run4
  Run 5/10... Avg agent reward (episode): -157.496 | Total reward: -157.496
Saved run 5 to rl_training/butterfly_scA/rule_based_run5
  Run 6/10... Avg agent reward (episode): -147.575 | Total reward: -147.575
Saved run 6 to rl_training/butterfly_scA/rule_based_run6
  Run 7/10... Avg agent reward (episode): -149.647 | Total reward: -149.647
Saved run 7 to rl_training/butterfly_scA/rule_based_run7
  Run 8/10... Avg agent reward (episode): -137.389 | Total reward: -137.389
Saved run 8 to rl_training/butterfly_scA/rule_based_run8
  Run 9/10... Avg agent reward (episode): -147.099 | Total reward: -147.099
Saved run 9 to rl_training/butterfly_scA/rule_based_run9
  Run 10/10... Avg agent reward (episode): -155.972 | Total reward: -155.972
Saved run 10 to rl_training/butterfly_scA/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -134.651 ± 32.691
  Average reward: -134.651 ± 32.691
  Total reward: -134.651 ± 32.691
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -40.780 | Total reward: -40.780
Saved run 1 to rl_training/butterfly_scA/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -124.147 | Total reward: -124.147
Saved run 2 to rl_training/butterfly_scA/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -149.819 | Total reward: -149.819
Saved run 3 to rl_training/butterfly_scA/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -136.588 | Total reward: -136.588
Saved run 4 to rl_training/butterfly_scA/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -157.496 | Total reward: -157.496
Saved run 5 to rl_training/butterfly_scA/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -147.575 | Total reward: -147.575
Saved run 6 to rl_training/butterfly_scA/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -149.647 | Total reward: -149.647
Saved run 7 to rl_training/butterfly_scA/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -137.389 | Total reward: -137.389
Saved run 8 to rl_training/butterfly_scA/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -147.099 | Total reward: -147.099
Saved run 9 to rl_training/butterfly_scA/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -155.972 | Total reward: -155.972
Saved run 10 to rl_training/butterfly_scA/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -134.651 ± 32.691
  Average reward: -134.651 ± 32.691
  Total reward: -134.651 ± 32.691
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -134.651
Rule-based avg reward: -134.651
No control avg reward: -134.651
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
