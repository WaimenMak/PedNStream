Iteration 0: 100%|██████████| 10/10 [00:21<00:00,  2.16s/it, episode=10, norm_ret=-9.861, true_ret=-198451488.000, steps=600]
Agent gate_2 episode reward: [-70.48415466]
All agents episode reward: [-70.48415466]
Agent gate_2 episode reward: [-6.59693317]
All agents episode reward: [-6.59693317]
Agent gate_2 episode reward: [-0.1114071]
All agents episode reward: [-0.1114071]
Agent gate_2 episode reward: [-0.34071367]
All agents episode reward: [-0.34071367]
Agent gate_2 episode reward: [-0.1717283]
All agents episode reward: [-0.1717283]
Agent gate_2 episode reward: [-0.16882536]
All agents episode reward: [-0.16882536]
Agent gate_2 episode reward: [-19.557023]
All agents episode reward: [-19.557023]
Agent gate_2 episode reward: [-0.05634321]
All agents episode reward: [-0.05634321]
Agent gate_2 episode reward: [-0.9960375]
All agents episode reward: [-0.9960375]
Agent gate_2 episode reward: [-0.1312688]
All agents episode reward: [-0.1312688]
Iteration 1: 100%|██████████| 10/10 [00:21<00:00,  2.11s/it, episode=20, norm_ret=-4.131, true_ret=-9340950528.000, steps=600]
Agent gate_2 episode reward: [-0.13876352]
All agents episode reward: [-0.13876352]
Agent gate_2 episode reward: [-0.14119396]
All agents episode reward: [-0.14119396]
Agent gate_2 episode reward: [-0.08325963]
All agents episode reward: [-0.08325963]
Agent gate_2 episode reward: [-11.44264663]
All agents episode reward: [-11.44264663]
Agent gate_2 episode reward: [-0.13314389]
All agents episode reward: [-0.13314389]
Agent gate_2 episode reward: [-8.66018128]
All agents episode reward: [-8.66018128]
Agent gate_2 episode reward: [-0.20359317]
All agents episode reward: [-0.20359317]
Agent gate_2 episode reward: [-0.14188118]
All agents episode reward: [-0.14188118]
Agent gate_2 episode reward: [-13.63888223]
All agents episode reward: [-13.63888223]
Agent gate_2 episode reward: [-6.72221938]
All agents episode reward: [-6.72221938]
Iteration 2: 100%|██████████| 10/10 [00:21<00:00,  2.11s/it, episode=30, norm_ret=-0.440, true_ret=-146896144.000, steps=600]
Agent gate_2 episode reward: [-0.22131977]
All agents episode reward: [-0.22131977]
Agent gate_2 episode reward: [-0.14447589]
All agents episode reward: [-0.14447589]
Agent gate_2 episode reward: [-0.37314295]
All agents episode reward: [-0.37314295]
Agent gate_2 episode reward: [-0.2557968]
All agents episode reward: [-0.2557968]
Agent gate_2 episode reward: [-0.20287528]
All agents episode reward: [-0.20287528]
Agent gate_2 episode reward: [-0.61135737]
All agents episode reward: [-0.61135737]
Agent gate_2 episode reward: [-2.20894787]
All agents episode reward: [-2.20894787]
Agent gate_2 episode reward: [-0.10143122]
All agents episode reward: [-0.10143122]
Agent gate_2 episode reward: [-0.15445449]
All agents episode reward: [-0.15445449]
Agent gate_2 episode reward: [-0.12416992]
All agents episode reward: [-0.12416992]
Iteration 3: 100%|██████████| 10/10 [00:20<00:00,  2.02s/it, episode=40, norm_ret=-0.520, true_ret=-89388448.000, steps=600]
Agent gate_2 episode reward: [-0.49954242]
All agents episode reward: [-0.49954242]
Agent gate_2 episode reward: [-0.14544719]
All agents episode reward: [-0.14544719]
Agent gate_2 episode reward: [-3.44181644]
All agents episode reward: [-3.44181644]
Agent gate_2 episode reward: [-0.17382312]
All agents episode reward: [-0.17382312]
Agent gate_2 episode reward: [-0.16464214]
All agents episode reward: [-0.16464214]
Agent gate_2 episode reward: [-0.28277402]
All agents episode reward: [-0.28277402]
Agent gate_2 episode reward: [-0.14298955]
All agents episode reward: [-0.14298955]
Agent gate_2 episode reward: [-0.09658565]
All agents episode reward: [-0.09658565]
Agent gate_2 episode reward: [-0.16994671]
All agents episode reward: [-0.16994671]
Agent gate_2 episode reward: [-0.08493652]
All agents episode reward: [-0.08493652]
Iteration 4: 100%|██████████| 10/10 [00:21<00:00,  2.11s/it, episode=50, norm_ret=-3.255, true_ret=-17878607872.000, steps=600]
Agent gate_2 episode reward: [-0.32169947]
All agents episode reward: [-0.32169947]
Agent gate_2 episode reward: [-0.85972615]
All agents episode reward: [-0.85972615]
Agent gate_2 episode reward: [-0.11775845]
All agents episode reward: [-0.11775845]
Agent gate_2 episode reward: [0.]
All agents episode reward: [0.]
Agent gate_2 episode reward: [-0.07983989]
All agents episode reward: [-0.07983989]
Agent gate_2 episode reward: [-3.13000644]
All agents episode reward: [-3.13000644]
Agent gate_2 episode reward: [-9.25308484]
All agents episode reward: [-9.25308484]
Agent gate_2 episode reward: [-0.38210771]
All agents episode reward: [-0.38210771]
Agent gate_2 episode reward: [-0.18134492]
All agents episode reward: [-0.18134492]
Agent gate_2 episode reward: [-18.22007461]
All agents episode reward: [-18.22007461]
Iteration 5: 100%|██████████| 10/10 [00:24<00:00,  2.40s/it, episode=60, norm_ret=-1.124, true_ret=0.000, steps=600]
Agent gate_2 episode reward: [-0.18332201]
All agents episode reward: [-0.18332201]
Agent gate_2 episode reward: [-0.18163704]
All agents episode reward: [-0.18163704]
Agent gate_2 episode reward: [-0.15315902]
All agents episode reward: [-0.15315902]
Agent gate_2 episode reward: [-0.13529386]
All agents episode reward: [-0.13529386]
Saved 1 agents to ppo_agents_butterfly_scA
[Validation] New best avg return: -1381295872.000 at episode 55 (over 5 val episodes, saved to ppo_agents_butterfly_scA)
Agent gate_2 episode reward: [-10.42971036]
All agents episode reward: [-10.42971036]
Agent gate_2 episode reward: [-0.01515982]
All agents episode reward: [-0.01515982]
Agent gate_2 episode reward: [-0.06829291]
All agents episode reward: [-0.06829291]
Agent gate_2 episode reward: [-0.05602256]
All agents episode reward: [-0.05602256]
Agent gate_2 episode reward: [-0.01660843]
All agents episode reward: [-0.01660843]
Agent gate_2 episode reward: [0.]
All agents episode reward: [0.]
Iteration 6: 100%|██████████| 10/10 [00:27<00:00,  2.76s/it, episode=70, norm_ret=-9.189, true_ret=-1176490112.000, steps=600]
Agent gate_2 episode reward: [-0.0103196]
All agents episode reward: [-0.0103196]
Agent gate_2 episode reward: [-0.17362374]
All agents episode reward: [-0.17362374]
Agent gate_2 episode reward: [-0.17029443]
All agents episode reward: [-0.17029443]
Agent gate_2 episode reward: [-91.39443424]
All agents episode reward: [-91.39443424]
Agent gate_2 episode reward: [-0.10449426]
All agents episode reward: [-0.10449426]
Agent gate_2 episode reward: [-0.00240635]
All agents episode reward: [-0.00240635]
Agent gate_2 episode reward: [-0.01558861]
All agents episode reward: [-0.01558861]
Agent gate_2 episode reward: [-0.0026529]
All agents episode reward: [-0.0026529]
Agent gate_2 episode reward: [-0.00245191]
All agents episode reward: [-0.00245191]
Agent gate_2 episode reward: [-0.01586986]
All agents episode reward: [-0.01586986]
Iteration 7: 100%|██████████| 10/10 [00:27<00:00,  2.70s/it, episode=80, norm_ret=-0.057, true_ret=-8721029120.000, steps=600]
Agent gate_2 episode reward: [-0.04491582]
All agents episode reward: [-0.04491582]
Agent gate_2 episode reward: [-0.03237131]
All agents episode reward: [-0.03237131]
Agent gate_2 episode reward: [-0.00400002]
All agents episode reward: [-0.00400002]
Agent gate_2 episode reward: [-0.00276018]
All agents episode reward: [-0.00276018]
Agent gate_2 episode reward: [-0.05256142]
All agents episode reward: [-0.05256142]
Agent gate_2 episode reward: [-0.11886887]
All agents episode reward: [-0.11886887]
Agent gate_2 episode reward: [-0.11690299]
All agents episode reward: [-0.11690299]
Agent gate_2 episode reward: [-0.0027383]
All agents episode reward: [-0.0027383]
Agent gate_2 episode reward: [-0.06002588]
All agents episode reward: [-0.06002588]
Agent gate_2 episode reward: [-0.13076159]
All agents episode reward: [-0.13076159]
Iteration 8: 100%|██████████| 10/10 [00:25<00:00,  2.54s/it, episode=90, norm_ret=-0.212, true_ret=-34572808192.000, steps=600]
Agent gate_2 episode reward: [-0.26542734]
All agents episode reward: [-0.26542734]
Agent gate_2 episode reward: [-0.00309387]
All agents episode reward: [-0.00309387]
Agent gate_2 episode reward: [-0.13764174]
All agents episode reward: [-0.13764174]
Agent gate_2 episode reward: [-0.18454877]
All agents episode reward: [-0.18454877]
Agent gate_2 episode reward: [-0.15881161]
All agents episode reward: [-0.15881161]
Agent gate_2 episode reward: [-0.19912015]
All agents episode reward: [-0.19912015]
Agent gate_2 episode reward: [-0.43562534]
All agents episode reward: [-0.43562534]
Agent gate_2 episode reward: [-0.00629299]
All agents episode reward: [-0.00629299]
Agent gate_2 episode reward: [-0.16223093]
All agents episode reward: [-0.16223093]
Agent gate_2 episode reward: [-0.56552415]
All agents episode reward: [-0.56552415]
Iteration 9: 100%|██████████| 10/10 [00:25<00:00,  2.60s/it, episode=100, norm_ret=-38.048, true_ret=-26518452224.000, steps=600]
Agent gate_2 episode reward: [-0.5090884]
All agents episode reward: [-0.5090884]
Agent gate_2 episode reward: [-0.20400077]
All agents episode reward: [-0.20400077]
Agent gate_2 episode reward: [-379.74238518]
All agents episode reward: [-379.74238518]
Agent gate_2 episode reward: [-0.00410985]
All agents episode reward: [-0.00410985]
Agent gate_2 episode reward: [-0.00204388]
All agents episode reward: [-0.00204388]
Agent gate_2 episode reward: [-0.00425371]
All agents episode reward: [-0.00425371]
Agent gate_2 episode reward: [-0.00463833]
All agents episode reward: [-0.00463833]
Agent gate_2 episode reward: [-0.001391]
All agents episode reward: [-0.001391]
Agent gate_2 episode reward: [-0.00014976]
All agents episode reward: [-0.00014976]
Agent gate_2 episode reward: [-0.00463888]
All agents episode reward: [-0.00463888]
Loaded 1 agents from ppo_agents_butterfly_scA
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -1373336320.000 | Total reward: -1373336320.000
Saved run 1 to rl_training/butterfly_scA/ppo_run1
  Run 2/10... Avg agent reward (episode): -1376346240.000 | Total reward: -1376346240.000
Saved run 2 to rl_training/butterfly_scA/ppo_run2
  Run 3/10... Avg agent reward (episode): -1371099520.000 | Total reward: -1371099520.000
Saved run 3 to rl_training/butterfly_scA/ppo_run3
  Run 4/10... Avg agent reward (episode): -1394834560.000 | Total reward: -1394834560.000
Saved run 4 to rl_training/butterfly_scA/ppo_run4
  Run 5/10... Avg agent reward (episode): -1386998656.000 | Total reward: -1386998656.000
Saved run 5 to rl_training/butterfly_scA/ppo_run5
  Run 6/10... Avg agent reward (episode): -1390844544.000 | Total reward: -1390844544.000
Saved run 6 to rl_training/butterfly_scA/ppo_run6
  Run 7/10... Avg agent reward (episode): -1386771712.000 | Total reward: -1386771712.000
Saved run 7 to rl_training/butterfly_scA/ppo_run7
  Run 8/10... Avg agent reward (episode): -1391281024.000 | Total reward: -1391281024.000
Saved run 8 to rl_training/butterfly_scA/ppo_run8
  Run 9/10... Avg agent reward (episode): -1383653376.000 | Total reward: -1383653376.000
Saved run 9 to rl_training/butterfly_scA/ppo_run9
  Run 10/10... Avg agent reward (episode): -1396705664.000 | Total reward: -1396705664.000
Saved run 10 to rl_training/butterfly_scA/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -1385187072.000 ± 8485871.000
  Average reward: -1385187072.000 ± 8485871.000
  Total reward: -1385187072.000 ± 8485871.000
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): 0.000 | Total reward: 0.000
Saved run 1 to rl_training/butterfly_scA/rule_based_run1
  Run 2/10... Avg agent reward (episode): -119211448.000 | Total reward: -119211448.000
Saved run 2 to rl_training/butterfly_scA/rule_based_run2
  Run 3/10... Avg agent reward (episode): -189657632.000 | Total reward: -189657632.000
Saved run 3 to rl_training/butterfly_scA/rule_based_run3
  Run 4/10... Avg agent reward (episode): -145388784.000 | Total reward: -145388784.000
Saved run 4 to rl_training/butterfly_scA/rule_based_run4
  Run 5/10... Avg agent reward (episode): -203586272.000 | Total reward: -203586272.000
Saved run 5 to rl_training/butterfly_scA/rule_based_run5
  Run 6/10... Avg agent reward (episode): -187474448.000 | Total reward: -187474448.000
Saved run 6 to rl_training/butterfly_scA/rule_based_run6
  Run 7/10... Avg agent reward (episode): -191172144.000 | Total reward: -191172144.000
Saved run 7 to rl_training/butterfly_scA/rule_based_run7
  Run 8/10... Avg agent reward (episode): -160784048.000 | Total reward: -160784048.000
Saved run 8 to rl_training/butterfly_scA/rule_based_run8
  Run 9/10... Avg agent reward (episode): -185810640.000 | Total reward: -185810640.000
Saved run 9 to rl_training/butterfly_scA/rule_based_run9
  Run 10/10... Avg agent reward (episode): -201018592.000 | Total reward: -201018592.000
Saved run 10 to rl_training/butterfly_scA/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -158410416.000 ± 58572300.000
  Average reward: -158410416.000 ± 58572300.000
  Total reward: -158410416.000 ± 58572300.000
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): 0.000 | Total reward: 0.000
Saved run 1 to rl_training/butterfly_scA/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -119211448.000 | Total reward: -119211448.000
Saved run 2 to rl_training/butterfly_scA/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -189657632.000 | Total reward: -189657632.000
Saved run 3 to rl_training/butterfly_scA/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -145388784.000 | Total reward: -145388784.000
Saved run 4 to rl_training/butterfly_scA/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -203586272.000 | Total reward: -203586272.000
Saved run 5 to rl_training/butterfly_scA/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -187474448.000 | Total reward: -187474448.000
Saved run 6 to rl_training/butterfly_scA/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -191172144.000 | Total reward: -191172144.000
Saved run 7 to rl_training/butterfly_scA/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -160784048.000 | Total reward: -160784048.000
Saved run 8 to rl_training/butterfly_scA/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -185810640.000 | Total reward: -185810640.000
Saved run 9 to rl_training/butterfly_scA/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -201018592.000 | Total reward: -201018592.000
Saved run 10 to rl_training/butterfly_scA/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -158410416.000 ± 58572300.000
  Average reward: -158410416.000 ± 58572300.000
  Total reward: -158410416.000 ± 58572300.000
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -1385187072.000
Rule-based avg reward: -158410416.000
No control avg reward: -158410416.000
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
