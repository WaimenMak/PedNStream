Iteration 0: 100%|██████████| 10/10 [00:23<00:00,  2.38s/it, episode=10, norm_ret=-12.804, true_ret=-1038752320.000, steps=600]
Agent gate_2 episode reward: [-106.19923931]
All agents episode reward: [-106.19923931]
Agent gate_2 episode reward: [-0.52172849]
All agents episode reward: [-0.52172849]
Agent gate_2 episode reward: [-8.60721229]
All agents episode reward: [-8.60721229]
Agent gate_2 episode reward: [-0.21946273]
All agents episode reward: [-0.21946273]
Agent gate_2 episode reward: [-4.46772088]
All agents episode reward: [-4.46772088]
Agent gate_2 episode reward: [-1.97349197]
All agents episode reward: [-1.97349197]
Agent gate_2 episode reward: [-4.15428351]
All agents episode reward: [-4.15428351]
Agent gate_2 episode reward: [-0.47942081]
All agents episode reward: [-0.47942081]
Agent gate_2 episode reward: [-0.4155987]
All agents episode reward: [-0.4155987]
Agent gate_2 episode reward: [-0.99753361]
All agents episode reward: [-0.99753361]
Iteration 1: 100%|██████████| 10/10 [00:23<00:00,  2.38s/it, episode=20, norm_ret=-0.412, true_ret=-468708224.000, steps=600]
Agent gate_2 episode reward: [-0.77239475]
All agents episode reward: [-0.77239475]
Agent gate_2 episode reward: [-0.34709216]
All agents episode reward: [-0.34709216]
Agent gate_2 episode reward: [-0.3130819]
All agents episode reward: [-0.3130819]
Agent gate_2 episode reward: [-0.33656359]
All agents episode reward: [-0.33656359]
Agent gate_2 episode reward: [-0.35537946]
All agents episode reward: [-0.35537946]
Agent gate_2 episode reward: [-0.32324402]
All agents episode reward: [-0.32324402]
Agent gate_2 episode reward: [-0.37517227]
All agents episode reward: [-0.37517227]
Agent gate_2 episode reward: [-0.36554248]
All agents episode reward: [-0.36554248]
Agent gate_2 episode reward: [-0.32485337]
All agents episode reward: [-0.32485337]
Agent gate_2 episode reward: [-0.60787228]
All agents episode reward: [-0.60787228]
Iteration 2: 100%|██████████| 10/10 [00:22<00:00,  2.27s/it, episode=30, norm_ret=-0.415, true_ret=-284673664.000, steps=600]
Agent gate_2 episode reward: [-0.3852144]
All agents episode reward: [-0.3852144]
Agent gate_2 episode reward: [-0.3872642]
All agents episode reward: [-0.3872642]
Agent gate_2 episode reward: [-0.39337305]
All agents episode reward: [-0.39337305]
Agent gate_2 episode reward: [-0.41890724]
All agents episode reward: [-0.41890724]
Agent gate_2 episode reward: [-0.41156815]
All agents episode reward: [-0.41156815]
Agent gate_2 episode reward: [-0.42675625]
All agents episode reward: [-0.42675625]
Agent gate_2 episode reward: [-0.41472877]
All agents episode reward: [-0.41472877]
Agent gate_2 episode reward: [-0.42603864]
All agents episode reward: [-0.42603864]
Agent gate_2 episode reward: [-0.44564478]
All agents episode reward: [-0.44564478]
Agent gate_2 episode reward: [-0.44430452]
All agents episode reward: [-0.44430452]
Iteration 3: 100%|██████████| 10/10 [00:22<00:00,  2.28s/it, episode=40, norm_ret=-0.501, true_ret=-292490592.000, steps=600]
Agent gate_2 episode reward: [-0.47927041]
All agents episode reward: [-0.47927041]
Agent gate_2 episode reward: [-0.45961202]
All agents episode reward: [-0.45961202]
Agent gate_2 episode reward: [-0.51543883]
All agents episode reward: [-0.51543883]
Agent gate_2 episode reward: [-0.49474621]
All agents episode reward: [-0.49474621]
Agent gate_2 episode reward: [-0.4922847]
All agents episode reward: [-0.4922847]
Agent gate_2 episode reward: [-0.48216842]
All agents episode reward: [-0.48216842]
Agent gate_2 episode reward: [-0.51689166]
All agents episode reward: [-0.51689166]
Agent gate_2 episode reward: [-0.53465553]
All agents episode reward: [-0.53465553]
Agent gate_2 episode reward: [-0.51032269]
All agents episode reward: [-0.51032269]
Agent gate_2 episode reward: [-0.52307106]
All agents episode reward: [-0.52307106]
Iteration 4: 100%|██████████| 10/10 [00:22<00:00,  2.29s/it, episode=50, norm_ret=-0.562, true_ret=-271899712.000, steps=600]
Agent gate_2 episode reward: [-0.56566372]
All agents episode reward: [-0.56566372]
Agent gate_2 episode reward: [-0.55734388]
All agents episode reward: [-0.55734388]
Agent gate_2 episode reward: [-0.62867199]
All agents episode reward: [-0.62867199]
Agent gate_2 episode reward: [-0.5397708]
All agents episode reward: [-0.5397708]
Agent gate_2 episode reward: [-0.53201178]
All agents episode reward: [-0.53201178]
Agent gate_2 episode reward: [-0.53599584]
All agents episode reward: [-0.53599584]
Agent gate_2 episode reward: [-0.5579592]
All agents episode reward: [-0.5579592]
Agent gate_2 episode reward: [-0.58870839]
All agents episode reward: [-0.58870839]
Agent gate_2 episode reward: [-0.57564899]
All agents episode reward: [-0.57564899]
Agent gate_2 episode reward: [-0.5412831]
All agents episode reward: [-0.5412831]
Iteration 5: 100%|██████████| 10/10 [00:22<00:00,  2.24s/it, episode=60, norm_ret=-0.616, true_ret=-269268000.000, steps=600]
Agent gate_2 episode reward: [-0.60721385]
All agents episode reward: [-0.60721385]
Agent gate_2 episode reward: [-0.59316795]
All agents episode reward: [-0.59316795]
Agent gate_2 episode reward: [-0.58147844]
All agents episode reward: [-0.58147844]
Agent gate_2 episode reward: [-0.5860944]
All agents episode reward: [-0.5860944]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -0.676 at episode 55 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.67597369]
All agents episode reward: [-0.67597369]
Agent gate_2 episode reward: [-0.59247759]
All agents episode reward: [-0.59247759]
Agent gate_2 episode reward: [-0.5653663]
All agents episode reward: [-0.5653663]
Agent gate_2 episode reward: [-0.632758]
All agents episode reward: [-0.632758]
Agent gate_2 episode reward: [-0.74066054]
All agents episode reward: [-0.74066054]
Saved 1 agents to ppo_agents_butterfly_scC
New best average return achieved: -0.585 at episode 60 (saved all agents to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.58536515]
All agents episode reward: [-0.58536515]
Iteration 6: 100%|██████████| 10/10 [00:22<00:00,  2.28s/it, episode=70, norm_ret=-0.665, true_ret=-290859232.000, steps=600]
Agent gate_2 episode reward: [-0.62076259]
All agents episode reward: [-0.62076259]
Agent gate_2 episode reward: [-0.64410795]
All agents episode reward: [-0.64410795]
Agent gate_2 episode reward: [-0.65016482]
All agents episode reward: [-0.65016482]
Agent gate_2 episode reward: [-0.66630272]
All agents episode reward: [-0.66630272]
Agent gate_2 episode reward: [-0.63627828]
All agents episode reward: [-0.63627828]
Agent gate_2 episode reward: [-0.73539154]
All agents episode reward: [-0.73539154]
Agent gate_2 episode reward: [-0.67273035]
All agents episode reward: [-0.67273035]
Agent gate_2 episode reward: [-0.62299589]
All agents episode reward: [-0.62299589]
Agent gate_2 episode reward: [-0.71494135]
All agents episode reward: [-0.71494135]
Agent gate_2 episode reward: [-0.68138458]
All agents episode reward: [-0.68138458]
Iteration 7: 100%|██████████| 10/10 [00:22<00:00,  2.24s/it, episode=80, norm_ret=-0.710, true_ret=-295636832.000, steps=600]
Agent gate_2 episode reward: [-0.69261311]
All agents episode reward: [-0.69261311]
Agent gate_2 episode reward: [-0.66756338]
All agents episode reward: [-0.66756338]
Agent gate_2 episode reward: [-0.68423631]
All agents episode reward: [-0.68423631]
Agent gate_2 episode reward: [-0.72527272]
All agents episode reward: [-0.72527272]
Agent gate_2 episode reward: [-0.68889553]
All agents episode reward: [-0.68889553]
Agent gate_2 episode reward: [-0.76586646]
All agents episode reward: [-0.76586646]
Agent gate_2 episode reward: [-0.69641611]
All agents episode reward: [-0.69641611]
Agent gate_2 episode reward: [-0.70841344]
All agents episode reward: [-0.70841344]
Agent gate_2 episode reward: [-0.73161745]
All agents episode reward: [-0.73161745]
Agent gate_2 episode reward: [-0.73907382]
All agents episode reward: [-0.73907382]
Iteration 8: 100%|██████████| 10/10 [00:22<00:00,  2.23s/it, episode=90, norm_ret=-1.070, true_ret=-280132672.000, steps=600]
Agent gate_2 episode reward: [-0.74084462]
All agents episode reward: [-0.74084462]
Agent gate_2 episode reward: [-0.67736349]
All agents episode reward: [-0.67736349]
Agent gate_2 episode reward: [-0.70171213]
All agents episode reward: [-0.70171213]
Agent gate_2 episode reward: [-0.95145005]
All agents episode reward: [-0.95145005]
Agent gate_2 episode reward: [-0.85522404]
All agents episode reward: [-0.85522404]
Agent gate_2 episode reward: [-0.66452645]
All agents episode reward: [-0.66452645]
Agent gate_2 episode reward: [-1.76574744]
All agents episode reward: [-1.76574744]
Agent gate_2 episode reward: [-2.82996]
All agents episode reward: [-2.82996]
Agent gate_2 episode reward: [-0.77438095]
All agents episode reward: [-0.77438095]
Agent gate_2 episode reward: [-0.74071023]
All agents episode reward: [-0.74071023]
Iteration 9: 100%|██████████| 10/10 [00:22<00:00,  2.27s/it, episode=100, norm_ret=-3.406, true_ret=-294677216.000, steps=600]
Agent gate_2 episode reward: [-1.08000412]
All agents episode reward: [-1.08000412]
Agent gate_2 episode reward: [-0.74421534]
All agents episode reward: [-0.74421534]
Agent gate_2 episode reward: [-6.2406834]
All agents episode reward: [-6.2406834]
Agent gate_2 episode reward: [-14.75163825]
All agents episode reward: [-14.75163825]
Agent gate_2 episode reward: [-0.88674954]
All agents episode reward: [-0.88674954]
Agent gate_2 episode reward: [-0.84915598]
All agents episode reward: [-0.84915598]
Agent gate_2 episode reward: [-0.89829125]
All agents episode reward: [-0.89829125]
Agent gate_2 episode reward: [-7.00036449]
All agents episode reward: [-7.00036449]
Agent gate_2 episode reward: [-0.83084351]
All agents episode reward: [-0.83084351]
Agent gate_2 episode reward: [-0.77844533]
All agents episode reward: [-0.77844533]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -208450240.000 | Total reward: -208450240.000
Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Avg agent reward (episode): -368379040.000 | Total reward: -368379040.000
Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Avg agent reward (episode): -424928064.000 | Total reward: -424928064.000
Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Avg agent reward (episode): -509150880.000 | Total reward: -509150880.000
Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Avg agent reward (episode): -340241952.000 | Total reward: -340241952.000
Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Avg agent reward (episode): -424589088.000 | Total reward: -424589088.000
Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Avg agent reward (episode): -452144832.000 | Total reward: -452144832.000
Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Avg agent reward (episode): -374830624.000 | Total reward: -374830624.000
Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Avg agent reward (episode): -386448544.000 | Total reward: -386448544.000
Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Avg agent reward (episode): -282968512.000 | Total reward: -282968512.000
Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -377213216.000 ± 81512184.000
  Average reward: -377213216.000 ± 81512184.000
  Total reward: -377213216.000 ± 81512184.000
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -212225136.000 | Total reward: -212225136.000
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Avg agent reward (episode): -368379040.000 | Total reward: -368379040.000
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Avg agent reward (episode): -424930720.000 | Total reward: -424930720.000
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Avg agent reward (episode): -509151424.000 | Total reward: -509151424.000
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Avg agent reward (episode): -340241952.000 | Total reward: -340241952.000
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Avg agent reward (episode): -424589088.000 | Total reward: -424589088.000
Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Avg agent reward (episode): -454989280.000 | Total reward: -454989280.000
Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Avg agent reward (episode): -372457056.000 | Total reward: -372457056.000
Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Avg agent reward (episode): -388921664.000 | Total reward: -388921664.000
Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Avg agent reward (episode): -282969408.000 | Total reward: -282969408.000
Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -377885472.000 ± 81044008.000
  Average reward: -377885472.000 ± 81044008.000
  Total reward: -377885472.000 ± 81044008.000
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -212225136.000 | Total reward: -212225136.000
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -368379040.000 | Total reward: -368379040.000
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -424930720.000 | Total reward: -424930720.000
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -509151424.000 | Total reward: -509151424.000
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -340241952.000 | Total reward: -340241952.000
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -424589088.000 | Total reward: -424589088.000
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -454989280.000 | Total reward: -454989280.000
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -372457056.000 | Total reward: -372457056.000
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -388921664.000 | Total reward: -388921664.000
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -282969408.000 | Total reward: -282969408.000
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -377885472.000 ± 81044008.000
  Average reward: -377885472.000 ± 81044008.000
  Total reward: -377885472.000 ± 81044008.000
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -377213216.000
Rule-based avg reward: -377885472.000
No control avg reward: -377885472.000
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
