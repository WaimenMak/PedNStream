Iteration 0: 100%|██████████| 10/10 [00:20<00:00,  2.10s/it, episode=10, norm_ret=-11.315, true_ret=-542716.438, steps=600]
Agent gate_2 episode reward: [-50.73574961]
All agents episode reward: [-50.73574961]
Agent gate_2 episode reward: [-24.26555885]
All agents episode reward: [-24.26555885]
Agent gate_2 episode reward: [-4.03396406]
All agents episode reward: [-4.03396406]
Agent gate_2 episode reward: [-6.14530119]
All agents episode reward: [-6.14530119]
Agent gate_2 episode reward: [-3.44422038]
All agents episode reward: [-3.44422038]
Agent gate_2 episode reward: [-5.06479909]
All agents episode reward: [-5.06479909]
Agent gate_2 episode reward: [-6.09224714]
All agents episode reward: [-6.09224714]
Agent gate_2 episode reward: [-4.39373796]
All agents episode reward: [-4.39373796]
Agent gate_2 episode reward: [-5.02277583]
All agents episode reward: [-5.02277583]
Agent gate_2 episode reward: [-3.95277913]
All agents episode reward: [-3.95277913]
Iteration 1: 100%|██████████| 10/10 [00:21<00:00,  2.11s/it, episode=20, norm_ret=-6.018, true_ret=-708410.500, steps=600]
Agent gate_2 episode reward: [-4.13409631]
All agents episode reward: [-4.13409631]
Agent gate_2 episode reward: [-6.24566851]
All agents episode reward: [-6.24566851]
Agent gate_2 episode reward: [-3.09433587]
All agents episode reward: [-3.09433587]
Agent gate_2 episode reward: [-2.70721266]
All agents episode reward: [-2.70721266]
Agent gate_2 episode reward: [-8.02687292]
All agents episode reward: [-8.02687292]
Agent gate_2 episode reward: [-7.75025787]
All agents episode reward: [-7.75025787]
Agent gate_2 episode reward: [-6.9138448]
All agents episode reward: [-6.9138448]
Agent gate_2 episode reward: [-6.55596673]
All agents episode reward: [-6.55596673]
Agent gate_2 episode reward: [-8.02904687]
All agents episode reward: [-8.02904687]
Agent gate_2 episode reward: [-6.72333248]
All agents episode reward: [-6.72333248]
Iteration 2: 100%|██████████| 10/10 [00:20<00:00,  2.01s/it, episode=30, norm_ret=-7.343, true_ret=-676510.750, steps=600]
Agent gate_2 episode reward: [-6.63148979]
All agents episode reward: [-6.63148979]
Agent gate_2 episode reward: [-5.41451454]
All agents episode reward: [-5.41451454]
Agent gate_2 episode reward: [-8.22743675]
All agents episode reward: [-8.22743675]
Agent gate_2 episode reward: [-6.9467084]
All agents episode reward: [-6.9467084]
Agent gate_2 episode reward: [-7.94783684]
All agents episode reward: [-7.94783684]
Agent gate_2 episode reward: [-8.72078119]
All agents episode reward: [-8.72078119]
Agent gate_2 episode reward: [-6.62670789]
All agents episode reward: [-6.62670789]
Agent gate_2 episode reward: [-6.3578225]
All agents episode reward: [-6.3578225]
Agent gate_2 episode reward: [-9.06883212]
All agents episode reward: [-9.06883212]
Agent gate_2 episode reward: [-7.48483954]
All agents episode reward: [-7.48483954]
Iteration 3: 100%|██████████| 10/10 [00:20<00:00,  2.00s/it, episode=40, norm_ret=-9.410, true_ret=-704354.688, steps=600]
Agent gate_2 episode reward: [-7.60766506]
All agents episode reward: [-7.60766506]
Agent gate_2 episode reward: [-7.81209162]
All agents episode reward: [-7.81209162]
Agent gate_2 episode reward: [-9.47531322]
All agents episode reward: [-9.47531322]
Agent gate_2 episode reward: [-19.45824039]
All agents episode reward: [-19.45824039]
Agent gate_2 episode reward: [-8.90026192]
All agents episode reward: [-8.90026192]
Agent gate_2 episode reward: [-8.45498647]
All agents episode reward: [-8.45498647]
Agent gate_2 episode reward: [-10.17601807]
All agents episode reward: [-10.17601807]
Agent gate_2 episode reward: [-5.40850518]
All agents episode reward: [-5.40850518]
Agent gate_2 episode reward: [-9.42872568]
All agents episode reward: [-9.42872568]
Agent gate_2 episode reward: [-7.38039278]
All agents episode reward: [-7.38039278]
Iteration 4: 100%|██████████| 10/10 [00:20<00:00,  2.03s/it, episode=50, norm_ret=-9.333, true_ret=-915972.500, steps=600]
Agent gate_2 episode reward: [-7.427542]
All agents episode reward: [-7.427542]
Agent gate_2 episode reward: [-7.4367575]
All agents episode reward: [-7.4367575]
Agent gate_2 episode reward: [-8.3038337]
All agents episode reward: [-8.3038337]
Agent gate_2 episode reward: [-9.61034606]
All agents episode reward: [-9.61034606]
Agent gate_2 episode reward: [-8.98750637]
All agents episode reward: [-8.98750637]
Agent gate_2 episode reward: [-8.8056852]
All agents episode reward: [-8.8056852]
Agent gate_2 episode reward: [-9.64391352]
All agents episode reward: [-9.64391352]
Agent gate_2 episode reward: [-11.30769265]
All agents episode reward: [-11.30769265]
Agent gate_2 episode reward: [-11.54921998]
All agents episode reward: [-11.54921998]
Agent gate_2 episode reward: [-10.25374582]
All agents episode reward: [-10.25374582]
Iteration 5: 100%|██████████| 10/10 [00:35<00:00,  3.59s/it, episode=60, norm_ret=-7.957, true_ret=-733556.500, steps=600]
Agent gate_2 episode reward: [-10.65717027]
All agents episode reward: [-10.65717027]
Agent gate_2 episode reward: [-3.69489367]
All agents episode reward: [-3.69489367]
Agent gate_2 episode reward: [-10.05668837]
All agents episode reward: [-10.05668837]
Agent gate_2 episode reward: [-7.78398324]
All agents episode reward: [-7.78398324]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -687903.375 at episode 55 (over 10 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-3.74934378]
All agents episode reward: [-3.74934378]
Agent gate_2 episode reward: [-8.353479]
All agents episode reward: [-8.353479]
Agent gate_2 episode reward: [-7.61722279]
All agents episode reward: [-7.61722279]
Agent gate_2 episode reward: [-9.5037457]
All agents episode reward: [-9.5037457]
Agent gate_2 episode reward: [-9.07691857]
All agents episode reward: [-9.07691857]
Agent gate_2 episode reward: [-9.0765112]
All agents episode reward: [-9.0765112]
Iteration 6: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it, episode=70, norm_ret=-8.369, true_ret=-454386.625, steps=600]
Agent gate_2 episode reward: [-4.19512869]
All agents episode reward: [-4.19512869]
Agent gate_2 episode reward: [-11.00600882]
All agents episode reward: [-11.00600882]
Agent gate_2 episode reward: [-8.45939493]
All agents episode reward: [-8.45939493]
Agent gate_2 episode reward: [-8.10099716]
All agents episode reward: [-8.10099716]
Agent gate_2 episode reward: [-12.92875541]
All agents episode reward: [-12.92875541]
Agent gate_2 episode reward: [-10.84282194]
All agents episode reward: [-10.84282194]
Agent gate_2 episode reward: [-8.57019963]
All agents episode reward: [-8.57019963]
Agent gate_2 episode reward: [-8.98462033]
All agents episode reward: [-8.98462033]
Agent gate_2 episode reward: [-4.45827519]
All agents episode reward: [-4.45827519]
Agent gate_2 episode reward: [-6.14456501]
All agents episode reward: [-6.14456501]
Iteration 7: 100%|██████████| 10/10 [00:34<00:00,  3.48s/it, episode=80, norm_ret=-11.200, true_ret=-345034.438, steps=600]
Agent gate_2 episode reward: [-12.3383201]
All agents episode reward: [-12.3383201]
Agent gate_2 episode reward: [-12.42679584]
All agents episode reward: [-12.42679584]
Agent gate_2 episode reward: [-11.72327272]
All agents episode reward: [-11.72327272]
Agent gate_2 episode reward: [-11.0900068]
All agents episode reward: [-11.0900068]
Agent gate_2 episode reward: [-12.39236646]
All agents episode reward: [-12.39236646]
Agent gate_2 episode reward: [-13.99340309]
All agents episode reward: [-13.99340309]
Agent gate_2 episode reward: [-13.31955576]
All agents episode reward: [-13.31955576]
Agent gate_2 episode reward: [-11.43050033]
All agents episode reward: [-11.43050033]
Agent gate_2 episode reward: [-8.36149514]
All agents episode reward: [-8.36149514]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -674802.625 at episode 80 (over 10 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.92159483]
All agents episode reward: [-4.92159483]
Iteration 8: 100%|██████████| 10/10 [00:36<00:00,  3.67s/it, episode=90, norm_ret=-10.455, true_ret=-654587.062, steps=600]
Agent gate_2 episode reward: [-7.77518709]
All agents episode reward: [-7.77518709]
Agent gate_2 episode reward: [-11.11734387]
All agents episode reward: [-11.11734387]
Agent gate_2 episode reward: [-4.76889308]
All agents episode reward: [-4.76889308]
Agent gate_2 episode reward: [-12.59053453]
All agents episode reward: [-12.59053453]
Agent gate_2 episode reward: [-11.30805033]
All agents episode reward: [-11.30805033]
Agent gate_2 episode reward: [-12.00791519]
All agents episode reward: [-12.00791519]
Agent gate_2 episode reward: [-12.64342065]
All agents episode reward: [-12.64342065]
Agent gate_2 episode reward: [-10.26629614]
All agents episode reward: [-10.26629614]
Agent gate_2 episode reward: [-12.32307364]
All agents episode reward: [-12.32307364]
Agent gate_2 episode reward: [-9.75092677]
All agents episode reward: [-9.75092677]
Iteration 9: 100%|██████████| 10/10 [00:36<00:00,  3.63s/it, episode=100, norm_ret=-10.830, true_ret=-330076.375, steps=600]
Agent gate_2 episode reward: [-15.28676919]
All agents episode reward: [-15.28676919]
Agent gate_2 episode reward: [-14.84796197]
All agents episode reward: [-14.84796197]
Agent gate_2 episode reward: [-10.66051234]
All agents episode reward: [-10.66051234]
Agent gate_2 episode reward: [-10.18837842]
All agents episode reward: [-10.18837842]
Agent gate_2 episode reward: [-9.49323629]
All agents episode reward: [-9.49323629]
Agent gate_2 episode reward: [-10.39941879]
All agents episode reward: [-10.39941879]
Agent gate_2 episode reward: [-13.38052267]
All agents episode reward: [-13.38052267]
Agent gate_2 episode reward: [-11.76282847]
All agents episode reward: [-11.76282847]
Agent gate_2 episode reward: [-7.23048726]
All agents episode reward: [-7.23048726]
Agent gate_2 episode reward: [-5.05357243]
All agents episode reward: [-5.05357243]
Saved 1 agents to ppo_agents_butterfly_scC
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -614315.000 | Total reward: -614315.000
Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Avg agent reward (episode): -801556.438 | Total reward: -801556.438
Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Avg agent reward (episode): -863936.750 | Total reward: -863936.750
Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Avg agent reward (episode): -961324.812 | Total reward: -961324.812
Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Avg agent reward (episode): -765001.125 | Total reward: -765001.125
Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Avg agent reward (episode): -862312.625 | Total reward: -862312.625
Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Avg agent reward (episode): -896855.812 | Total reward: -896855.812
Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Avg agent reward (episode): -803582.875 | Total reward: -803582.875
Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Avg agent reward (episode): -824091.250 | Total reward: -824091.250
Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Avg agent reward (episode): -709565.312 | Total reward: -709565.312
Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -810254.250 ± 93168.633
  Average reward: -810254.250 ± 93168.633
  Total reward: -810254.250 ± 93168.633
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -769929.375 | Total reward: -769929.375
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Avg agent reward (episode): -1122176.625 | Total reward: -1122176.625
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Avg agent reward (episode): -1188117.750 | Total reward: -1188117.750
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Avg agent reward (episode): -1527874.500 | Total reward: -1527874.500
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Avg agent reward (episode): -1647312512.000 | Total reward: -1647312512.000
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Avg agent reward (episode): -1179711.125 | Total reward: -1179711.125
Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Avg agent reward (episode): -1203257.500 | Total reward: -1203257.500
Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Avg agent reward (episode): -2040014848.000 | Total reward: -2040014848.000
Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Avg agent reward (episode): -1143458.125 | Total reward: -1143458.125
Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Avg agent reward (episode): -895271.812 | Total reward: -895271.812
Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -369635680.000 ± 742226624.000
  Average reward: -369635680.000 ± 742226624.000
  Total reward: -369635680.000 ± 742226624.000
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -614315.000 | Total reward: -614315.000
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -801556.438 | Total reward: -801556.438
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -863936.750 | Total reward: -863936.750
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -961324.812 | Total reward: -961324.812
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -765001.125 | Total reward: -765001.125
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -862312.625 | Total reward: -862312.625
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -896855.812 | Total reward: -896855.812
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -803582.875 | Total reward: -803582.875
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -824091.250 | Total reward: -824091.250
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -709565.312 | Total reward: -709565.312
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -810254.250 ± 93168.633
  Average reward: -810254.250 ± 93168.633
  Total reward: -810254.250 ± 93168.633
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -810254.250
Rule-based avg reward: -369635680.000
No control avg reward: -810254.250
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
