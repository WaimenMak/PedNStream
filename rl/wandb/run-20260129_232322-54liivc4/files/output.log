Iteration 0: 100%|██████████| 10/10 [00:17<00:00,  1.72s/it, episode=10, norm_ret=-9.560, true_ret=-938921.500, steps=600]
Agent gate_2 episode reward: [-57.36716083]
All agents episode reward: [-57.36716083]
Agent gate_2 episode reward: [-5.58683516]
All agents episode reward: [-5.58683516]
Agent gate_2 episode reward: [-9.73212648]
All agents episode reward: [-9.73212648]
Agent gate_2 episode reward: [-3.65040438]
All agents episode reward: [-3.65040438]
Agent gate_2 episode reward: [-2.54805461]
All agents episode reward: [-2.54805461]
Agent gate_2 episode reward: [-5.33604135]
All agents episode reward: [-5.33604135]
Agent gate_2 episode reward: [-1.97073403]
All agents episode reward: [-1.97073403]
Agent gate_2 episode reward: [-3.06899981]
All agents episode reward: [-3.06899981]
Agent gate_2 episode reward: [-4.41541883]
All agents episode reward: [-4.41541883]
Agent gate_2 episode reward: [-1.92301504]
All agents episode reward: [-1.92301504]
Iteration 1: 100%|██████████| 10/10 [00:17<00:00,  1.78s/it, episode=20, norm_ret=-2.548, true_ret=-825875.062, steps=600]
Agent gate_2 episode reward: [-2.45388933]
All agents episode reward: [-2.45388933]
Agent gate_2 episode reward: [-1.8607304]
All agents episode reward: [-1.8607304]
Agent gate_2 episode reward: [-2.04970093]
All agents episode reward: [-2.04970093]
Agent gate_2 episode reward: [-5.21486345]
All agents episode reward: [-5.21486345]
Agent gate_2 episode reward: [-2.28322477]
All agents episode reward: [-2.28322477]
Agent gate_2 episode reward: [-1.79039224]
All agents episode reward: [-1.79039224]
Agent gate_2 episode reward: [-2.75060103]
All agents episode reward: [-2.75060103]
Agent gate_2 episode reward: [-2.06982302]
All agents episode reward: [-2.06982302]
Agent gate_2 episode reward: [-2.8246442]
All agents episode reward: [-2.8246442]
Agent gate_2 episode reward: [-2.18587105]
All agents episode reward: [-2.18587105]
Iteration 2: 100%|██████████| 10/10 [00:17<00:00,  1.71s/it, episode=30, norm_ret=-2.422, true_ret=-777797.938, steps=600]
Agent gate_2 episode reward: [-2.08045124]
All agents episode reward: [-2.08045124]
Agent gate_2 episode reward: [-2.01041832]
All agents episode reward: [-2.01041832]
Agent gate_2 episode reward: [-2.16307061]
All agents episode reward: [-2.16307061]
Agent gate_2 episode reward: [-2.08916106]
All agents episode reward: [-2.08916106]
Agent gate_2 episode reward: [-4.21258258]
All agents episode reward: [-4.21258258]
Agent gate_2 episode reward: [-1.98486755]
All agents episode reward: [-1.98486755]
Agent gate_2 episode reward: [-2.40514901]
All agents episode reward: [-2.40514901]
Agent gate_2 episode reward: [-2.4867514]
All agents episode reward: [-2.4867514]
Agent gate_2 episode reward: [-2.3612726]
All agents episode reward: [-2.3612726]
Agent gate_2 episode reward: [-2.42808576]
All agents episode reward: [-2.42808576]
Iteration 3: 100%|██████████| 10/10 [00:17<00:00,  1.71s/it, episode=40, norm_ret=-2.452, true_ret=-746417.500, steps=600]
Agent gate_2 episode reward: [-2.39158012]
All agents episode reward: [-2.39158012]
Agent gate_2 episode reward: [-2.59628445]
All agents episode reward: [-2.59628445]
Agent gate_2 episode reward: [-2.35429778]
All agents episode reward: [-2.35429778]
Agent gate_2 episode reward: [-2.23759463]
All agents episode reward: [-2.23759463]
Agent gate_2 episode reward: [-2.45711118]
All agents episode reward: [-2.45711118]
Agent gate_2 episode reward: [-2.56362335]
All agents episode reward: [-2.56362335]
Agent gate_2 episode reward: [-2.40989534]
All agents episode reward: [-2.40989534]
Agent gate_2 episode reward: [-2.3027983]
All agents episode reward: [-2.3027983]
Agent gate_2 episode reward: [-2.57016059]
All agents episode reward: [-2.57016059]
Agent gate_2 episode reward: [-2.63539188]
All agents episode reward: [-2.63539188]
Iteration 4: 100%|██████████| 10/10 [00:17<00:00,  1.74s/it, episode=50, norm_ret=-3.059, true_ret=-839875.250, steps=600]
Agent gate_2 episode reward: [-2.7461171]
All agents episode reward: [-2.7461171]
Agent gate_2 episode reward: [-2.5490202]
All agents episode reward: [-2.5490202]
Agent gate_2 episode reward: [-3.56018298]
All agents episode reward: [-3.56018298]
Agent gate_2 episode reward: [-3.08393581]
All agents episode reward: [-3.08393581]
Agent gate_2 episode reward: [-2.91281995]
All agents episode reward: [-2.91281995]
Agent gate_2 episode reward: [-2.72715466]
All agents episode reward: [-2.72715466]
Agent gate_2 episode reward: [-3.06248997]
All agents episode reward: [-3.06248997]
Agent gate_2 episode reward: [-3.45539773]
All agents episode reward: [-3.45539773]
Agent gate_2 episode reward: [-3.22029008]
All agents episode reward: [-3.22029008]
Agent gate_2 episode reward: [-3.27222015]
All agents episode reward: [-3.27222015]
Iteration 5: 100%|██████████| 10/10 [00:29<00:00,  2.95s/it, episode=60, norm_ret=-1.749, true_ret=-875839.625, steps=600]
Agent gate_2 episode reward: [-3.13092952]
All agents episode reward: [-3.13092952]
Agent gate_2 episode reward: [-3.21839124]
All agents episode reward: [-3.21839124]
Agent gate_2 episode reward: [-3.97219422]
All agents episode reward: [-3.97219422]
Agent gate_2 episode reward: [-4.12376088]
All agents episode reward: [-4.12376088]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -2179556352.000 at episode 55 (over 10 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-3.03420197]
All agents episode reward: [-3.03420197]
Agent gate_2 episode reward: [-0.0013708]
All agents episode reward: [-0.0013708]
Agent gate_2 episode reward: [-0.00187644]
All agents episode reward: [-0.00187644]
Agent gate_2 episode reward: [-0.00169774]
All agents episode reward: [-0.00169774]
Agent gate_2 episode reward: [-0.00261931]
All agents episode reward: [-0.00261931]
Agent gate_2 episode reward: [-0.00166443]
All agents episode reward: [-0.00166443]
Iteration 6: 100%|██████████| 10/10 [00:29<00:00,  2.91s/it, episode=70, norm_ret=-0.000, true_ret=-657272.875, steps=600]
Agent gate_2 episode reward: [-0.00025248]
All agents episode reward: [-0.00025248]
Agent gate_2 episode reward: [-0.00011855]
All agents episode reward: [-0.00011855]
Agent gate_2 episode reward: [-0.00022936]
All agents episode reward: [-0.00022936]
Agent gate_2 episode reward: [-0.00040432]
All agents episode reward: [-0.00040432]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -2829866.000 at episode 65 (over 10 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-0.00036516]
All agents episode reward: [-0.00036516]
Agent gate_2 episode reward: [-7.26244713e-05]
All agents episode reward: [-7.26244713e-05]
Agent gate_2 episode reward: [-0.00013045]
All agents episode reward: [-0.00013045]
Agent gate_2 episode reward: [-6.91426216e-05]
All agents episode reward: [-6.91426216e-05]
Agent gate_2 episode reward: [-6.49576027e-05]
All agents episode reward: [-6.49576027e-05]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -2068392.625 at episode 70 (over 10 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-6.74724662e-05]
All agents episode reward: [-6.74724662e-05]
Iteration 7: 100%|██████████| 10/10 [00:29<00:00,  2.97s/it, episode=80, norm_ret=-0.001, true_ret=-18111642.000, steps=600]
Agent gate_2 episode reward: [-0.00127536]
All agents episode reward: [-0.00127536]
Agent gate_2 episode reward: [-0.0002655]
All agents episode reward: [-0.0002655]
Agent gate_2 episode reward: [-0.00033378]
All agents episode reward: [-0.00033378]
Agent gate_2 episode reward: [-0.00116955]
All agents episode reward: [-0.00116955]
Agent gate_2 episode reward: [-0.00102428]
All agents episode reward: [-0.00102428]
Agent gate_2 episode reward: [-0.00109716]
All agents episode reward: [-0.00109716]
Agent gate_2 episode reward: [-0.00013691]
All agents episode reward: [-0.00013691]
Agent gate_2 episode reward: [-0.00080113]
All agents episode reward: [-0.00080113]
Agent gate_2 episode reward: [-0.0011036]
All agents episode reward: [-0.0011036]
Agent gate_2 episode reward: [-0.00211369]
All agents episode reward: [-0.00211369]
Iteration 8: 100%|██████████| 10/10 [00:29<00:00,  2.94s/it, episode=90, norm_ret=-0.002, true_ret=-29737372.000, steps=600]
Agent gate_2 episode reward: [-0.00098291]
All agents episode reward: [-0.00098291]
Agent gate_2 episode reward: [-0.00164018]
All agents episode reward: [-0.00164018]
Agent gate_2 episode reward: [-0.00176739]
All agents episode reward: [-0.00176739]
Agent gate_2 episode reward: [-0.00109661]
All agents episode reward: [-0.00109661]
Agent gate_2 episode reward: [-0.00282603]
All agents episode reward: [-0.00282603]
Agent gate_2 episode reward: [-0.00181757]
All agents episode reward: [-0.00181757]
Agent gate_2 episode reward: [-0.00232266]
All agents episode reward: [-0.00232266]
Agent gate_2 episode reward: [-0.00421811]
All agents episode reward: [-0.00421811]
Agent gate_2 episode reward: [-0.00248542]
All agents episode reward: [-0.00248542]
Agent gate_2 episode reward: [-0.00357936]
All agents episode reward: [-0.00357936]
Iteration 9: 100%|██████████| 10/10 [00:29<00:00,  2.96s/it, episode=100, norm_ret=-0.003, true_ret=-19058272.000, steps=600]
Agent gate_2 episode reward: [-0.00278663]
All agents episode reward: [-0.00278663]
Agent gate_2 episode reward: [-0.00316862]
All agents episode reward: [-0.00316862]
Agent gate_2 episode reward: [-0.00513266]
All agents episode reward: [-0.00513266]
Agent gate_2 episode reward: [-0.00511892]
All agents episode reward: [-0.00511892]
Agent gate_2 episode reward: [-0.00381885]
All agents episode reward: [-0.00381885]
Agent gate_2 episode reward: [-0.00203465]
All agents episode reward: [-0.00203465]
Agent gate_2 episode reward: [-0.00155391]
All agents episode reward: [-0.00155391]
Agent gate_2 episode reward: [-0.00243059]
All agents episode reward: [-0.00243059]
Agent gate_2 episode reward: [-0.00210425]
All agents episode reward: [-0.00210425]
Agent gate_2 episode reward: [-0.00154208]
All agents episode reward: [-0.00154208]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -1681241.250 | Total reward: -1681241.250
Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Avg agent reward (episode): -1701332.125 | Total reward: -1701332.125
Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Avg agent reward (episode): -1801305.375 | Total reward: -1801305.375
Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Avg agent reward (episode): -1832888.000 | Total reward: -1832888.000
Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Avg agent reward (episode): -1481542.250 | Total reward: -1481542.250
Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Avg agent reward (episode): -1773108.375 | Total reward: -1773108.375
Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Avg agent reward (episode): -1852693.875 | Total reward: -1852693.875
Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Avg agent reward (episode): -1466317.875 | Total reward: -1466317.875
Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Avg agent reward (episode): -1860409.750 | Total reward: -1860409.750
Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Avg agent reward (episode): -1599212.750 | Total reward: -1599212.750
Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -1705005.250 ± 139608.703
  Average reward: -1705005.250 ± 139608.703
  Total reward: -1705005.250 ± 139608.703
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -773379.438 | Total reward: -773379.438
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Avg agent reward (episode): -1126116.750 | Total reward: -1126116.750
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Avg agent reward (episode): -1192027.750 | Total reward: -1192027.750
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Avg agent reward (episode): -1531984.500 | Total reward: -1531984.500
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Avg agent reward (episode): -1647315840.000 | Total reward: -1647315840.000
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Avg agent reward (episode): -1183640.875 | Total reward: -1183640.875
Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Avg agent reward (episode): -1207137.500 | Total reward: -1207137.500
Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Avg agent reward (episode): -2040018304.000 | Total reward: -2040018304.000
Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Avg agent reward (episode): -1147738.125 | Total reward: -1147738.125
Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Avg agent reward (episode): -899541.812 | Total reward: -899541.812
Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -369639584.000 ± 742226496.000
  Average reward: -369639584.000 ± 742226496.000
  Total reward: -369639584.000 ± 742226496.000
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -619125.000 | Total reward: -619125.000
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -806306.500 | Total reward: -806306.500
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -868706.875 | Total reward: -868706.875
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -967434.812 | Total reward: -967434.812
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -768901.188 | Total reward: -768901.188
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -867192.625 | Total reward: -867192.625
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -902275.938 | Total reward: -902275.938
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -808262.875 | Total reward: -808262.875
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -828881.250 | Total reward: -828881.250
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -714115.375 | Total reward: -714115.375
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -815120.250 ± 93512.188
  Average reward: -815120.250 ± 93512.188
  Total reward: -815120.250 ± 93512.188
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -1705005.250
Rule-based avg reward: -369639584.000
No control avg reward: -815120.250
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
