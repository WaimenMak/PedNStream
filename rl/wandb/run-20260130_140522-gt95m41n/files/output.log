Iteration 0: 100%|██████████| 10/10 [00:23<00:00,  2.37s/it, episode=10, norm_ret=-7.427, true_ret=-867847.938, steps=600]
Agent gate_2 episode reward: [-55.67774931]
All agents episode reward: [-55.67774931]
Agent gate_2 episode reward: [-2.77814428]
All agents episode reward: [-2.77814428]
Agent gate_2 episode reward: [-1.84899619]
All agents episode reward: [-1.84899619]
Agent gate_2 episode reward: [-1.78044982]
All agents episode reward: [-1.78044982]
Agent gate_2 episode reward: [-2.09585076]
All agents episode reward: [-2.09585076]
Agent gate_2 episode reward: [-1.63383173]
All agents episode reward: [-1.63383173]
Agent gate_2 episode reward: [-1.84915575]
All agents episode reward: [-1.84915575]
Agent gate_2 episode reward: [-2.03028831]
All agents episode reward: [-2.03028831]
Agent gate_2 episode reward: [-2.04329005]
All agents episode reward: [-2.04329005]
Agent gate_2 episode reward: [-2.53628944]
All agents episode reward: [-2.53628944]
Iteration 1: 100%|██████████| 10/10 [00:22<00:00,  2.29s/it, episode=20, norm_ret=-2.564, true_ret=-741215.438, steps=600]
Agent gate_2 episode reward: [-2.18898906]
All agents episode reward: [-2.18898906]
Agent gate_2 episode reward: [-2.11688461]
All agents episode reward: [-2.11688461]
Agent gate_2 episode reward: [-2.42049737]
All agents episode reward: [-2.42049737]
Agent gate_2 episode reward: [-2.43088809]
All agents episode reward: [-2.43088809]
Agent gate_2 episode reward: [-2.66832214]
All agents episode reward: [-2.66832214]
Agent gate_2 episode reward: [-2.66755027]
All agents episode reward: [-2.66755027]
Agent gate_2 episode reward: [-2.6355502]
All agents episode reward: [-2.6355502]
Agent gate_2 episode reward: [-2.68828595]
All agents episode reward: [-2.68828595]
Agent gate_2 episode reward: [-2.84431835]
All agents episode reward: [-2.84431835]
Agent gate_2 episode reward: [-2.97964289]
All agents episode reward: [-2.97964289]
Iteration 2: 100%|██████████| 10/10 [00:24<00:00,  2.45s/it, episode=30, norm_ret=-3.271, true_ret=-705025.375, steps=600]
Agent gate_2 episode reward: [-3.13652926]
All agents episode reward: [-3.13652926]
Agent gate_2 episode reward: [-3.0650321]
All agents episode reward: [-3.0650321]
Agent gate_2 episode reward: [-3.05990637]
All agents episode reward: [-3.05990637]
Agent gate_2 episode reward: [-3.22210782]
All agents episode reward: [-3.22210782]
Agent gate_2 episode reward: [-3.21184598]
All agents episode reward: [-3.21184598]
Agent gate_2 episode reward: [-3.35710099]
All agents episode reward: [-3.35710099]
Agent gate_2 episode reward: [-3.32982862]
All agents episode reward: [-3.32982862]
Agent gate_2 episode reward: [-3.38026336]
All agents episode reward: [-3.38026336]
Agent gate_2 episode reward: [-3.52862662]
All agents episode reward: [-3.52862662]
Agent gate_2 episode reward: [-3.42237193]
All agents episode reward: [-3.42237193]
Iteration 3: 100%|██████████| 10/10 [00:24<00:00,  2.44s/it, episode=40, norm_ret=-3.816, true_ret=-723642.062, steps=600]
Agent gate_2 episode reward: [-3.64696494]
All agents episode reward: [-3.64696494]
Agent gate_2 episode reward: [-3.63138125]
All agents episode reward: [-3.63138125]
Agent gate_2 episode reward: [-3.68295429]
All agents episode reward: [-3.68295429]
Agent gate_2 episode reward: [-3.75388206]
All agents episode reward: [-3.75388206]
Agent gate_2 episode reward: [-3.80371819]
All agents episode reward: [-3.80371819]
Agent gate_2 episode reward: [-3.83584815]
All agents episode reward: [-3.83584815]
Agent gate_2 episode reward: [-3.84721117]
All agents episode reward: [-3.84721117]
Agent gate_2 episode reward: [-3.87594826]
All agents episode reward: [-3.87594826]
Agent gate_2 episode reward: [-4.07284955]
All agents episode reward: [-4.07284955]
Agent gate_2 episode reward: [-4.01146293]
All agents episode reward: [-4.01146293]
Iteration 4: 100%|██████████| 10/10 [00:23<00:00,  2.35s/it, episode=50, norm_ret=-4.244, true_ret=-733077.812, steps=600]
Agent gate_2 episode reward: [-3.97848296]
All agents episode reward: [-3.97848296]
Agent gate_2 episode reward: [-4.11665595]
All agents episode reward: [-4.11665595]
Agent gate_2 episode reward: [-3.97749616]
All agents episode reward: [-3.97749616]
Agent gate_2 episode reward: [-4.11204384]
All agents episode reward: [-4.11204384]
Agent gate_2 episode reward: [-4.34499787]
All agents episode reward: [-4.34499787]
Agent gate_2 episode reward: [-4.38869755]
All agents episode reward: [-4.38869755]
Agent gate_2 episode reward: [-4.29341731]
All agents episode reward: [-4.29341731]
Agent gate_2 episode reward: [-4.25351109]
All agents episode reward: [-4.25351109]
Agent gate_2 episode reward: [-4.47135215]
All agents episode reward: [-4.47135215]
Agent gate_2 episode reward: [-4.49881717]
All agents episode reward: [-4.49881717]
Iteration 5: 100%|██████████| 10/10 [00:28<00:00,  2.84s/it, episode=60, norm_ret=-4.719, true_ret=-737829.750, steps=600]
Agent gate_2 episode reward: [-4.58836848]
All agents episode reward: [-4.58836848]
Agent gate_2 episode reward: [-4.81839571]
All agents episode reward: [-4.81839571]
Agent gate_2 episode reward: [-4.52832742]
All agents episode reward: [-4.52832742]
Agent gate_2 episode reward: [-4.63105054]
All agents episode reward: [-4.63105054]
Agent gate_2 episode reward: [-4.68565543]
All agents episode reward: [-4.68565543]
Agent gate_2 episode reward: [-4.79451044]
All agents episode reward: [-4.79451044]
Agent gate_2 episode reward: [-4.83669902]
All agents episode reward: [-4.83669902]
Agent gate_2 episode reward: [-4.79680273]
All agents episode reward: [-4.79680273]
Agent gate_2 episode reward: [-4.59210757]
All agents episode reward: [-4.59210757]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -789775.562 at episode 60 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.91407969]
All agents episode reward: [-4.91407969]
Iteration 6: 100%|██████████| 10/10 [00:27<00:00,  2.74s/it, episode=70, norm_ret=-5.644, true_ret=-764643.438, steps=600]
Agent gate_2 episode reward: [-5.53678109]
All agents episode reward: [-5.53678109]
Agent gate_2 episode reward: [-5.51556231]
All agents episode reward: [-5.51556231]
Agent gate_2 episode reward: [-5.69140703]
All agents episode reward: [-5.69140703]
Agent gate_2 episode reward: [-5.57612042]
All agents episode reward: [-5.57612042]
Agent gate_2 episode reward: [-5.45916944]
All agents episode reward: [-5.45916944]
Agent gate_2 episode reward: [-5.91536932]
All agents episode reward: [-5.91536932]
Agent gate_2 episode reward: [-5.92814749]
All agents episode reward: [-5.92814749]
Agent gate_2 episode reward: [-5.57272063]
All agents episode reward: [-5.57272063]
Agent gate_2 episode reward: [-5.64201111]
All agents episode reward: [-5.64201111]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -640721.312 at episode 70 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-5.60768843]
All agents episode reward: [-5.60768843]
Iteration 7: 100%|██████████| 10/10 [00:28<00:00,  2.87s/it, episode=80, norm_ret=-6.370, true_ret=-825491.188, steps=600]
Agent gate_2 episode reward: [-6.22330725]
All agents episode reward: [-6.22330725]
Agent gate_2 episode reward: [-6.21113416]
All agents episode reward: [-6.21113416]
Agent gate_2 episode reward: [-6.32358598]
All agents episode reward: [-6.32358598]
Agent gate_2 episode reward: [-6.30742047]
All agents episode reward: [-6.30742047]
Agent gate_2 episode reward: [-6.32084093]
All agents episode reward: [-6.32084093]
Agent gate_2 episode reward: [-6.44439088]
All agents episode reward: [-6.44439088]
Agent gate_2 episode reward: [-6.45138909]
All agents episode reward: [-6.45138909]
Agent gate_2 episode reward: [-6.37062792]
All agents episode reward: [-6.37062792]
Agent gate_2 episode reward: [-6.52479068]
All agents episode reward: [-6.52479068]
Agent gate_2 episode reward: [-6.52434904]
All agents episode reward: [-6.52434904]
Iteration 8: 100%|██████████| 10/10 [00:25<00:00,  2.57s/it, episode=90, norm_ret=-6.412, true_ret=-774712.062, steps=600]
Agent gate_2 episode reward: [-6.25443679]
All agents episode reward: [-6.25443679]
Agent gate_2 episode reward: [-6.34404935]
All agents episode reward: [-6.34404935]
Agent gate_2 episode reward: [-6.32770546]
All agents episode reward: [-6.32770546]
Agent gate_2 episode reward: [-6.36004055]
All agents episode reward: [-6.36004055]
Agent gate_2 episode reward: [-6.41908298]
All agents episode reward: [-6.41908298]
Agent gate_2 episode reward: [-6.44407354]
All agents episode reward: [-6.44407354]
Agent gate_2 episode reward: [-6.47669912]
All agents episode reward: [-6.47669912]
Agent gate_2 episode reward: [-6.44952318]
All agents episode reward: [-6.44952318]
Agent gate_2 episode reward: [-6.53579782]
All agents episode reward: [-6.53579782]
Agent gate_2 episode reward: [-6.50676704]
All agents episode reward: [-6.50676704]
Iteration 9: 100%|██████████| 10/10 [00:26<00:00,  2.63s/it, episode=100, norm_ret=-6.738, true_ret=-776560.812, steps=600]
Agent gate_2 episode reward: [-6.57257083]
All agents episode reward: [-6.57257083]
Agent gate_2 episode reward: [-6.66821881]
All agents episode reward: [-6.66821881]
Agent gate_2 episode reward: [-6.72645423]
All agents episode reward: [-6.72645423]
Agent gate_2 episode reward: [-6.68284982]
All agents episode reward: [-6.68284982]
Agent gate_2 episode reward: [-6.7105917]
All agents episode reward: [-6.7105917]
Agent gate_2 episode reward: [-6.7367359]
All agents episode reward: [-6.7367359]
Agent gate_2 episode reward: [-6.81801069]
All agents episode reward: [-6.81801069]
Agent gate_2 episode reward: [-6.78109834]
All agents episode reward: [-6.78109834]
Agent gate_2 episode reward: [-6.80354255]
All agents episode reward: [-6.80354255]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -602797.438 at episode 100 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-6.8848204]
All agents episode reward: [-6.8848204]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -618266.688 | Total reward: -618266.688
Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Avg agent reward (episode): -807061.750 | Total reward: -807061.750
Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Avg agent reward (episode): -868924.812 | Total reward: -868924.812
Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Avg agent reward (episode): -967434.812 | Total reward: -967434.812
Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Avg agent reward (episode): -768901.188 | Total reward: -768901.188
Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Avg agent reward (episode): -867192.625 | Total reward: -867192.625
Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Avg agent reward (episode): -901375.875 | Total reward: -901375.875
Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Avg agent reward (episode): -811098.812 | Total reward: -811098.812
Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Avg agent reward (episode): -825840.000 | Total reward: -825840.000
Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Avg agent reward (episode): -710529.500 | Total reward: -710529.500
Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -814662.625 ± 93950.477
  Average reward: -814662.625 ± 93950.477
  Total reward: -814662.625 ± 93950.477
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -643709.938 | Total reward: -643709.938
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Avg agent reward (episode): -851560.312 | Total reward: -851560.312
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Avg agent reward (episode): -920434.062 | Total reward: -920434.062
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Avg agent reward (episode): -1048110.500 | Total reward: -1048110.500
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Avg agent reward (episode): -791232.562 | Total reward: -791232.562
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Avg agent reward (episode): -907931.000 | Total reward: -907931.000
Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Avg agent reward (episode): -959172.375 | Total reward: -959172.375
Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Avg agent reward (episode): -854216.688 | Total reward: -854216.688
Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Avg agent reward (episode): -870864.875 | Total reward: -870864.875
Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Avg agent reward (episode): -711501.500 | Total reward: -711501.500
Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -855873.375 ± 111707.195
  Average reward: -855873.375 ± 111707.195
  Total reward: -855873.375 ± 111707.195
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -619125.000 | Total reward: -619125.000
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -806306.500 | Total reward: -806306.500
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -868706.875 | Total reward: -868706.875
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -967434.812 | Total reward: -967434.812
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -768901.188 | Total reward: -768901.188
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -867192.625 | Total reward: -867192.625
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -902275.938 | Total reward: -902275.938
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -808262.875 | Total reward: -808262.875
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -828881.250 | Total reward: -828881.250
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -714115.375 | Total reward: -714115.375
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -815120.250 ± 93512.188
  Average reward: -815120.250 ± 93512.188
  Total reward: -815120.250 ± 93512.188
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -814662.625
Rule-based avg reward: -855873.375
No control avg reward: -815120.250
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
