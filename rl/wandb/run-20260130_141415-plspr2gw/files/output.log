Iteration 0: 100%|██████████| 10/10 [00:22<00:00,  2.30s/it, episode=10, norm_ret=-15.500, true_ret=-757254.750, steps=600]
Agent gate_2 episode reward: [-52.09895413]
All agents episode reward: [-52.09895413]
Agent gate_2 episode reward: [-14.0512954]
All agents episode reward: [-14.0512954]
Agent gate_2 episode reward: [-9.80872089]
All agents episode reward: [-9.80872089]
Agent gate_2 episode reward: [-12.44366585]
All agents episode reward: [-12.44366585]
Agent gate_2 episode reward: [-13.03224045]
All agents episode reward: [-13.03224045]
Agent gate_2 episode reward: [-9.58231392]
All agents episode reward: [-9.58231392]
Agent gate_2 episode reward: [-10.33066215]
All agents episode reward: [-10.33066215]
Agent gate_2 episode reward: [-10.54654856]
All agents episode reward: [-10.54654856]
Agent gate_2 episode reward: [-11.2043111]
All agents episode reward: [-11.2043111]
Agent gate_2 episode reward: [-11.90289381]
All agents episode reward: [-11.90289381]
Iteration 1: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it, episode=20, norm_ret=-12.143, true_ret=-646837.000, steps=600]
Agent gate_2 episode reward: [-11.20582437]
All agents episode reward: [-11.20582437]
Agent gate_2 episode reward: [-11.73923952]
All agents episode reward: [-11.73923952]
Agent gate_2 episode reward: [-11.89819158]
All agents episode reward: [-11.89819158]
Agent gate_2 episode reward: [-12.55417078]
All agents episode reward: [-12.55417078]
Agent gate_2 episode reward: [-12.08998049]
All agents episode reward: [-12.08998049]
Agent gate_2 episode reward: [-12.73768641]
All agents episode reward: [-12.73768641]
Agent gate_2 episode reward: [-11.62827111]
All agents episode reward: [-11.62827111]
Agent gate_2 episode reward: [-12.55664964]
All agents episode reward: [-12.55664964]
Agent gate_2 episode reward: [-13.44589992]
All agents episode reward: [-13.44589992]
Agent gate_2 episode reward: [-11.57134633]
All agents episode reward: [-11.57134633]
Iteration 2: 100%|██████████| 10/10 [00:22<00:00,  2.22s/it, episode=30, norm_ret=-14.545, true_ret=-717589.188, steps=600]
Agent gate_2 episode reward: [-20.19881259]
All agents episode reward: [-20.19881259]
Agent gate_2 episode reward: [-13.82429969]
All agents episode reward: [-13.82429969]
Agent gate_2 episode reward: [-11.933772]
All agents episode reward: [-11.933772]
Agent gate_2 episode reward: [-11.99683744]
All agents episode reward: [-11.99683744]
Agent gate_2 episode reward: [-12.57768305]
All agents episode reward: [-12.57768305]
Agent gate_2 episode reward: [-12.95375616]
All agents episode reward: [-12.95375616]
Agent gate_2 episode reward: [-12.66580471]
All agents episode reward: [-12.66580471]
Agent gate_2 episode reward: [-12.0645192]
All agents episode reward: [-12.0645192]
Agent gate_2 episode reward: [-27.44323731]
All agents episode reward: [-27.44323731]
Agent gate_2 episode reward: [-9.79510878]
All agents episode reward: [-9.79510878]
Iteration 3: 100%|██████████| 10/10 [00:22<00:00,  2.26s/it, episode=40, norm_ret=-10.425, true_ret=-743326.062, steps=600]
Agent gate_2 episode reward: [-10.27607245]
All agents episode reward: [-10.27607245]
Agent gate_2 episode reward: [-9.98031478]
All agents episode reward: [-9.98031478]
Agent gate_2 episode reward: [-10.08918212]
All agents episode reward: [-10.08918212]
Agent gate_2 episode reward: [-10.20725197]
All agents episode reward: [-10.20725197]
Agent gate_2 episode reward: [-10.69073933]
All agents episode reward: [-10.69073933]
Agent gate_2 episode reward: [-10.34141525]
All agents episode reward: [-10.34141525]
Agent gate_2 episode reward: [-10.20962611]
All agents episode reward: [-10.20962611]
Agent gate_2 episode reward: [-10.40638485]
All agents episode reward: [-10.40638485]
Agent gate_2 episode reward: [-11.07858414]
All agents episode reward: [-11.07858414]
Agent gate_2 episode reward: [-10.9729911]
All agents episode reward: [-10.9729911]
Iteration 4: 100%|██████████| 10/10 [00:22<00:00,  2.23s/it, episode=50, norm_ret=-11.071, true_ret=-724777.312, steps=600]
Agent gate_2 episode reward: [-10.74730626]
All agents episode reward: [-10.74730626]
Agent gate_2 episode reward: [-10.51477864]
All agents episode reward: [-10.51477864]
Agent gate_2 episode reward: [-11.73381887]
All agents episode reward: [-11.73381887]
Agent gate_2 episode reward: [-10.92775345]
All agents episode reward: [-10.92775345]
Agent gate_2 episode reward: [-11.17280939]
All agents episode reward: [-11.17280939]
Agent gate_2 episode reward: [-11.39719566]
All agents episode reward: [-11.39719566]
Agent gate_2 episode reward: [-10.97787668]
All agents episode reward: [-10.97787668]
Agent gate_2 episode reward: [-10.76473958]
All agents episode reward: [-10.76473958]
Agent gate_2 episode reward: [-11.18161457]
All agents episode reward: [-11.18161457]
Agent gate_2 episode reward: [-11.29191847]
All agents episode reward: [-11.29191847]
Iteration 5: 100%|██████████| 10/10 [00:27<00:00,  2.75s/it, episode=60, norm_ret=-11.568, true_ret=-719001.812, steps=600]
Agent gate_2 episode reward: [-10.80033325]
All agents episode reward: [-10.80033325]
Agent gate_2 episode reward: [-11.19712936]
All agents episode reward: [-11.19712936]
Agent gate_2 episode reward: [-11.6906648]
All agents episode reward: [-11.6906648]
Agent gate_2 episode reward: [-11.72703744]
All agents episode reward: [-11.72703744]
Agent gate_2 episode reward: [-11.90128593]
All agents episode reward: [-11.90128593]
Agent gate_2 episode reward: [-11.92016781]
All agents episode reward: [-11.92016781]
Agent gate_2 episode reward: [-11.39710627]
All agents episode reward: [-11.39710627]
Agent gate_2 episode reward: [-11.83186051]
All agents episode reward: [-11.83186051]
Agent gate_2 episode reward: [-11.56745587]
All agents episode reward: [-11.56745587]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -728089.375 at episode 60 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-11.64896088]
All agents episode reward: [-11.64896088]
Iteration 6: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it, episode=70, norm_ret=-10.081, true_ret=-604329.000, steps=600]
Agent gate_2 episode reward: [-9.82304839]
All agents episode reward: [-9.82304839]
Agent gate_2 episode reward: [-9.88429939]
All agents episode reward: [-9.88429939]
Agent gate_2 episode reward: [-10.04207217]
All agents episode reward: [-10.04207217]
Agent gate_2 episode reward: [-9.93949718]
All agents episode reward: [-9.93949718]
Agent gate_2 episode reward: [-10.18413483]
All agents episode reward: [-10.18413483]
Agent gate_2 episode reward: [-10.0232476]
All agents episode reward: [-10.0232476]
Agent gate_2 episode reward: [-10.17620227]
All agents episode reward: [-10.17620227]
Agent gate_2 episode reward: [-10.29263157]
All agents episode reward: [-10.29263157]
Agent gate_2 episode reward: [-10.12805063]
All agents episode reward: [-10.12805063]
Agent gate_2 episode reward: [-10.31433352]
All agents episode reward: [-10.31433352]
Iteration 7: 100%|██████████| 10/10 [00:27<00:00,  2.72s/it, episode=80, norm_ret=-14.905, true_ret=-866120.562, steps=600]
Agent gate_2 episode reward: [-14.86375308]
All agents episode reward: [-14.86375308]
Agent gate_2 episode reward: [-15.30527136]
All agents episode reward: [-15.30527136]
Agent gate_2 episode reward: [-14.79482375]
All agents episode reward: [-14.79482375]
Agent gate_2 episode reward: [-14.91828285]
All agents episode reward: [-14.91828285]
Agent gate_2 episode reward: [-15.04137481]
All agents episode reward: [-15.04137481]
Agent gate_2 episode reward: [-14.8699009]
All agents episode reward: [-14.8699009]
Agent gate_2 episode reward: [-14.97350441]
All agents episode reward: [-14.97350441]
Agent gate_2 episode reward: [-14.94347759]
All agents episode reward: [-14.94347759]
Agent gate_2 episode reward: [-14.64730036]
All agents episode reward: [-14.64730036]
Agent gate_2 episode reward: [-14.69486835]
All agents episode reward: [-14.69486835]
Iteration 8: 100%|██████████| 10/10 [00:26<00:00,  2.62s/it, episode=90, norm_ret=-13.683, true_ret=-804083.625, steps=600]
Agent gate_2 episode reward: [-13.61940423]
All agents episode reward: [-13.61940423]
Agent gate_2 episode reward: [-13.78884262]
All agents episode reward: [-13.78884262]
Agent gate_2 episode reward: [-13.58267832]
All agents episode reward: [-13.58267832]
Agent gate_2 episode reward: [-13.56287616]
All agents episode reward: [-13.56287616]
Agent gate_2 episode reward: [-13.65379812]
All agents episode reward: [-13.65379812]
Agent gate_2 episode reward: [-13.68015721]
All agents episode reward: [-13.68015721]
Agent gate_2 episode reward: [-13.54992375]
All agents episode reward: [-13.54992375]
Agent gate_2 episode reward: [-13.78963262]
All agents episode reward: [-13.78963262]
Agent gate_2 episode reward: [-13.79238802]
All agents episode reward: [-13.79238802]
Agent gate_2 episode reward: [-13.81278943]
All agents episode reward: [-13.81278943]
Iteration 9: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it, episode=100, norm_ret=-17.121, true_ret=-1023143.312, steps=600]
Agent gate_2 episode reward: [-17.04358895]
All agents episode reward: [-17.04358895]
Agent gate_2 episode reward: [-17.25695938]
All agents episode reward: [-17.25695938]
Agent gate_2 episode reward: [-17.04799723]
All agents episode reward: [-17.04799723]
Agent gate_2 episode reward: [-17.02014039]
All agents episode reward: [-17.02014039]
Agent gate_2 episode reward: [-17.00732231]
All agents episode reward: [-17.00732231]
Agent gate_2 episode reward: [-17.36795499]
All agents episode reward: [-17.36795499]
Agent gate_2 episode reward: [-17.23642455]
All agents episode reward: [-17.23642455]
Agent gate_2 episode reward: [-17.16992771]
All agents episode reward: [-17.16992771]
Agent gate_2 episode reward: [-16.93065693]
All agents episode reward: [-16.93065693]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -717752.750 at episode 100 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-17.12458138]
All agents episode reward: [-17.12458138]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -619125.000 | Total reward: -619125.000
Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Avg agent reward (episode): -806306.500 | Total reward: -806306.500
Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Avg agent reward (episode): -868706.875 | Total reward: -868706.875
Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Avg agent reward (episode): -967434.812 | Total reward: -967434.812
Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Avg agent reward (episode): -768901.188 | Total reward: -768901.188
Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Avg agent reward (episode): -867192.625 | Total reward: -867192.625
Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Avg agent reward (episode): -902275.938 | Total reward: -902275.938
Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Avg agent reward (episode): -808262.875 | Total reward: -808262.875
Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Avg agent reward (episode): -828881.250 | Total reward: -828881.250
Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Avg agent reward (episode): -714115.375 | Total reward: -714115.375
Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -815120.250 ± 93512.188
  Average reward: -815120.250 ± 93512.188
  Total reward: -815120.250 ± 93512.188
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -643709.938 | Total reward: -643709.938
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Avg agent reward (episode): -851560.312 | Total reward: -851560.312
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Avg agent reward (episode): -920434.062 | Total reward: -920434.062
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Avg agent reward (episode): -1048110.500 | Total reward: -1048110.500
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Avg agent reward (episode): -791232.562 | Total reward: -791232.562
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Avg agent reward (episode): -907931.000 | Total reward: -907931.000
Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Avg agent reward (episode): -959172.375 | Total reward: -959172.375
Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Avg agent reward (episode): -854216.688 | Total reward: -854216.688
Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Avg agent reward (episode): -870864.875 | Total reward: -870864.875
Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Avg agent reward (episode): -711501.500 | Total reward: -711501.500
Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -855873.375 ± 111707.195
  Average reward: -855873.375 ± 111707.195
  Total reward: -855873.375 ± 111707.195
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -619125.000 | Total reward: -619125.000
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -806306.500 | Total reward: -806306.500
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -868706.875 | Total reward: -868706.875
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -967434.812 | Total reward: -967434.812
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -768901.188 | Total reward: -768901.188
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -867192.625 | Total reward: -867192.625
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -902275.938 | Total reward: -902275.938
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -808262.875 | Total reward: -808262.875
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -828881.250 | Total reward: -828881.250
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -714115.375 | Total reward: -714115.375
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -815120.250 ± 93512.188
  Average reward: -815120.250 ± 93512.188
  Total reward: -815120.250 ± 93512.188
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -815120.250
Rule-based avg reward: -855873.375
No control avg reward: -815120.250
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
