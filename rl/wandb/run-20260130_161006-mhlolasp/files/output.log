Iteration 0:  80%|████████  | 16/20 [00:31<00:08,  2.02s/it, episode=10, norm_ret=-7.751, true_ret=-734806.125, steps=600]
Agent gate_2 episode reward: [-56.81497729]
All agents episode reward: [-56.81497729]
Agent gate_2 episode reward: [-11.20824782]
All agents episode reward: [-11.20824782]
Agent gate_2 episode reward: [-1.04391297]
All agents episode reward: [-1.04391297]
Agent gate_2 episode reward: [-0.81916673]
All agents episode reward: [-0.81916673]
Agent gate_2 episode reward: [-1.53098426]
All agents episode reward: [-1.53098426]
Agent gate_2 episode reward: [-1.13819825]
All agents episode reward: [-1.13819825]
Agent gate_2 episode reward: [-1.11892898]
All agents episode reward: [-1.11892898]
Agent gate_2 episode reward: [-1.23397551]
All agents episode reward: [-1.23397551]
Agent gate_2 episode reward: [-1.25762982]
All agents episode reward: [-1.25762982]
Agent gate_2 episode reward: [-1.34729749]
All agents episode reward: [-1.34729749]
Agent gate_2 episode reward: [-1.34876399]
All agents episode reward: [-1.34876399]
Agent gate_2 episode reward: [-1.39768511]
All agents episode reward: [-1.39768511]
Agent gate_2 episode reward: [-1.47208531]
All agents episode reward: [-1.47208531]
Agent gate_2 episode reward: [-1.56820585]
All agents episode reward: [-1.56820585]
Agent gate_2 episode reward: [-1.56001427]
All agents episode reward: [-1.56001427]
Agent gate_2 episode reward: [-1.63601607]
All agents episode reward: [-1.63601607]
Agent gate_2 episode reward: [-1.76768422]
All agents episode reward: [-1.76768422]
Agent gate_2 episode reward: [-1.70250081]
All agents episode reward: [-1.70250081]
Agent gate_2 episode reward: [-1.73779536]
All agents episode reward: [-1.73779536]
Agent gate_2 episode reward: [-1.79841288]
All agents episode reward: [-1.79841288]
Iteration 1:  80%|████████  | 16/20 [00:33<00:07,  1.98s/it, episode=30, norm_ret=-2.072, true_ret=-722761.562, steps=600]
Agent gate_2 episode reward: [-1.91191979]
All agents episode reward: [-1.91191979]
Agent gate_2 episode reward: [-1.92664972]
All agents episode reward: [-1.92664972]
Agent gate_2 episode reward: [-2.00462439]
All agents episode reward: [-2.00462439]
Agent gate_2 episode reward: [-2.02589081]
All agents episode reward: [-2.02589081]
Agent gate_2 episode reward: [-2.0187086]
All agents episode reward: [-2.0187086]
Agent gate_2 episode reward: [-2.03360056]
All agents episode reward: [-2.03360056]
Agent gate_2 episode reward: [-2.18576636]
All agents episode reward: [-2.18576636]
Agent gate_2 episode reward: [-2.11641034]
All agents episode reward: [-2.11641034]
Agent gate_2 episode reward: [-2.28554712]
All agents episode reward: [-2.28554712]
Agent gate_2 episode reward: [-2.20916857]
All agents episode reward: [-2.20916857]
Agent gate_2 episode reward: [-2.23537624]
All agents episode reward: [-2.23537624]
Agent gate_2 episode reward: [-2.31892023]
All agents episode reward: [-2.31892023]
Agent gate_2 episode reward: [-2.33950797]
All agents episode reward: [-2.33950797]
Agent gate_2 episode reward: [-2.24519128]
All agents episode reward: [-2.24519128]
Agent gate_2 episode reward: [-2.52506233]
All agents episode reward: [-2.52506233]
Agent gate_2 episode reward: [-3.07191271]
All agents episode reward: [-3.07191271]
Agent gate_2 episode reward: [-7.76279422]
All agents episode reward: [-7.76279422]
Agent gate_2 episode reward: [-3.43565122]
All agents episode reward: [-3.43565122]
Agent gate_2 episode reward: [-5.63985317]
All agents episode reward: [-5.63985317]
Agent gate_2 episode reward: [-4.826842]
All agents episode reward: [-4.826842]
Iteration 2:  80%|████████  | 16/20 [00:33<00:08,  2.04s/it, episode=50, norm_ret=-7.050, true_ret=-700773.562, steps=600]
Agent gate_2 episode reward: [-2.5031326]
All agents episode reward: [-2.5031326]
Agent gate_2 episode reward: [-2.60725935]
All agents episode reward: [-2.60725935]
Agent gate_2 episode reward: [-7.94987141]
All agents episode reward: [-7.94987141]
Agent gate_2 episode reward: [-48.46489666]
All agents episode reward: [-48.46489666]
Agent gate_2 episode reward: [-1.58456102]
All agents episode reward: [-1.58456102]
Agent gate_2 episode reward: [-1.33682594]
All agents episode reward: [-1.33682594]
Agent gate_2 episode reward: [-2.04440114]
All agents episode reward: [-2.04440114]
Agent gate_2 episode reward: [-1.35314469]
All agents episode reward: [-1.35314469]
Agent gate_2 episode reward: [-1.3155538]
All agents episode reward: [-1.3155538]
Agent gate_2 episode reward: [-1.33992648]
All agents episode reward: [-1.33992648]
Agent gate_2 episode reward: [-1.4232621]
All agents episode reward: [-1.4232621]
Agent gate_2 episode reward: [-1.48779349]
All agents episode reward: [-1.48779349]
Agent gate_2 episode reward: [-1.4010268]
All agents episode reward: [-1.4010268]
Agent gate_2 episode reward: [-1.3757905]
All agents episode reward: [-1.3757905]
Agent gate_2 episode reward: [-1.43152857]
All agents episode reward: [-1.43152857]
Agent gate_2 episode reward: [-1.4212392]
All agents episode reward: [-1.4212392]
Agent gate_2 episode reward: [-1.44112471]
All agents episode reward: [-1.44112471]
Agent gate_2 episode reward: [-1.4651748]
All agents episode reward: [-1.4651748]
Agent gate_2 episode reward: [-1.45472775]
All agents episode reward: [-1.45472775]
Agent gate_2 episode reward: [-1.47589563]
All agents episode reward: [-1.47589563]
Iteration 3:  80%|████████  | 16/20 [00:31<00:07,  1.99s/it, episode=70, norm_ret=-1.586, true_ret=-725488.875, steps=600]
Agent gate_2 episode reward: [-1.50541623]
All agents episode reward: [-1.50541623]
Agent gate_2 episode reward: [-1.60116472]
All agents episode reward: [-1.60116472]
Agent gate_2 episode reward: [-1.48280283]
All agents episode reward: [-1.48280283]
Agent gate_2 episode reward: [-1.50891365]
All agents episode reward: [-1.50891365]
Agent gate_2 episode reward: [-1.62737637]
All agents episode reward: [-1.62737637]
Agent gate_2 episode reward: [-1.52500708]
All agents episode reward: [-1.52500708]
Agent gate_2 episode reward: [-1.57908395]
All agents episode reward: [-1.57908395]
Agent gate_2 episode reward: [-1.53627105]
All agents episode reward: [-1.53627105]
Agent gate_2 episode reward: [-1.86192208]
All agents episode reward: [-1.86192208]
Agent gate_2 episode reward: [-1.6343217]
All agents episode reward: [-1.6343217]
Agent gate_2 episode reward: [-1.60313724]
All agents episode reward: [-1.60313724]
Agent gate_2 episode reward: [-1.65343235]
All agents episode reward: [-1.65343235]
Agent gate_2 episode reward: [-1.64708173]
All agents episode reward: [-1.64708173]
Agent gate_2 episode reward: [-1.70063115]
All agents episode reward: [-1.70063115]
Agent gate_2 episode reward: [-1.96100231]
All agents episode reward: [-1.96100231]
Agent gate_2 episode reward: [-1.7524856]
All agents episode reward: [-1.7524856]
Agent gate_2 episode reward: [-2.68875366]
All agents episode reward: [-2.68875366]
Agent gate_2 episode reward: [-2.56973568]
All agents episode reward: [-2.56973568]
Agent gate_2 episode reward: [-1.92947088]
All agents episode reward: [-1.92947088]
Agent gate_2 episode reward: [-2.02349331]
All agents episode reward: [-2.02349331]
Iteration 4:  80%|████████  | 16/20 [00:31<00:07,  1.93s/it, episode=90, norm_ret=-2.486, true_ret=-1014630.312, steps=600]
Agent gate_2 episode reward: [-2.54651548]
All agents episode reward: [-2.54651548]
Agent gate_2 episode reward: [-4.03618298]
All agents episode reward: [-4.03618298]
Agent gate_2 episode reward: [-1.8283737]
All agents episode reward: [-1.8283737]
Agent gate_2 episode reward: [-2.25677579]
All agents episode reward: [-2.25677579]
Agent gate_2 episode reward: [-1.86774792]
All agents episode reward: [-1.86774792]
Agent gate_2 episode reward: [-2.17751331]
All agents episode reward: [-2.17751331]
Agent gate_2 episode reward: [-2.44935244]
All agents episode reward: [-2.44935244]
Agent gate_2 episode reward: [-2.10714224]
All agents episode reward: [-2.10714224]
Agent gate_2 episode reward: [-3.00658606]
All agents episode reward: [-3.00658606]
Agent gate_2 episode reward: [-2.57991998]
All agents episode reward: [-2.57991998]
Agent gate_2 episode reward: [-2.09372624]
All agents episode reward: [-2.09372624]
Agent gate_2 episode reward: [-1.83584156]
All agents episode reward: [-1.83584156]
Agent gate_2 episode reward: [-2.5495352]
All agents episode reward: [-2.5495352]
Agent gate_2 episode reward: [-2.35059652]
All agents episode reward: [-2.35059652]
Agent gate_2 episode reward: [-2.08669461]
All agents episode reward: [-2.08669461]
Agent gate_2 episode reward: [-2.07650323]
All agents episode reward: [-2.07650323]
Agent gate_2 episode reward: [-2.66148716]
All agents episode reward: [-2.66148716]
Agent gate_2 episode reward: [-2.19559513]
All agents episode reward: [-2.19559513]
Agent gate_2 episode reward: [-2.07199105]
All agents episode reward: [-2.07199105]
Agent gate_2 episode reward: [-2.77923604]
All agents episode reward: [-2.77923604]
Iteration 5:  75%|███████▌  | 15/20 [00:33<00:10,  2.20s/it, episode=110, norm_ret=-2.563, true_ret=-783914.188, steps=600]
Agent gate_2 episode reward: [-2.6080076]
All agents episode reward: [-2.6080076]
Agent gate_2 episode reward: [-2.38771359]
All agents episode reward: [-2.38771359]
Agent gate_2 episode reward: [-2.67271079]
All agents episode reward: [-2.67271079]
Agent gate_2 episode reward: [-2.30994285]
All agents episode reward: [-2.30994285]
Agent gate_2 episode reward: [-2.42711119]
All agents episode reward: [-2.42711119]
Agent gate_2 episode reward: [-3.0570599]
All agents episode reward: [-3.0570599]
Agent gate_2 episode reward: [-3.17966485]
All agents episode reward: [-3.17966485]
Agent gate_2 episode reward: [-2.55000605]
All agents episode reward: [-2.55000605]
Agent gate_2 episode reward: [-2.23855167]
All agents episode reward: [-2.23855167]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -834844.688 at episode 110 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-2.19458222]
All agents episode reward: [-2.19458222]
Agent gate_2 episode reward: [-3.11814785]
All agents episode reward: [-3.11814785]
Agent gate_2 episode reward: [-2.82149436]
All agents episode reward: [-2.82149436]
Agent gate_2 episode reward: [-2.64619208]
All agents episode reward: [-2.64619208]
Agent gate_2 episode reward: [-3.40055664]
All agents episode reward: [-3.40055664]
Agent gate_2 episode reward: [-4.73197153]
All agents episode reward: [-4.73197153]
Agent gate_2 episode reward: [-6.42506131]
All agents episode reward: [-6.42506131]
Agent gate_2 episode reward: [-4.14313298]
All agents episode reward: [-4.14313298]
Agent gate_2 episode reward: [-2.75947275]
All agents episode reward: [-2.75947275]
Agent gate_2 episode reward: [-2.4829607]
All agents episode reward: [-2.4829607]
Agent gate_2 episode reward: [-2.97005952]
All agents episode reward: [-2.97005952]
Iteration 6:  80%|████████  | 16/20 [00:35<00:08,  2.14s/it, episode=130, norm_ret=-2.151, true_ret=-1174658.750, steps=600]
Agent gate_2 episode reward: [-1.53030673]
All agents episode reward: [-1.53030673]
Agent gate_2 episode reward: [-1.49893211]
All agents episode reward: [-1.49893211]
Agent gate_2 episode reward: [-1.92746769]
All agents episode reward: [-1.92746769]
Agent gate_2 episode reward: [-1.8891751]
All agents episode reward: [-1.8891751]
Agent gate_2 episode reward: [-1.55249224]
All agents episode reward: [-1.55249224]
Agent gate_2 episode reward: [-1.27786983]
All agents episode reward: [-1.27786983]
Agent gate_2 episode reward: [-1.64231079]
All agents episode reward: [-1.64231079]
Agent gate_2 episode reward: [-3.12476069]
All agents episode reward: [-3.12476069]
Agent gate_2 episode reward: [-3.39334692]
All agents episode reward: [-3.39334692]
Agent gate_2 episode reward: [-3.67433057]
All agents episode reward: [-3.67433057]
Agent gate_2 episode reward: [-2.25194463]
All agents episode reward: [-2.25194463]
Agent gate_2 episode reward: [-5.32230466]
All agents episode reward: [-5.32230466]
Agent gate_2 episode reward: [-7.34536957]
All agents episode reward: [-7.34536957]
Agent gate_2 episode reward: [-5.81031134]
All agents episode reward: [-5.81031134]
Agent gate_2 episode reward: [-3.01314299]
All agents episode reward: [-3.01314299]
Agent gate_2 episode reward: [-2.66323088]
All agents episode reward: [-2.66323088]
Agent gate_2 episode reward: [-2.32667302]
All agents episode reward: [-2.32667302]
Agent gate_2 episode reward: [-2.20500835]
All agents episode reward: [-2.20500835]
Agent gate_2 episode reward: [-2.36200637]
All agents episode reward: [-2.36200637]
Agent gate_2 episode reward: [-2.44519348]
All agents episode reward: [-2.44519348]
Iteration 7:  80%|████████  | 16/20 [00:36<00:08,  2.14s/it, episode=150, norm_ret=-4.066, true_ret=-1187751.875, steps=600]
Agent gate_2 episode reward: [-4.11195912]
All agents episode reward: [-4.11195912]
Agent gate_2 episode reward: [-3.45348405]
All agents episode reward: [-3.45348405]
Agent gate_2 episode reward: [-5.3159714]
All agents episode reward: [-5.3159714]
Agent gate_2 episode reward: [-7.04820054]
All agents episode reward: [-7.04820054]
Agent gate_2 episode reward: [-3.15928254]
All agents episode reward: [-3.15928254]
Agent gate_2 episode reward: [-3.42394099]
All agents episode reward: [-3.42394099]
Agent gate_2 episode reward: [-3.37287808]
All agents episode reward: [-3.37287808]
Agent gate_2 episode reward: [-3.35259616]
All agents episode reward: [-3.35259616]
Agent gate_2 episode reward: [-3.43229707]
All agents episode reward: [-3.43229707]
Agent gate_2 episode reward: [-3.99218863]
All agents episode reward: [-3.99218863]
Agent gate_2 episode reward: [-4.05668266]
All agents episode reward: [-4.05668266]
Agent gate_2 episode reward: [-6.90280475]
All agents episode reward: [-6.90280475]
Agent gate_2 episode reward: [-4.24838345]
All agents episode reward: [-4.24838345]
Agent gate_2 episode reward: [-4.33997172]
All agents episode reward: [-4.33997172]
Agent gate_2 episode reward: [-3.9078507]
All agents episode reward: [-3.9078507]
Agent gate_2 episode reward: [-3.99332221]
All agents episode reward: [-3.99332221]
Agent gate_2 episode reward: [-4.9748085]
All agents episode reward: [-4.9748085]
Agent gate_2 episode reward: [-6.69610666]
All agents episode reward: [-6.69610666]
Agent gate_2 episode reward: [-4.38282885]
All agents episode reward: [-4.38282885]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -578306.875 at episode 160 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-4.15899463]
All agents episode reward: [-4.15899463]
Iteration 8:  80%|████████  | 16/20 [00:35<00:08,  2.11s/it, episode=170, norm_ret=-1.240, true_ret=-339165.031, steps=600]
Agent gate_2 episode reward: [-1.20381792]
All agents episode reward: [-1.20381792]
Agent gate_2 episode reward: [-1.25554436]
All agents episode reward: [-1.25554436]
Agent gate_2 episode reward: [-1.22821759]
All agents episode reward: [-1.22821759]
Agent gate_2 episode reward: [-1.23281593]
All agents episode reward: [-1.23281593]
Agent gate_2 episode reward: [-1.22090795]
All agents episode reward: [-1.22090795]
Agent gate_2 episode reward: [-1.25218764]
All agents episode reward: [-1.25218764]
Agent gate_2 episode reward: [-1.24668542]
All agents episode reward: [-1.24668542]
Agent gate_2 episode reward: [-1.26462614]
All agents episode reward: [-1.26462614]
Agent gate_2 episode reward: [-1.2682273]
All agents episode reward: [-1.2682273]
Agent gate_2 episode reward: [-1.22497177]
All agents episode reward: [-1.22497177]
Agent gate_2 episode reward: [-2.64686297]
All agents episode reward: [-2.64686297]
Agent gate_2 episode reward: [-2.72784793]
All agents episode reward: [-2.72784793]
Agent gate_2 episode reward: [-2.53899148]
All agents episode reward: [-2.53899148]
Agent gate_2 episode reward: [-2.63334792]
All agents episode reward: [-2.63334792]
Agent gate_2 episode reward: [-3.13127269]
All agents episode reward: [-3.13127269]
Agent gate_2 episode reward: [-2.63355179]
All agents episode reward: [-2.63355179]
Agent gate_2 episode reward: [-2.63935124]
All agents episode reward: [-2.63935124]
Agent gate_2 episode reward: [-2.50242957]
All agents episode reward: [-2.50242957]
Agent gate_2 episode reward: [-2.53870299]
All agents episode reward: [-2.53870299]
Agent gate_2 episode reward: [-2.62412044]
All agents episode reward: [-2.62412044]
Iteration 9:  80%|████████  | 16/20 [00:35<00:08,  2.06s/it, episode=190, norm_ret=-3.143, true_ret=-825362.250, steps=600]
Agent gate_2 episode reward: [-3.22297736]
All agents episode reward: [-3.22297736]
Agent gate_2 episode reward: [-3.0299962]
All agents episode reward: [-3.0299962]
Agent gate_2 episode reward: [-3.11488803]
All agents episode reward: [-3.11488803]
Agent gate_2 episode reward: [-3.13146897]
All agents episode reward: [-3.13146897]
Agent gate_2 episode reward: [-3.12211298]
All agents episode reward: [-3.12211298]
Agent gate_2 episode reward: [-3.023113]
All agents episode reward: [-3.023113]
Agent gate_2 episode reward: [-3.18725425]
All agents episode reward: [-3.18725425]
Agent gate_2 episode reward: [-3.17507454]
All agents episode reward: [-3.17507454]
Agent gate_2 episode reward: [-3.23762307]
All agents episode reward: [-3.23762307]
Agent gate_2 episode reward: [-3.1832666]
All agents episode reward: [-3.1832666]
Agent gate_2 episode reward: [-4.77894636]
All agents episode reward: [-4.77894636]
Agent gate_2 episode reward: [-4.82883892]
All agents episode reward: [-4.82883892]
Agent gate_2 episode reward: [-4.97811596]
All agents episode reward: [-4.97811596]
Agent gate_2 episode reward: [-4.90864458]
All agents episode reward: [-4.90864458]
Agent gate_2 episode reward: [-4.29470077]
All agents episode reward: [-4.29470077]
Agent gate_2 episode reward: [-4.80884256]
All agents episode reward: [-4.80884256]
Agent gate_2 episode reward: [-4.57103838]
All agents episode reward: [-4.57103838]
Agent gate_2 episode reward: [-4.7702264]
All agents episode reward: [-4.7702264]
Agent gate_2 episode reward: [-5.17386804]
All agents episode reward: [-5.17386804]
Agent gate_2 episode reward: [-4.75577676]
All agents episode reward: [-4.75577676]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -775264.312 | Total reward: -775264.312
Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Avg agent reward (episode): -792843.688 | Total reward: -792843.688
Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Avg agent reward (episode): -915125.938 | Total reward: -915125.938
Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Avg agent reward (episode): -1327455.125 | Total reward: -1327455.125
Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Avg agent reward (episode): -954653.125 | Total reward: -954653.125
Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Avg agent reward (episode): -933681.250 | Total reward: -933681.250
Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Avg agent reward (episode): -1457003.875 | Total reward: -1457003.875
Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Avg agent reward (episode): -1153211.875 | Total reward: -1153211.875
Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Avg agent reward (episode): -909431.250 | Total reward: -909431.250
Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Avg agent reward (episode): -738244.750 | Total reward: -738244.750
Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -995691.500 ± 228727.812
  Average reward: -995691.500 ± 228727.812
  Total reward: -995691.500 ± 228727.812
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -643709.938 | Total reward: -643709.938
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Avg agent reward (episode): -851560.312 | Total reward: -851560.312
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Avg agent reward (episode): -920434.188 | Total reward: -920434.188
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Avg agent reward (episode): -1048110.500 | Total reward: -1048110.500
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Avg agent reward (episode): -791232.562 | Total reward: -791232.562
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Avg agent reward (episode): -907931.062 | Total reward: -907931.062
Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Avg agent reward (episode): -959172.375 | Total reward: -959172.375
Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Avg agent reward (episode): -854216.688 | Total reward: -854216.688
Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Avg agent reward (episode): -870864.938 | Total reward: -870864.938
Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Avg agent reward (episode): -711501.500 | Total reward: -711501.500
Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -855873.375 ± 111707.203
  Average reward: -855873.375 ± 111707.203
  Total reward: -855873.375 ± 111707.203
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -619125.000 | Total reward: -619125.000
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -806306.500 | Total reward: -806306.500
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -868706.875 | Total reward: -868706.875
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -967434.938 | Total reward: -967434.938
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -768901.188 | Total reward: -768901.188
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -867192.688 | Total reward: -867192.688
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -902276.062 | Total reward: -902276.062
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -808263.062 | Total reward: -808263.062
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -828881.250 | Total reward: -828881.250
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -714115.438 | Total reward: -714115.438
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -815120.250 ± 93512.219
  Average reward: -815120.250 ± 93512.219
  Total reward: -815120.250 ± 93512.219
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -995691.500
Rule-based avg reward: -855873.375
No control avg reward: -815120.250
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
