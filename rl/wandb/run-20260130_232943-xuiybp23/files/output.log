Iteration 0:  80%|████████  | 16/20 [00:38<00:09,  2.28s/it, episode=10, norm_ret=-7.729, true_ret=-880978.000, steps=600]
Agent gate_2 episode reward: [-56.78837385]
All agents episode reward: [-56.78837385]
Agent gate_2 episode reward: [-12.3300954]
All agents episode reward: [-12.3300954]
Agent gate_2 episode reward: [-1.22158629]
All agents episode reward: [-1.22158629]
Agent gate_2 episode reward: [-0.82233312]
All agents episode reward: [-0.82233312]
Agent gate_2 episode reward: [-1.24334787]
All agents episode reward: [-1.24334787]
Agent gate_2 episode reward: [-0.73090709]
All agents episode reward: [-0.73090709]
Agent gate_2 episode reward: [-1.11758458]
All agents episode reward: [-1.11758458]
Agent gate_2 episode reward: [-1.00225807]
All agents episode reward: [-1.00225807]
Agent gate_2 episode reward: [-0.84938399]
All agents episode reward: [-0.84938399]
Agent gate_2 episode reward: [-1.17964962]
All agents episode reward: [-1.17964962]
Agent gate_2 episode reward: [-1.16082428]
All agents episode reward: [-1.16082428]
Agent gate_2 episode reward: [-1.31389165]
All agents episode reward: [-1.31389165]
Agent gate_2 episode reward: [-1.29726945]
All agents episode reward: [-1.29726945]
Agent gate_2 episode reward: [-1.21159455]
All agents episode reward: [-1.21159455]
Agent gate_2 episode reward: [-1.05519725]
All agents episode reward: [-1.05519725]
Agent gate_2 episode reward: [-0.7488245]
All agents episode reward: [-0.7488245]
Agent gate_2 episode reward: [-1.64659968]
All agents episode reward: [-1.64659968]
Agent gate_2 episode reward: [-1.50190837]
All agents episode reward: [-1.50190837]
Agent gate_2 episode reward: [-1.3149462]
All agents episode reward: [-1.3149462]
Agent gate_2 episode reward: [-1.96602373]
All agents episode reward: [-1.96602373]
Iteration 1:  80%|████████  | 16/20 [00:36<00:09,  2.34s/it, episode=30, norm_ret=-2.084, true_ret=-943152.625, steps=600]
Agent gate_2 episode reward: [-1.83587344]
All agents episode reward: [-1.83587344]
Agent gate_2 episode reward: [-1.71127221]
All agents episode reward: [-1.71127221]
Agent gate_2 episode reward: [-1.56767546]
All agents episode reward: [-1.56767546]
Agent gate_2 episode reward: [-1.43990082]
All agents episode reward: [-1.43990082]
Agent gate_2 episode reward: [-1.9547265]
All agents episode reward: [-1.9547265]
Agent gate_2 episode reward: [-1.67043162]
All agents episode reward: [-1.67043162]
Agent gate_2 episode reward: [-3.65381701]
All agents episode reward: [-3.65381701]
Agent gate_2 episode reward: [-2.64168022]
All agents episode reward: [-2.64168022]
Agent gate_2 episode reward: [-2.2482942]
All agents episode reward: [-2.2482942]
Agent gate_2 episode reward: [-2.11909103]
All agents episode reward: [-2.11909103]
Agent gate_2 episode reward: [-1.62545089]
All agents episode reward: [-1.62545089]
Agent gate_2 episode reward: [-2.12071855]
All agents episode reward: [-2.12071855]
Agent gate_2 episode reward: [-2.07008477]
All agents episode reward: [-2.07008477]
Agent gate_2 episode reward: [-1.99706688]
All agents episode reward: [-1.99706688]
Agent gate_2 episode reward: [-1.87788652]
All agents episode reward: [-1.87788652]
Agent gate_2 episode reward: [-2.07401525]
All agents episode reward: [-2.07401525]
Agent gate_2 episode reward: [-4.07035804]
All agents episode reward: [-4.07035804]
Agent gate_2 episode reward: [-2.49880043]
All agents episode reward: [-2.49880043]
Agent gate_2 episode reward: [-1.26469747]
All agents episode reward: [-1.26469747]
Agent gate_2 episode reward: [-2.2197737]
All agents episode reward: [-2.2197737]
Iteration 2:  80%|████████  | 16/20 [00:36<00:09,  2.27s/it, episode=50, norm_ret=-2.162, true_ret=-705896.875, steps=600]
Agent gate_2 episode reward: [-2.29342619]
All agents episode reward: [-2.29342619]
Agent gate_2 episode reward: [-1.87878682]
All agents episode reward: [-1.87878682]
Agent gate_2 episode reward: [-0.94406927]
All agents episode reward: [-0.94406927]
Agent gate_2 episode reward: [-3.05210276]
All agents episode reward: [-3.05210276]
Agent gate_2 episode reward: [-2.37291826]
All agents episode reward: [-2.37291826]
Agent gate_2 episode reward: [-3.17637452]
All agents episode reward: [-3.17637452]
Agent gate_2 episode reward: [-2.40885822]
All agents episode reward: [-2.40885822]
Agent gate_2 episode reward: [-1.23898845]
All agents episode reward: [-1.23898845]
Agent gate_2 episode reward: [-2.23893548]
All agents episode reward: [-2.23893548]
Agent gate_2 episode reward: [-2.01837808]
All agents episode reward: [-2.01837808]
Agent gate_2 episode reward: [-2.3025465]
All agents episode reward: [-2.3025465]
Agent gate_2 episode reward: [-2.49535113]
All agents episode reward: [-2.49535113]
Agent gate_2 episode reward: [-2.6914193]
All agents episode reward: [-2.6914193]
Agent gate_2 episode reward: [-2.6606357]
All agents episode reward: [-2.6606357]
Agent gate_2 episode reward: [-2.42876366]
All agents episode reward: [-2.42876366]
Agent gate_2 episode reward: [-3.07953065]
All agents episode reward: [-3.07953065]
Agent gate_2 episode reward: [-2.68854269]
All agents episode reward: [-2.68854269]
Agent gate_2 episode reward: [-4.17047691]
All agents episode reward: [-4.17047691]
Agent gate_2 episode reward: [-3.24154195]
All agents episode reward: [-3.24154195]
Agent gate_2 episode reward: [-2.60318612]
All agents episode reward: [-2.60318612]
Iteration 3:  80%|████████  | 16/20 [00:36<00:08,  2.24s/it, episode=70, norm_ret=-3.290, true_ret=-732499.250, steps=600]
Agent gate_2 episode reward: [-2.70114292]
All agents episode reward: [-2.70114292]
Agent gate_2 episode reward: [-1.33662809]
All agents episode reward: [-1.33662809]
Agent gate_2 episode reward: [-2.37317008]
All agents episode reward: [-2.37317008]
Agent gate_2 episode reward: [-3.12843422]
All agents episode reward: [-3.12843422]
Agent gate_2 episode reward: [-10.53516001]
All agents episode reward: [-10.53516001]
Agent gate_2 episode reward: [-2.256046]
All agents episode reward: [-2.256046]
Agent gate_2 episode reward: [-2.97978686]
All agents episode reward: [-2.97978686]
Agent gate_2 episode reward: [-2.58467253]
All agents episode reward: [-2.58467253]
Agent gate_2 episode reward: [-2.59605997]
All agents episode reward: [-2.59605997]
Agent gate_2 episode reward: [-2.40894511]
All agents episode reward: [-2.40894511]
Agent gate_2 episode reward: [-2.34312727]
All agents episode reward: [-2.34312727]
Agent gate_2 episode reward: [-3.96191578]
All agents episode reward: [-3.96191578]
Agent gate_2 episode reward: [-4.08090879]
All agents episode reward: [-4.08090879]
Agent gate_2 episode reward: [-1.19159448]
All agents episode reward: [-1.19159448]
Agent gate_2 episode reward: [-3.29354158]
All agents episode reward: [-3.29354158]
Agent gate_2 episode reward: [-2.69745589]
All agents episode reward: [-2.69745589]
Agent gate_2 episode reward: [-6.63826983]
All agents episode reward: [-6.63826983]
Agent gate_2 episode reward: [-5.32075442]
All agents episode reward: [-5.32075442]
Agent gate_2 episode reward: [-4.85125423]
All agents episode reward: [-4.85125423]
Agent gate_2 episode reward: [-1.19203577]
All agents episode reward: [-1.19203577]
Iteration 4:  80%|████████  | 16/20 [00:36<00:08,  2.25s/it, episode=90, norm_ret=-3.058, true_ret=-929695.312, steps=600]
Agent gate_2 episode reward: [-1.16108042]
All agents episode reward: [-1.16108042]
Agent gate_2 episode reward: [-2.39495199]
All agents episode reward: [-2.39495199]
Agent gate_2 episode reward: [-2.44206503]
All agents episode reward: [-2.44206503]
Agent gate_2 episode reward: [-3.02458039]
All agents episode reward: [-3.02458039]
Agent gate_2 episode reward: [-2.60620884]
All agents episode reward: [-2.60620884]
Agent gate_2 episode reward: [-4.19227167]
All agents episode reward: [-4.19227167]
Agent gate_2 episode reward: [-4.14208214]
All agents episode reward: [-4.14208214]
Agent gate_2 episode reward: [-2.59241713]
All agents episode reward: [-2.59241713]
Agent gate_2 episode reward: [-4.6053816]
All agents episode reward: [-4.6053816]
Agent gate_2 episode reward: [-3.41426048]
All agents episode reward: [-3.41426048]
Agent gate_2 episode reward: [-5.35584026]
All agents episode reward: [-5.35584026]
Agent gate_2 episode reward: [-2.79080275]
All agents episode reward: [-2.79080275]
Agent gate_2 episode reward: [-5.0391664]
All agents episode reward: [-5.0391664]
Agent gate_2 episode reward: [-4.21206968]
All agents episode reward: [-4.21206968]
Agent gate_2 episode reward: [-4.25837385]
All agents episode reward: [-4.25837385]
Agent gate_2 episode reward: [-4.28644948]
All agents episode reward: [-4.28644948]
Agent gate_2 episode reward: [-2.59617578]
All agents episode reward: [-2.59617578]
Agent gate_2 episode reward: [-2.58710607]
All agents episode reward: [-2.58710607]
Agent gate_2 episode reward: [-2.33495881]
All agents episode reward: [-2.33495881]
Agent gate_2 episode reward: [-4.13135312]
All agents episode reward: [-4.13135312]
Iteration 5:  75%|███████▌  | 15/20 [00:38<00:12,  2.53s/it, episode=110, norm_ret=-3.849, true_ret=-871900.500, steps=600]
Agent gate_2 episode reward: [-4.14694885]
All agents episode reward: [-4.14694885]
Agent gate_2 episode reward: [-3.18935182]
All agents episode reward: [-3.18935182]
Agent gate_2 episode reward: [-3.23909736]
All agents episode reward: [-3.23909736]
Agent gate_2 episode reward: [-3.23284717]
All agents episode reward: [-3.23284717]
Agent gate_2 episode reward: [-2.61542361]
All agents episode reward: [-2.61542361]
Agent gate_2 episode reward: [-4.40957232]
All agents episode reward: [-4.40957232]
Agent gate_2 episode reward: [-4.12164796]
All agents episode reward: [-4.12164796]
Agent gate_2 episode reward: [-5.2568386]
All agents episode reward: [-5.2568386]
Agent gate_2 episode reward: [-4.77348115]
All agents episode reward: [-4.77348115]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -802808.375 at episode 110 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-3.50641318]
All agents episode reward: [-3.50641318]
Agent gate_2 episode reward: [-3.08890264]
All agents episode reward: [-3.08890264]
Agent gate_2 episode reward: [-5.061028]
All agents episode reward: [-5.061028]
Agent gate_2 episode reward: [-4.34773493]
All agents episode reward: [-4.34773493]
Agent gate_2 episode reward: [-7.22929248]
All agents episode reward: [-7.22929248]
Agent gate_2 episode reward: [-3.89726348]
All agents episode reward: [-3.89726348]
Agent gate_2 episode reward: [-10.52036866]
All agents episode reward: [-10.52036866]
Agent gate_2 episode reward: [-6.02138496]
All agents episode reward: [-6.02138496]
Agent gate_2 episode reward: [-5.92108903]
All agents episode reward: [-5.92108903]
Agent gate_2 episode reward: [-3.79215336]
All agents episode reward: [-3.79215336]
Agent gate_2 episode reward: [-4.75015469]
All agents episode reward: [-4.75015469]
Iteration 6:  80%|████████  | 16/20 [00:40<00:09,  2.45s/it, episode=130, norm_ret=-3.302, true_ret=-863156.875, steps=600]
Agent gate_2 episode reward: [-3.00421504]
All agents episode reward: [-3.00421504]
Agent gate_2 episode reward: [-2.66335159]
All agents episode reward: [-2.66335159]
Agent gate_2 episode reward: [-6.02370838]
All agents episode reward: [-6.02370838]
Agent gate_2 episode reward: [-3.65801736]
All agents episode reward: [-3.65801736]
Agent gate_2 episode reward: [-1.58208646]
All agents episode reward: [-1.58208646]
Agent gate_2 episode reward: [-1.63196904]
All agents episode reward: [-1.63196904]
Agent gate_2 episode reward: [-1.2071178]
All agents episode reward: [-1.2071178]
Agent gate_2 episode reward: [-4.39552827]
All agents episode reward: [-4.39552827]
Agent gate_2 episode reward: [-6.99527637]
All agents episode reward: [-6.99527637]
Agent gate_2 episode reward: [-1.86153891]
All agents episode reward: [-1.86153891]
Agent gate_2 episode reward: [-2.45509858]
All agents episode reward: [-2.45509858]
Agent gate_2 episode reward: [-4.90403817]
All agents episode reward: [-4.90403817]
Agent gate_2 episode reward: [-2.74501907]
All agents episode reward: [-2.74501907]
Agent gate_2 episode reward: [-1.78166207]
All agents episode reward: [-1.78166207]
Agent gate_2 episode reward: [-1.7836714]
All agents episode reward: [-1.7836714]
Agent gate_2 episode reward: [-1.24907341]
All agents episode reward: [-1.24907341]
Agent gate_2 episode reward: [-1.45808859]
All agents episode reward: [-1.45808859]
Agent gate_2 episode reward: [-0.74525184]
All agents episode reward: [-0.74525184]
Agent gate_2 episode reward: [-1.3410085]
All agents episode reward: [-1.3410085]
Agent gate_2 episode reward: [-0.83965405]
All agents episode reward: [-0.83965405]
Iteration 7:  75%|███████▌  | 15/20 [00:37<00:12,  2.45s/it, episode=150, norm_ret=-1.594, true_ret=-778073.875, steps=600]
Agent gate_2 episode reward: [-1.39192333]
All agents episode reward: [-1.39192333]
Agent gate_2 episode reward: [-1.37305172]
All agents episode reward: [-1.37305172]
Agent gate_2 episode reward: [-2.1655801]
All agents episode reward: [-2.1655801]
Agent gate_2 episode reward: [-1.66830721]
All agents episode reward: [-1.66830721]
Agent gate_2 episode reward: [-1.99021817]
All agents episode reward: [-1.99021817]
Agent gate_2 episode reward: [-1.54347013]
All agents episode reward: [-1.54347013]
Agent gate_2 episode reward: [-1.60583121]
All agents episode reward: [-1.60583121]
Agent gate_2 episode reward: [-0.90302134]
All agents episode reward: [-0.90302134]
Agent gate_2 episode reward: [-1.70604189]
All agents episode reward: [-1.70604189]
Saved 1 agents to ppo_agents_butterfly_scC
[Validation] New best avg return: -746039.062 at episode 150 (over 5 val episodes, saved to ppo_agents_butterfly_scC)
Agent gate_2 episode reward: [-1.58953186]
All agents episode reward: [-1.58953186]
Agent gate_2 episode reward: [-1.39283188]
All agents episode reward: [-1.39283188]
Agent gate_2 episode reward: [-1.59385896]
All agents episode reward: [-1.59385896]
Agent gate_2 episode reward: [-0.83004746]
All agents episode reward: [-0.83004746]
Agent gate_2 episode reward: [-1.68966919]
All agents episode reward: [-1.68966919]
Agent gate_2 episode reward: [-1.76461446]
All agents episode reward: [-1.76461446]
Agent gate_2 episode reward: [-1.74858463]
All agents episode reward: [-1.74858463]
Agent gate_2 episode reward: [-3.41434176]
All agents episode reward: [-3.41434176]
Agent gate_2 episode reward: [-1.80768543]
All agents episode reward: [-1.80768543]
Agent gate_2 episode reward: [-1.668388]
All agents episode reward: [-1.668388]
Agent gate_2 episode reward: [-1.32894338]
All agents episode reward: [-1.32894338]
Iteration 8:  80%|████████  | 16/20 [00:39<00:09,  2.35s/it, episode=170, norm_ret=-1.969, true_ret=-762525.000, steps=600]
Agent gate_2 episode reward: [-1.67325118]
All agents episode reward: [-1.67325118]
Agent gate_2 episode reward: [-2.03551956]
All agents episode reward: [-2.03551956]
Agent gate_2 episode reward: [-1.92877601]
All agents episode reward: [-1.92877601]
Agent gate_2 episode reward: [-1.98952108]
All agents episode reward: [-1.98952108]
Agent gate_2 episode reward: [-1.58139027]
All agents episode reward: [-1.58139027]
Agent gate_2 episode reward: [-2.40123017]
All agents episode reward: [-2.40123017]
Agent gate_2 episode reward: [-1.51634138]
All agents episode reward: [-1.51634138]
Agent gate_2 episode reward: [-2.10795066]
All agents episode reward: [-2.10795066]
Agent gate_2 episode reward: [-2.77091234]
All agents episode reward: [-2.77091234]
Agent gate_2 episode reward: [-1.68424074]
All agents episode reward: [-1.68424074]
Agent gate_2 episode reward: [-1.39246038]
All agents episode reward: [-1.39246038]
Agent gate_2 episode reward: [-2.62124495]
All agents episode reward: [-2.62124495]
Agent gate_2 episode reward: [-2.04016126]
All agents episode reward: [-2.04016126]
Agent gate_2 episode reward: [-1.17570211]
All agents episode reward: [-1.17570211]
Agent gate_2 episode reward: [-1.74508508]
All agents episode reward: [-1.74508508]
Agent gate_2 episode reward: [-1.65270163]
All agents episode reward: [-1.65270163]
Agent gate_2 episode reward: [-2.10845227]
All agents episode reward: [-2.10845227]
Agent gate_2 episode reward: [-1.90354392]
All agents episode reward: [-1.90354392]
Agent gate_2 episode reward: [-2.24343494]
All agents episode reward: [-2.24343494]
Agent gate_2 episode reward: [-3.03491619]
All agents episode reward: [-3.03491619]
Iteration 9:  80%|████████  | 16/20 [00:39<00:09,  2.37s/it, episode=190, norm_ret=-2.241, true_ret=-786517.875, steps=600]
Agent gate_2 episode reward: [-3.11880289]
All agents episode reward: [-3.11880289]
Agent gate_2 episode reward: [-2.14797759]
All agents episode reward: [-2.14797759]
Agent gate_2 episode reward: [-1.70596367]
All agents episode reward: [-1.70596367]
Agent gate_2 episode reward: [-1.75414419]
All agents episode reward: [-1.75414419]
Agent gate_2 episode reward: [-2.84220124]
All agents episode reward: [-2.84220124]
Agent gate_2 episode reward: [-2.81710812]
All agents episode reward: [-2.81710812]
Agent gate_2 episode reward: [-1.67132379]
All agents episode reward: [-1.67132379]
Agent gate_2 episode reward: [-2.41861983]
All agents episode reward: [-2.41861983]
Agent gate_2 episode reward: [-2.07865188]
All agents episode reward: [-2.07865188]
Agent gate_2 episode reward: [-1.85802492]
All agents episode reward: [-1.85802492]
Agent gate_2 episode reward: [-1.92088145]
All agents episode reward: [-1.92088145]
Agent gate_2 episode reward: [-3.03652427]
All agents episode reward: [-3.03652427]
Agent gate_2 episode reward: [-2.37456389]
All agents episode reward: [-2.37456389]
Agent gate_2 episode reward: [-2.6882812]
All agents episode reward: [-2.6882812]
Agent gate_2 episode reward: [-2.08307517]
All agents episode reward: [-2.08307517]
Agent gate_2 episode reward: [-1.33304304]
All agents episode reward: [-1.33304304]
Agent gate_2 episode reward: [-2.32315614]
All agents episode reward: [-2.32315614]
Agent gate_2 episode reward: [-1.86370608]
All agents episode reward: [-1.86370608]
Agent gate_2 episode reward: [-1.9740485]
All agents episode reward: [-1.9740485]
Agent gate_2 episode reward: [-3.08634302]
All agents episode reward: [-3.08634302]
Loaded 1 agents from ppo_agents_butterfly_scC
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -867370.688 | Total reward: -867370.688
Saved run 1 to rl_training/butterfly_scC/ppo_run1
  Run 2/10... Avg agent reward (episode): -789720.312 | Total reward: -789720.312
Saved run 2 to rl_training/butterfly_scC/ppo_run2
  Run 3/10... Avg agent reward (episode): -918459.938 | Total reward: -918459.938
Saved run 3 to rl_training/butterfly_scC/ppo_run3
  Run 4/10... Avg agent reward (episode): -1116005.250 | Total reward: -1116005.250
Saved run 4 to rl_training/butterfly_scC/ppo_run4
  Run 5/10... Avg agent reward (episode): -748825.625 | Total reward: -748825.625
Saved run 5 to rl_training/butterfly_scC/ppo_run5
  Run 6/10... Avg agent reward (episode): -876714.875 | Total reward: -876714.875
Saved run 6 to rl_training/butterfly_scC/ppo_run6
  Run 7/10... Avg agent reward (episode): -1133798.625 | Total reward: -1133798.625
Saved run 7 to rl_training/butterfly_scC/ppo_run7
  Run 8/10... Avg agent reward (episode): -765995.938 | Total reward: -765995.938
Saved run 8 to rl_training/butterfly_scC/ppo_run8
  Run 9/10... Avg agent reward (episode): -843428.438 | Total reward: -843428.438
Saved run 9 to rl_training/butterfly_scC/ppo_run9
  Run 10/10... Avg agent reward (episode): -750569.375 | Total reward: -750569.375
Saved run 10 to rl_training/butterfly_scC/ppo_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -881088.875 ± 133489.203
  Average reward: -881088.875 ± 133489.203
  Total reward: -881088.875 ± 133489.203
============================================================
Running 10 evaluation runs...
  Run 1/10... Avg agent reward (episode): -658559.375 | Total reward: -658559.375
Saved run 1 to rl_training/butterfly_scC/rule_based_run1
  Run 2/10... Avg agent reward (episode): -875377.625 | Total reward: -875377.625
Saved run 2 to rl_training/butterfly_scC/rule_based_run2
  Run 3/10... Avg agent reward (episode): -945928.000 | Total reward: -945928.000
Saved run 3 to rl_training/butterfly_scC/rule_based_run3
  Run 4/10... Avg agent reward (episode): -1076692.375 | Total reward: -1076692.375
Saved run 4 to rl_training/butterfly_scC/rule_based_run4
  Run 5/10... Avg agent reward (episode): -811311.062 | Total reward: -811311.062
Saved run 5 to rl_training/butterfly_scC/rule_based_run5
  Run 6/10... Avg agent reward (episode): -931933.875 | Total reward: -931933.875
Saved run 6 to rl_training/butterfly_scC/rule_based_run6
  Run 7/10... Avg agent reward (episode): -986011.250 | Total reward: -986011.250
Saved run 7 to rl_training/butterfly_scC/rule_based_run7
  Run 8/10... Avg agent reward (episode): -877214.562 | Total reward: -877214.562
Saved run 8 to rl_training/butterfly_scC/rule_based_run8
  Run 9/10... Avg agent reward (episode): -895241.312 | Total reward: -895241.312
Saved run 9 to rl_training/butterfly_scC/rule_based_run9
  Run 10/10... Avg agent reward (episode): -728699.000 | Total reward: -728699.000
Saved run 10 to rl_training/butterfly_scC/rule_based_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -878696.812 ± 115698.078
  Average reward: -878696.812 ± 115698.078
  Total reward: -878696.812 ± 115698.078
============================================================
Running 10 evaluation runs...
  Run 1/10... No actions provided, skipping action application.
Avg agent reward (episode): -633154.438 | Total reward: -633154.438
Saved run 1 to rl_training/butterfly_scC/no_control_run1
  Run 2/10... No actions provided, skipping action application.
Avg agent reward (episode): -829243.125 | Total reward: -829243.125
Saved run 2 to rl_training/butterfly_scC/no_control_run2
  Run 3/10... No actions provided, skipping action application.
Avg agent reward (episode): -892837.938 | Total reward: -892837.938
Saved run 3 to rl_training/butterfly_scC/no_control_run3
  Run 4/10... No actions provided, skipping action application.
Avg agent reward (episode): -993297.500 | Total reward: -993297.500
Saved run 4 to rl_training/butterfly_scC/no_control_run4
  Run 5/10... No actions provided, skipping action application.
Avg agent reward (episode): -788849.125 | Total reward: -788849.125
Saved run 5 to rl_training/butterfly_scC/no_control_run5
  Run 6/10... No actions provided, skipping action application.
Avg agent reward (episode): -891637.375 | Total reward: -891637.375
Saved run 6 to rl_training/butterfly_scC/no_control_run6
  Run 7/10... No actions provided, skipping action application.
Avg agent reward (episode): -926869.625 | Total reward: -926869.625
Saved run 7 to rl_training/butterfly_scC/no_control_run7
  Run 8/10... No actions provided, skipping action application.
Avg agent reward (episode): -829928.000 | Total reward: -829928.000
Saved run 8 to rl_training/butterfly_scC/no_control_run8
  Run 9/10... No actions provided, skipping action application.
Avg agent reward (episode): -852769.625 | Total reward: -852769.625
Saved run 9 to rl_training/butterfly_scC/no_control_run9
  Run 10/10... No actions provided, skipping action application.
Avg agent reward (episode): -733755.875 | Total reward: -733755.875
Saved run 10 to rl_training/butterfly_scC/no_control_run10
============================================================
Evaluation Results
  Number of runs: 10
============================================================
  Agent gate_2: -837234.250 ± 96694.234
  Average reward: -837234.250 ± 96694.234
  Total reward: -837234.250 ± 96694.234
============================================================

============================================================
Comparison of All Methods
============================================================
ppo avg reward:        -881088.875
Rule-based avg reward: -878696.812
No control avg reward: -837234.250
============================================================
/Users/mmai/anaconda3/envs/control/lib/python3.11/site-packages/matplotlib/patches.py:3421: RuntimeWarning: invalid value encountered in scalar divide
  cos_t, sin_t = head_length / head_dist, head_width / head_dist
